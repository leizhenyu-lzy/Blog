[Transformers, Simply Explained | Deep Learning -  - YouTube(DeepBean)](https://www.youtube.com/watch?v=UPhaYex4zZk)


解决 旧架构(eg : RNN) 中的问题
1. Long-Range : information 难以保持 很多步 timestamps
2. Parallel   : 需要类似自回归方式，难以并行(train & inference)

Basic Idea : **Self-Attention**


Operation
1. <img src="Pics/transformer001.png" width=700>


Architecture
1. <img src="Pics/transformer002.png" width=700>
2. Word Vectorization/Embedding


