# RAG(Retrieval-Augmented Generation) - æ£€ç´¢å¢å¼ºç”Ÿæˆ

è§£å†³ LLM é—®é¢˜
1. ä¿¡æ¯æ»å - é™æ€çŸ¥è¯†
2. æ¨¡å‹å¹»è§‰ - **hallucination**
3. ç§æœ‰æ•°æ®åŒ®ä¹ - è®­ç»ƒæ•°æ®æ¥è‡ªäº’è”ç½‘å…¬å¼€æ•°æ®ï¼Œæ„å»ºç§æœ‰çŸ¥è¯†åº“

---

## Table of Content
- [RAG(Retrieval-Augmented Generation) - æ£€ç´¢å¢å¼ºç”Ÿæˆ](#ragretrieval-augmented-generation---æ£€ç´¢å¢å¼ºç”Ÿæˆ)
  - [Table of Content](#table-of-content)
- [Prompt-Engineer \& RAG \& Fine-Tunning](#prompt-engineer--rag--fine-tunning)
- [RAG é«˜æ•ˆåº”ç”¨æŒ‡å— - AIèŠ±æœå±±](#rag-é«˜æ•ˆåº”ç”¨æŒ‡å—---aièŠ±æœå±±)
  - [æ–‡æ¡£æ™ºèƒ½è§£æ](#æ–‡æ¡£æ™ºèƒ½è§£æ)
  - [æ–‡æœ¬åˆ†å— Text Chunking/Splitting](#æ–‡æœ¬åˆ†å—-text-chunkingsplitting)
- [IBM Technology - Introductions](#ibm-technology---introductions)
  - [How Large Language Models Work?](#how-large-language-models-work)
  - [Why Are There So Many Foundation Models?](#why-are-there-so-many-foundation-models)
  - [What is Retrieval-Augmented Generation (RAG)?](#what-is-retrieval-augmented-generation-rag)
  - [Why Large Language Models Hallucinate?](#why-large-language-models-hallucinate)
  - [What is a Vector Database?](#what-is-a-vector-database)
- [Retrieval-Augmented Generation for Large Language Models: A Survey](#retrieval-augmented-generation-for-large-language-models-a-survey)
- [Unifying RAG and long context LLMs](#unifying-rag-and-long-context-llms)
- [RAG \& KnowledgeGraph](#rag--knowledgegraph)
  - [GraphRAG - Microsoft](#graphrag---microsoft)
  - [LLM \& KnowledgeGraph](#llm--knowledgegraph)
- [Advanced RAG Techniques : an Illustrated Overview](#advanced-rag-techniques--an-illustrated-overview)
  - [Vector Search - å‘é‡æœç´¢](#vector-search---å‘é‡æœç´¢)
  - [LLM äº§å“](#llm-äº§å“)
  - [LLM-based pipelines \& applications](#llm-based-pipelines--applications)
  - [Naive RAG](#naive-rag)
  - [Advanced RAG](#advanced-rag)
    - [01 - Chunking \& vectorization](#01---chunking--vectorization)
    - [02 - Index](#02---index)
      - [Vector store index](#vector-store-index)
      - [Hierarchical indices](#hierarchical-indices)
      - [Hypothetical Questions and HyDE](#hypothetical-questions-and-hyde)
      - [Context enrichment](#context-enrichment)
        - [Sentence Window Retrieval](#sentence-window-retrieval)
        - [Auto-merging Retriever (Parent Document Retriever)](#auto-merging-retriever-parent-document-retriever)
        - [Fusion Retrieval or Hybrid Search](#fusion-retrieval-or-hybrid-search)
    - [03 - Reranking \& filtering](#03---reranking--filtering)
    - [04 - Query transformations](#04---query-transformations)
    - [05 - Reference citations](#05---reference-citations)
    - [06 - Chat Engine](#06---chat-engine)
    - [07 - Query Routing](#07---query-routing)
    - [08 - Agents in RAG](#08---agents-in-rag)
    - [09 - Response synthesizer](#09---response-synthesizer)
    - [10 - Encoder and LLM fine-tuning](#10---encoder-and-llm-fine-tuning)
    - [11 - Evaluation](#11---evaluation)
    - [12 - Conclusion](#12---conclusion)
- [12 RAG Pain Points and Proposed Solutions-Solving the core challenges of Retrieval-Augmented Generation](#12-rag-pain-points-and-proposed-solutions-solving-the-core-challenges-of-retrieval-augmented-generation)

---

![](Pics/rag002.png)

![](Pics/rag024.png)

---

# Prompt-Engineer & RAG & Fine-Tunning

å¯¹äº é”™è¯¯å›å¤
1. Prompt-Engineer : è§£å†³é—®é¢˜é—®ä¸æ¸…æ¥š(ä¸æ”¹åŠ¨LLM)
2. RAG : è§£å†³ç¼ºä¹ç›¸å…³çŸ¥è¯†(ä¸æ”¹åŠ¨LLM)
3. Fine-Tunning : è§£å†³æ¨¡å‹èƒ½åŠ›ä¸è¶³(æ”¹åŠ¨LLM)ï¼Œä¸å»ºè®®ä¸€å¼€å§‹å°±è¿›è¡Œæ¨¡å‹å¾®è°ƒ

---

# RAG é«˜æ•ˆåº”ç”¨æŒ‡å— - AIèŠ±æœå±±

![](Pics/rag015.webp)

RAG åº”ç”¨çš„åŸºæœ¬æ¶æ„ï¼Œå¯ä»¥åˆ†ä¸ºç¦»çº¿å’Œåœ¨çº¿ä¸¤éƒ¨åˆ†
1. ç¦»çº¿ - å¯¹çŸ¥è¯†åº“æ–‡æ¡£è¿›è¡Œè§£æã€æ‹†åˆ†ã€ç´¢å¼•æ„å»ºå’Œå…¥åº“
2. åœ¨çº¿ - queryåˆ†æã€å¬å›æ£€ç´¢ã€è”ç½‘æœç´¢ã€é‡æ‹ã€queryå›ç­”

## æ–‡æ¡£æ™ºèƒ½è§£æ

ä» éç»“æ„åŒ–æ•°æ®(PDF, Word, PPT, Excel, å›¾åƒ, å›¾è¡¨) ä¸­ æå–å‡ºå†…å®¹

[ç‰ˆé¢åˆ†æé‚£äº›äº‹ - ä¸€äº›ä¼ ç»Ÿå®ç°](https://zhuanlan.zhihu.com/p/35910823)

Layout Analysis - å¸ƒå±€åˆ†æ
1. æ–‡æœ¬ - æ ‡é¢˜ã€å‰¯æ ‡é¢˜ã€æ­£æ–‡æ–‡æœ¬
   1. OCRå¼•æ“ - Optical Character Recognition
2. å›¾åƒ - å›¾ç‰‡ã€å›¾è¡¨ã€å…¬å¼
3. è¡¨æ ¼
   1. è¡¨æ ¼è¯†åˆ« - TSRï¼ŒTable Structure Recognition

ç›¸å…³æ¨¡å‹
1. [å¾®è½¯ LayoutLM å¾®è½¯äºšç ”é™¢](https://www.msra.cn/zh-cn/news/features/layoutlmv3)
   1. [Github](https://github.com/microsoft/unilm/tree/master/layoutlmv3)
   2. [è®ºæ–‡](https://arxiv.org/pdf/2204.08387)
2. å¾®è½¯ Table Transformer
3. [Donut æ¨¡å‹](https://arxiv.org/pdf/2111.15664v5)
4. [RAGFlow](https://github.com/infiniflow/ragflow)
   1. [æ£€ç´¢å¢å¼ºç”Ÿæˆå¼•æ“ RAGFlow æ­£å¼å¼€æº - InfoQ](https://www.infoq.cn/article/hjjm3kv620idoyyobtps)
   2. [RAGFlow å®˜ç½‘](https://ragflow.io/)
   3. [Github - RAGFlow](https://github.com/infiniflow/ragflow/blob/main/README_zh.md)
   4.
5. [Unstructured](https://github.com/Unstructured-IO/unstructured) - ä¸“é—¨ç”¨äºå¤„ç†éç»“æ„åŒ–æ•°æ®
6. [æ—·ä¸– OneChart](https://onechartt.github.io/)
7. [ç™¾åº¦ PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)


## æ–‡æœ¬åˆ†å— Text Chunking/Splitting

å°†é•¿æ–‡æœ¬åˆ†è§£ä¸ºè¾ƒå°çš„æ–‡æœ¬å—(åˆ†è§£æˆæ˜“äºç®¡ç†çš„éƒ¨åˆ†(ç« èŠ‚ã€æ®µè½ã€å¥å­))

å—è¢«åµŒå…¥ã€ç´¢å¼•ã€å­˜å‚¨ï¼Œç„¶åç”¨äºåç»­çš„æ£€ç´¢

ä¼˜ç‚¹
1. å¢åŠ å‡†ç¡®æ€§
2. æå‡æ€§èƒ½ - LLM åœ¨å¤„ç†è¿‡é•¿çš„æ–‡æœ¬æ—¶å¯èƒ½ä¼šé‡åˆ°æ€§èƒ½ç“¶é¢ˆ

æ–¹å¼
1. æŒ‰å¤§å°åˆ†å— - å°†æ–‡æœ¬æŒ‰å›ºå®šå­—ç¬¦æ•°æˆ–å•è¯æ•°è¿›è¡Œåˆ†å‰²
   1. æœ€ç›´æ¥ã€æœ€ç»æµ
   2. è¯­ä¹‰ä¸è¿è´¯ã€ä¸å®Œæ•´
2. ç‰¹å®šæ ¼å¼åˆ†å—
3. é€’å½’åˆ†å— - ä»¥ä¸€ç»„åˆ†éš”ç¬¦ä¸ºå‚æ•°ï¼Œä»¥é€’å½’çš„æ–¹å¼å°†æ–‡æœ¬åˆ†æˆæ›´å°çš„å—
4. è¯­ä¹‰åˆ†å—(Semantic Chunking) - åœ¨å¥å­ä¹‹é—´è¿›è¡Œåˆ†å‰²ï¼Œä½¿ç”¨ Embedding è¡¨å¾å¥å­ï¼Œç›¸ä¼¼çš„å¥å­ç»„åˆåœ¨ä¸€èµ·å½¢æˆå—ï¼Œä¿æŒå¥å­çš„é¡ºåº
5. å‘½é¢˜åˆ†å—(Propositional Chunking) - è¯­ä¹‰åˆ†å—çš„ä¸€ç§ï¼Œå°†å¥å­åˆ†è§£ä¸ºå‘½é¢˜(ä¸»è°“å®¾ï¼Œè¡¨è¾¾ä¸€ä¸ªå®Œæ•´æ€æƒ³çš„æœ€å°ä¿¡æ¯å•å…ƒ)


æ–‡æœ¬åˆ†å—å¹¶æ²¡æœ‰å›ºå®šçš„æœ€ä½³ç­–ç•¥ï¼Œé€‰æ‹©å“ªç§æ–¹å¼å–å†³äºå…·ä½“çš„éœ€æ±‚å’Œåœºæ™¯










---

# IBM Technology - Introductions

## How Large Language Models Work?

[How Large Language Models Work?](https://www.youtube.com/watch?v=5sLYAQS9sWQ)

GPT - generative pretrained transformer

instance(å®ä¾‹) of foundation model

self-supervised learning != unsupervised learning
1. self-supervised learning
   1. ä»æ•°æ®æœ¬èº«è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¸ä¾èµ–å¤–éƒ¨æä¾›çš„æ ‡ç­¾(é¢„æµ‹å›¾åƒçš„æŸä¸ªéƒ¨åˆ†ã€é¢„æµ‹å¥å­ä¸­é®æŒ¡çš„å•è¯)
   2. ä½†åˆ›å»ºäº†ä¸€ä¸ªç›‘ç£ä»»åŠ¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é¢„æµ‹æ•°æ®çš„æŸäº›æ–¹é¢æ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤º
2. unsupervised learning
   1. å‘ç°æ•°æ®ä¸­çš„éšè—ç»“æ„æˆ–æ¨¡å¼ï¼Œè€Œä¸æ˜¯ä»æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ 
      1. èšç±» - clustering - K-means & hierarchical
      2. é™ç»´ - dimension reduction - PCA & t-SNE
      3. å…³è”è§„åˆ™å­¦ä¹  - association rule learning

LLM = data + architecture(transformer) + training

business applicaitons
1. Customer Service Application - ChatBot
2. Content Creation - article / email / video
3. Software Development - code review

## Why Are There So Many Foundation Models?

[Why Are There So Many Foundation Models?](https://www.youtube.com/watch?v=QPQy7jUpmyA)

[IBM NASA Geospatial - HuggingFace](https://huggingface.co/ibm-nasa-geospatial)

HuggingFace - download open source foundation models

Foundation Model - è§„æ¨¡æ›´å¤§ã€åº”ç”¨æ›´å¹¿æ³› çš„é¢„è®­ç»ƒæ¨¡å‹(å¤šåŠŸèƒ½æ€§ & å¯é€‚åº”æ€§ & è¿ç§»å­¦ä¹ ï¼Œé€šè¿‡å°‘é‡çš„è°ƒæ•´æˆ–ç”šè‡³é›¶æ¬¡è°ƒæ•´ zero-shot learning å¤„ç†å„ç§ä»»åŠ¡)

Transformer - turn raw data to compressed representation & catch the data's basic structure

can be fine-tuned to perform different tasks



## What is Retrieval-Augmented Generation (RAG)?

[What is Retrieval-Augmented Generation (RAG)?](https://www.youtube.com/watch?v=T-D1OfcDW1M)

**RAG - Retrieval-Augmented Generation - æ£€ç´¢å¢å¼ºç”Ÿæˆ**

**Challenges**
1. no source
2. knowledge out of date

content source (Internet/Documents/Policies)

LLMs are instructed to pay attention to source data before respond

**Pros:**
1. less likely to hallucinate(å¹»è§‰)
2. less likely to leak data(æ•°æ®éš”ç¦»ã€è®¿é—®æ§åˆ¶)
3. know when to say "I don't know"

**Cons:**
1. if the retriever is bad -> user can't get an answer

## Why Large Language Models Hallucinate?

[Why Large Language Models Hallucinate?](https://www.youtube.com/watch?v=cfqtFvWOfg0)

plausible sounding nonsense

make up completely fabricated or contradictory statements

Different Granularity(é¢—ç²’åº¦)
1. sentense contradiction(contradict previous sentence)
2. prompt contradiction
3. factual contradiction
4. nonsensical/irrelevant information base hallucinations


Causes
1. data quality(not possible to cover all topics)
2. generation method
   1. beam search
   2. sampling
   3. maximum




## What is a Vector Database?

[What is a Vector Database?](https://www.youtube.com/watch?v=t9IDoenf-lo)

Database
1. SQL - ç»“æ„åŒ– æ•°æ® - tables
2. NoSQL(Not Only SQL) - éç»“æ„åŒ–/åŠç»“æ„åŒ– æ•°æ® - docs
3. Graph - å›¾ç»“æ„ æ•°æ® - nodes
4. Vector - å‘é‡ æ•°æ® - AI apps

2 concepts
1. Vector
   1. imagesã€textsã€documents -> represented in type of numerical value(array)
2. Embedding
   1. å¯¹è±¡ (å•è¯ã€å¥å­ã€å›¾åƒ) æ˜ å°„åˆ°å‘é‡ç©ºé—´
   2. flexibility(vary input)/scability





---

# Retrieval-Augmented Generation for Large Language Models: A Survey

[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)

[RAG-Survey - Github](https://github.com/Tongji-KGLLM/RAG-Survey)

[RAG-Survey - ReadPaper](https://readpaper.com/pdf-annotate/note?pdfId=2244532135023698688&noteId=2298121563230725632)


# Unifying RAG and long context LLMs

[Unifying RAG and long context LLMs - GoogleDocs](https://docs.google.com/presentation/d/1mJUiPBdtf58NfuSEQ7pVSEQ2Oqmek7F1i4gBwR6JDss/edit?pli=1#slide=id.g26c0cb8dc66_0_0)

[Unifying RAG and long context LLMs - PDF](./Paper/Unifying%20RAG%20and%20long%20context%20LLMs.pdf)

Context Window - ä¸Šä¸‹æ–‡çª—å£

T - trillion

Needle in a Haystack

recency bias æ¨¡å‹å€¾å‘äºè¿‡åˆ†å…³æ³¨æœ€è¿‘çš„tokensï¼Œè€Œå¿½è§†äº†æ›´è¿œå¤„çš„ã€å¯èƒ½åŒæ ·é‡è¦çš„ä»¤ç‰Œ

RoPE - Rotary Positional Embedding - æ—‹è½¬ä½ç½®ç¼–ç  - å¯¹äºè¿œè·ç¦»çš„æ³¨æ„åŠ›æœ‰å‡å°‘çš„å€¾å‘ å¯¼è‡´ recency bias



TODO

[å¤§æ¨¡å‹ragæŠ€æœ¯å¦‚æ­¤ä¹‹å¤šï¼Œå¦‚ä½•é€‰æ‹©ï¼Œå¤šçœ‹çœ‹è®ºæ–‡å’Œå®éªŒ](https://www.bilibili.com/video/BV1vD421T7aR/)





---

# RAG & KnowledgeGraph

## GraphRAG - Microsoft

[Project GraphRAG - LLM-Derived Knowledge Graphs](https://www.microsoft.com/en-us/research/project/graphrag/)

![](Pics/rag012.png)

[arxiv - From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130v1)

[ReadPaper - Paper & Note](https://readpaper.com/pdf-annotate/note?pdfId=4881783320383324161&noteId=2323244243193266176)

[WeChat æ–‡ç« ](https://mp.weixin.qq.com/s/JBQf8C-4KU8ot9G0-olSzA?poc_token=HDPWvGajAVZgIexn63pDnrb7e6NO4hjLV04azmLi)

[YouTube - GraphRAG: LLM-Derived Knowledge Graphs for RAG](https://www.youtube.com/watch?v=r09tJfON6kE&list=WL)

å¯¹äºå®è§‚é—®é¢˜ï¼Œéœ€è¦ éå†æ•´ä¸ªçŸ¥è¯†åº“ å¹¶æ€»ç»“ï¼ŒConnect the Dot æ‰¾åˆ°æ•£è½åœ¨å„å¤„çš„ä¿¡æ¯ï¼Œä¼ ç»Ÿ RAG æ•ˆæœä¸ç†æƒ³

ç›¸å½“äºæå‰å°†ä¿¡æ¯è¿›è¡Œ**æ•´ç†(Pre-Processing)**ï¼Œæ­å»º **çŸ¥è¯†å›¾è°±**(åŒæ—¶ä¹Ÿæ„å»ºäº†çŸ¥è¯†çš„å±‚æ¬¡ç»“æ„(Community Detection))ï¼Œé€šè¿‡æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œå¾ˆå¿«èƒ½å¤Ÿè·å–å…³è”ä¿¡æ¯

æ›´æ–°çŸ¥è¯†å›¾è°±å¯èƒ½æ¶ˆè€—èµ„æºè¿‡å¤š

ä½¿ç”¨ LLM(GPT) æ„å»º Graph, Graph ç”¨äº RAG

RAG
1. **excellent** at performing retrieval for highly specific facts (Vector similarity approach)
2. **poor** at performing any operation that require higher order reasoning and understanding

Graph
1. aggregate semantic concepts and derive holistic understanding of the whole sources
2. guide content discovery to provide the llm diverse comprehensive and topically relevant content

**Two Step Process**
1. indexing process on private data to create LLM-Derived Knowledge Graphs (LLM memory representation)
2. LLM Orchestration to utilize memory constructs in RAG operations

**Differentiators**
1. enhance search relevance
2. enable new scenarios that require large context

![](Pics/rag013.png)

Graph Machine Learning
1. semantic aggregation
2. hierarchy extraction







TODO

[å¤§æ¨¡å‹ç ”å‘æ ¸å¿ƒ - æ•°æ®å·¥ç¨‹ã€è‡ªåŠ¨åŒ–è¯„ä¼°åŠä¸çŸ¥è¯†å›¾è°±çš„ç»“åˆ - æ™ºæºç¤¾åŒº](https://hub.baai.ac.cn/view/28740)

[ä»ä¼ ç»ŸRAGåˆ°GraphRAG](https://www.bilibili.com/video/BV1bm41117XN/)


## LLM & KnowledgeGraph

çŸ¥è¯†å›¾è°± - ç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºæ–¹å¼ï¼Œèƒ½é€šè¿‡ç¬¦å·æ¨ç†äº§ç”Ÿ**å¯è§£é‡Šçš„ç»“æœ**ï¼Œå…·å¤‡ç²¾ç¡®å¯é çš„é¢†åŸŸçŸ¥è¯†

![](Pics/rag010.png)

[çŸ¥è¯†å›¾è°± + RAG](https://mp.weixin.qq.com/s/C5TY8wbrJLDb-jmLzGhbXg)

![](Pics/rag011.webp)

ä¸‰ç±»è·¯çº¿
1. åŸºäºçŸ¥è¯†å›¾è°±å¢å¼ºçš„å¤§æ¨¡å‹
2. åŸºäºå¤§æ¨¡å‹å¢å¼ºçš„çŸ¥è¯†å›¾è°±
3. å¤§æ¨¡å‹å’ŒçŸ¥è¯†å›¾è°±çš„èåˆ






---

# Advanced RAG Techniques : an Illustrated Overview

[Advanced RAG Techniques : an Illustrated Overview](https://medium.com/towards-artificial-intelligence/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)

[é«˜çº§ RAG æŠ€æœ¯ : å›¾è§£æ¦‚è§ˆ [è¯‘]](https://baoyu.io/translations/rag/advanced-rag-techniques-an-illustrated-overview)

## Vector Search - å‘é‡æœç´¢

[](../../Data/VectorDB/VectorDB.md#å¸¸ç”¨-å‘é‡æ•°æ®åº“)

## LLM äº§å“

LLM(**brain** for **RAG pipeline**)
1. [ChatGPT - OpenAI](https://openai.com/chatgpt) - Get answers. Find inspiration. Be more productive.
2. [Claude - Anthropic](https://www.anthropic.com/product) - a family of foundational AI models that can be used in a variety of applications
3. [Mixtral form Mistral](https://mistral.ai/news/mixtral-of-experts/) - A high quality Sparse Mixture-of-Experts
4. [Phi-2 from Microsoft](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)
5. [Llama2](https://huggingface.co/blog/zh/llama2) - a family of state-of-the-art open-access large language models
6. [OpenLLaMA - UC Berkelely](https://huggingface.co/openlm-research) - open source reproduction of Meta AIâ€™s LLaMA model trained on the RedPajama dataset
7. [Falcon](https://huggingface.co/tiiuae) - TII's flagship series of large language models, built from scratch using a custom data pipeline and distributed training library

## LLM-based pipelines & applications

**open source libraries for LLM-based pipelines & applications**
1. [LangChain ğŸ¦œï¸](https://python.langchain.com/docs/get_started/introduction/)
   ![](Pics/rag003.png)
2. [LlamaIndex ğŸ¦™](https://docs.llamaindex.ai/en/stable/) - a framework for building context-augmented LLM applications
   ![](Pics/rag004.png)
3. [Dify](https://dify.ai/zh) - **å¼€æºçš„** LLM åº”ç”¨å¼€å‘å¹³å°ã€‚æä¾›ä» Agent æ„å»ºåˆ° AI workflow ç¼–æ’ã€RAG æ£€ç´¢ã€æ¨¡å‹ç®¡ç†ç­‰èƒ½åŠ›ï¼Œè½»æ¾æ„å»ºå’Œè¿è¥ç”Ÿæˆå¼ AI åŸç”Ÿåº”ç”¨
   ![](Pics/rag005.png)
4. [BiSheng](https://bisheng.dataelem.com/) - ä¾¿æ·ã€çµæ´»ã€å¯é çš„ä¼ä¸šçº§å¤§æ¨¡å‹åº”ç”¨å¼€å‘å¹³å°
   ![](Pics/rag006.png)

çŸ¥è¯†åº“ æ¨ªè¯„

![](Pics/rag026.png)



## Naive RAG

![](Pics/rag007.webp)

**Vanilla RAG case**
1. split texts into chunks
2. **embed chunks into vectors** with some Transformer Encoder model
3. put all those vectors into an index
4. create **prompt** for LLM that tells the model to answers userâ€™s query(given the context found)

**Runtime**
1. **vectorize userâ€™s query** with the **same Encoder model**
2. execute search of this query vector against the index
3. find the top-k results
4. retrieve the corresponding text chunks from our database
5. feed them into the LLM prompt as context

Prompt engineering is the **cheapest** thing you can try to improve your RAG pipeline

[OpenAI - Prompt engineering(shares strategies and tactics for getting better results from large language models)](https://platform.openai.com/docs/guides/prompt-engineering)


## Advanced RAG

![](Pics/rag008.webp)

### 01 - Chunking & vectorization

Procedure
1. create an index of vectors(å‘é‡ç´¢å¼•), representing our document contents
2. in the runtime to search for the **least cosine distance**(ä½™å¼¦å€¼ç”¨æ¥è¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼æ€§)
3. query vector which corresponds to the closest semantic meaning

**least cosine distance**
**$$ \cos (x, y)
=\frac{x \cdot y}{|x| \cdot |y|}
=\frac{\sum_{i=1}^{n} x_{i} y_{i}}
{\sqrt{\sum_{i=1}^{n} x_{i}^{2}} \sqrt{\sum_{i=1}^{n} y_{i}^{2}}} $$**


**Chunking** - split documents in chunks without loosing meaning
1. ç›¸æ¯”äºå‡ é¡µæ–‡æœ¬çš„å¹³å‡å‘é‡ï¼Œä¸€å¥è¯æˆ–å‡ å¥è¯çš„å‘é‡æ›´èƒ½å‡†ç¡®åœ°ä»£è¡¨å…¶è¯­ä¹‰å«ä¹‰
2. size of the chunk depends on the embedding model and its capacity in tokens
3. [Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/)


**Vectorization**
1. embed chunks
2. search optimized models(ä¸ºæœç´¢ä¼˜åŒ–çš„æ¨¡å‹)
   1. [bge-large](https://huggingface.co/BAAI/bge-large-en-v1.5)
   2. [E5](https://huggingface.co/intfloat/multilingual-e5-large)
3. [Overall MTEB English leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Massive Text Embedding Benchmark
4. [FlagEmbedding - Github](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)



### 02 - Index

#### Vector store index

![](Pics/rag009.webp)

(omit the Encoder block)

**search index æœç´¢ç´¢å¼•**, store vectorized content

most naive implementation uses **flat index**(a brute force æš´åŠ› distance calculation)

vector index - optimized for efficient retrieval, using **Approximate Nearest Neighbour**(clustering, trees or HNSW algorithm)

[Vector Search - Hierarchical Navigable Small World (HNSW) graphs](https://www.pinecone.io/learn/series/faiss/hnsw/)

<center class='img'><img src="Pics/rag016.webp" width=50%></center>

LlamaIndex æ”¯æŒå¤šç§å‘é‡å­˜å‚¨ç´¢å¼•

#### Hierarchical indices

![](Pics/rag017.webp)

create two indices
1. summaries
2. document chunks

procedure
1. filter out the relevant docs by summaries
2. search inside this relevant group

#### Hypothetical Questions and HyDE

**Hypothetical Questions - å‡è®¾æ€§é—®é¢˜**
1. let LLM **generate a question for each chunk** and embed questions in vectors
2. at runtime performing query search against this index of question vectors

**HyDE**
1. ask an LLM to **generate a hypothetical response given the query**
2. use its vector along with the query vector to enhance search quality

#### Context enrichment

retrieve smaller chunks for better search quality(æ£€ç´¢æ›´å°çš„ä¿¡æ¯å—æ¥æé«˜æœç´¢è´¨é‡)

add up surrounding context for LLM to reason upon(ä¸ºå¤§è¯­è¨€æ¨¡å‹å¢åŠ æ›´å¤šå‘¨å›´è¯­å¢ƒä»¥ä¾¿å…¶è¿›è¡Œæ¨ç†)

##### Sentence Window Retrieval

![](Pics/rag018.webp)

each sentence is embedded(provides great accuracy)

extend the context window by **k sentences before and after** the retrieved sentence


##### Auto-merging Retriever (Parent Document Retriever)

![](Pics/rag019.webp)

documents are split into **smaller child chunks** referring to **larger parent chunks**

if more than n chunks in top k retrieved chunks are linked to the same parent node, replace the context fed to the LLM by this parent node

##### Fusion Retrieval or Hybrid Search

take into account both semantic similarity and keyword matching

![](Pics/rag020.webp)

take the best from
1. keyword-based old school search (è¯é¢‘ & å€’æ’ç´¢å¼•)
   1. TF-IDF (`Term Frequency` - `Inverse Document Frequency`)
   2. BM25 (ä¸€ç§æ”¹è¿›çš„åŸºäºæ¦‚ç‡æ¨¡å‹çš„æ£€ç´¢ç®—æ³•ï¼Œç»“åˆè¯é¢‘ã€é€†æ–‡æ¡£é¢‘ç‡ä»¥åŠæ–‡æ¡£é•¿åº¦ç­‰)
2. modern semantic or vector search


[Reciprocal Rank Fusion algorithm](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
1. properly combine the retrieved results with different similarity scores
2. re-ranking the retrieved results



### 03 - Reranking & filtering

final step before feeding our retrieved context to LLM

refine retrieval results through **filtering**, **re-ranking**, **transformation**


### 04 - Query transformations

modify user input in order to improve retrieval quality

![](Pics/rag021.webp)

LLM decompose complex query into several sub queries
1. Step-back prompting - uses LLM to **generate a more general query**(obtain a more general or high-level context)
2. Query Re-writing - uses LLM to **reformulate initial query**


### 05 - Reference citations

back reference sources
1. Insert referencing task into prompt
2. Match the parts of generated response to the original text chunks
   1. Fuzzy Citation Query




### 06 - Chat Engine

![](Pics/rag022.webp)

chat logic - taking into account the dialogue context

follow up **questions, anaphora(ç…§åº”ï¼Œeg:ä»£è¯), or arbitrary user commands relating to the previous dialogue context**

solved by **query compression technique** - taking chat context into account along with the user query
1. ContextChatEngine
   1. retrieve context relevant to userâ€™s query
   2. send **retrieved context** to LLM along with **chat history** from the memory buffer
2. CondensePlusContextMode
   1. in each interaction the chat history and last message are condensed into a new query
   2.  this query goes to the index


### 07 - Query Routing

Query routing is the step of **LLM-powered decision making** upon what to do next given the user query
1. summarize
2. perform search against some data index
3. try a number of different routes
4. synthesize their output in a single answer

Query routers are also used to select where to send user query
1. classic vector store
2. hierarchy of indices
3. graph DB
4. relational DB

Defining the query router includes **setting up the choices LLM can make**

selection of a routing option is performed with an LLM call


### 08 - Agents in RAG

provide an LLM, capable of reasoning, with a set of tools and a task to be completed

tools include functions like
1. code function
2. chat history
3. knowledge storage
4. document uploading interface
5. function calling API(external API) - capabilities to convert natural language into API calls to external tools or database queries
6. other agents

LLM chaining idea

![](Pics/rag023.webp)
1. Each **document agent** has two tools â€” a vector store index and a summary index
2. For the **top agent**, all document agents are tools

a lot of routing decisions made by each involved agent

**Drawback**
1. slow due to multiple back and forth iterations with the LLMs inside our agents
2. **LLM call** is always the longest operation in a RAG pipeline
3. **search** is optimized for speed by design



### 09 - Response synthesizer

the final step of RAG pipeline

generate an answer based on all the context retrieved

more sophisticated options involving multiple(å¤šæ¬¡) LLM calls to refine retrieved context and generate a better answer

main approaches
1. iteratively refine the answer
2. summarize the retrieved context
3. generate multiple answers (based on different context chunks) and summarize


### 10 - Encoder and LLM fine-tuning


2 DL models
1. Encoder - responsible for **embeddings quality** and thus **context retrieval quality**
2. LLM - responsible for the best usage of the provided context to answer user query

LLM is a good few shot learner

FineTuning
1. Encoder fine-tuning
2. Ranker fine-tuning
3. LLM fine-tuning
   1. OpenAI started providing LLM fine-tuning API
   2. LlamaIndex has a tutorial on fine-tuning GPT-3.5-turbo in RAG setting to distillè’¸é¦ some of the GPT-4 knowledge

### 11 - Evaluation

frameworks
1. answer relevance
2. answer groundedness - æ ¹æ®
3. faithfulness - çœŸå®æ€§
4. retrieved context relevance

### 12 - Conclusion

other things to consider
1. web search
2. agent architectures
3. LLMs Long-term memory
4. **speed**





---

# 12 RAG Pain Points and Proposed Solutions-Solving the core challenges of Retrieval-Augmented Generation

[12 RAG Pain Points and Proposed Solutions](https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c)

[12 ä¸ª RAG ç—›ç‚¹å’Œå»ºè®®çš„è§£å†³æ–¹æ¡ˆ (ä¸­æ–‡ç¿»è¯‘)](https://zhuanlan.zhihu.com/p/681351068)

![](Pics/rag001.webp)

Pain Point
1. **Missing Content (å†…å®¹ç¼ºå¤±)**
   1. provides a **plausible but incorrect answer** when **the actual answer is not in the knowledge base**, rather than stating it doesnâ€™t know
   2. users receive misleading information, leading to frustration
   3. **SOLUTION**
      1. **Clean your data** - Garbage in, garbage out, such as containing conflicting information. Clean data is the prerequisite for any RAG pipeline
      2. **Better prompting** - By instructing the system with prompts : "Tell me you donâ€™t know if you are not sure of the answer", encourage the model to acknowledge its limitations and communicate uncertainty
2. **Missed the Top Ranked Documents (é”™è¿‡æ’åé å‰çš„æ–‡æ¡£)**
   1. essential documents may not appear in the top results returned by the systemâ€™s retrieval component
   2. correct answer is overlooked, causing the system to fail to deliver accurate responses
   3. question is in the document but did not rank highly enough to be returned to the user
   4. **SOLUTION**
      1. **Hyperparameter tuning** for **chunk_size**(åˆ†å—å¤§å°) and **similarity_top_k**(ç›¸ä¼¼åº¦æœ€é«˜çš„Kä¸ª) - manage the efficiency and effectiveness of the data retrieval process, impact the **trade-off** between **computational efficiency** and the **quality of retrieved information**
      2. **Reranking retrieval results** - evaluate and enhance retriever performance using various **embeddings and rerankers** & finetune **a custom reranker**
3. **Not in Context - Consolidation Strategy Limitations (ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­-æ•´åˆç­–ç•¥çš„å±€é™æ€§)**
   1. Documents with the answer were retrieved from the database but **did not make it into the context for generating an answer**, a consolidation process takes place to retrieve the answer
   2. **SOLUTION**
      1. Tweak retrieval strategies
      2. Finetune embeddings
4. **Not Extracted (æœªæå–)**
   1. system struggles to extract the correct answer from the provided context, especially when overloaded with information
   2. Key details are missed, compromising the quality of responses
   3. occurs when there is too much noise or contradicting information in the context
   4. **SOLUTION**
      1. Clean your data
      2. Prompt Compression
      3. LongContextReorder - the best performance typically arises when crucial data is positioned at the start or conclusion of the input context
5. **Wrong Format (æ ¼å¼é”™è¯¯)**
   1. an instruction to extract information in a specific format(table & list) is overlooked
   2. **SOLUTION**
      1. Better prompting - Clarify the instructionsã€Simplify the request and use keywordsã€Give examplesã€Iterative prompting and asking follow-up questions
      2. Output parsing - to provide â€œparsingâ€ for LLM outputsã€provide formatting instructions
      3. Pydantic programs - serves as a versatile framework that converts an input string into a structured Pydantic object
      4. OpenAI JSON mode - enable JSON mode for the response
6. **Incorrect Specificity (ç‰¹å¾ä¸æ˜æ˜¾)**
   1. responses may lack the necessary detail or specificity, often requiring follow-up queries for clarification
   2. answers may be too vague or general, failing to meet the userâ€™s needs
   3. **SOLUTION**
      1. Advanced retrieval strategies
         1. small-to-big retrieval - å…ˆä»è¾ƒå°ã€è¾ƒç²¾ç¡®çš„æ•°æ®é›†å¼€å§‹æœç´¢ï¼Œå¦‚æœåœ¨è¿™äº›æ•°æ®é›†ä¸­æ‰¾ä¸åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œå°±é€æ¸æ‰©å±•åˆ°æ›´å¤§çš„æ•°æ®é›†
         2. sentence window retrieval - åŸºäºæ–‡æœ¬çª—å£çš„æ£€ç´¢ç­–ç•¥ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦ä»å¤§é‡æ–‡æœ¬ä¸­æå–å…·ä½“å¥å­æˆ–çŸ­æ–‡æœ¬ç‰‡æ®µçš„åœºæ™¯ï¼Œæ£€ç´¢ç³»ç»Ÿä¼šæ ¹æ®æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡æˆ–å…³é”®è¯ï¼Œç¡®å®šç›¸å…³å¥å­å‘¨å›´çš„â€œçª—å£â€èŒƒå›´ï¼Œå¹¶ä»è¿™äº›çª—å£ä¸­æå–ä¿¡æ¯
         3. recursive retrieval - è¿­ä»£çš„æ£€ç´¢æ–¹æ³•ï¼Œå®ƒåœ¨åˆæ¬¡æ£€ç´¢åä½¿ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯æ¥ç²¾ç»†åŒ–å’Œæ”¹è¿›åç»­çš„æœç´¢æŸ¥è¯¢
7. **Incomplete (ä¸å®Œæ•´)**
   1. partial responses arenâ€™t wrong; however, donâ€™t provide all the details, despite the information being present and accessible within the context
   2. **SOLUTION**
      1. Query transformations - add a query understanding layer before actually querying the vector store
8. **Data Ingestion Scalability (æ•°æ®æ‘„å–å¯æ‰©å±•æ€§)**
   1. the system struggles to efficiently manage and process large volumes of data
   2. lead to performance bottlenecks and potential system failure
   3. cause prolonged ingestion time, system overload, data quality issues, and limited availability
   4. **SOLUTION**
      1. Parallelizing ingestion pipeline - ingestion pipeline parallel processing
9. **Structured Data QA (ç»“æ„åŒ–æ•°æ® QA)**
   1. accurately interpreting user queries to retrieve relevant structured data
   2. especially with complex or ambiguous queries, inflexible text-to-SQL
   3. **SOLUTION** of LlamaIndex
      1. Chain-of-table Pack
      2. Mix-Self-Consistency Pack
10. **Data Extraction from Complex PDFs (å¤æ‚ PDF ä¸­çš„æ•°æ®æå–)**
    1. extract data from complex PDF documents(embedded tables)
    2. **SOLUTION**
       1. Embedded table retrieval - use [Unstructured.io](Unstructured.io) to parse out the embedded tables from an HTML document(use **pdf2htmlEX** to convert the PDF to HTML), build a node graph, use recursive retrieval to index/retrieve tables based on the user question
11. **Fallback Model(s) (åå¤‡æ¨¡å‹)**
    1. rate limit errors with OpenAIâ€™s models
    2. a fallback model(s) as the backup in case your **primary model malfunctions**
    3. **SOLUTION**
       1. Neutrino router - a collection of LLMs to which you can route queries
       2. OpenRouter - a unified API to access any LLM, finds the lowest price for any model and offers fallbacks in case the primary host is down
12. **LLM Security (LLMå®‰å…¨æ€§)**
    1. combat **prompt injection**, handle **insecure outputs**, prevent **sensitive information disclosure**
    2. **SOLUTION**
       1. Guard - classify content for LLMs by examining both the inputs (through **prompt classification**) and the outputs (via **response classification**), produce text outcomes that determine whether a specific prompt or response is considered safe or unsafe

