# 3B1B - Neural Networks

[3B1B - Neural Networks - YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

## Table of Contents


# 01 - Deep Learning

[Chapter 01 - Deep Learning](https://www.youtube.com/watch?v=aircAruvnKk)

Convolutional Neural Network - good for Image Recognition

Long-Short Term Memory Network - good for Speech Recognition

Plain Vanilla = Multilayer Perceptron

<img src="Pics/3b1b001.png" width=450>

Neuron hold a number(Activation 激励值，与 input 有关)，本质是一个 function(input 是上一次全部 neuron 值，输出是 0-1 的值)

Hidden Layers 可能的 工作原理 (pixel -> edge -> pattern -> digit，只是为了方便理解)
1. Hidden Layer 第一层 存储低维信息(小特征)
2. Hidden Layer 第二层 存储高维信息(大特征)
   1. <img src="Pics/3b1b003.png" width=500>
3. <img src="Pics/3b1b002.gif" width=600>

其他 模态的 Recognition Problem 也类似
1. <img src="Pics/3b1b004.png" width=600>

将上一层信息 使用 weighted sum + bias 得到下一层信息，下一层 Neuron 表明 某种 Pattern 得分，权重可以使用一个掩膜(矩阵)表示
1. <img src="Pics/3b1b011.png" width=600>
2. 每一个后续的 neuron 都和之前的 neuron 全连接，也就是每一个 后续 neuron 都对应了一个独自的 mask(同时也有 一个 bias)
3. <img src="Pics/3b1b017.png" width=600>

需要将 weighted sum + bias 的值，即 Neuron的值(Activation) 限制在 `[-1, 1]`
1. 需要 从实数轴 `[-∞，+∞]` 映射到 `[-1, 1]`
2. Sigmoid Function $\sigma$ (Slow Learner)
   1. <img src="Pics/3b1b005.png" width=500>
   2. $$\sigma(x) = \frac{1}{1+e^{-x}}$$
3. ReLU (Rectified Linear Unit)
   1. <img src="Pics/3b1b009.png" width=500>
   2. $$ReLU(a) = max(0, a)$$

Bias 可以理解为 给 Neuron 设定的 Activate 的 Threshold

参数量计算
1. <img src="Pics/3b1b006.png" width=600>

Learning 可以看做 寻找 最合适的 参数(Weights & Biases)

矩阵表示法
1. <img src="Pics/3b1b007.png" width=700>
2. <img src="Pics/3b1b008.png" width=700>

整个 Network 就是一个复杂的 Function

---

# 02 - Gradient Descent (Learning Find the Parameters)

[Chapter 02 - Gradient Descent](https://www.youtube.com/watch?v=IHZwWFHWa-w)

Adjust Parameters to Improve Performance on **Training** Data (获取合适的 weight & bias)

**Test** the Network that it Never Seen Before (hope it can generalize beyond the training data)

**MNIST Database** - LeCun, Cortes, Burges
1. hand-written digit recognition
2. <img src="Pics/3b1b010.png" width=300>

相当于 Finding the Minima of a Function(Loss)

最开始 initialize weights & bias randomly

<img src="Pics/3b1b013.png" width=550>

cost function - evaluate how bad the output is
1. 对于 Training Data, cost 是全部 train dataset 的共同结果
2. dataset 是 固定的
3. weight & bias 是 变量

<img src="Pics/3b1b012.png" width=550>

tell the network how to change the weight & bias (minimize the cost)

对于 有显式表达式的 函数，可以直接写出 minima，但是实际情况是 并不知道 表达式

使用迭代式的算法并不能保证得到全局最小值

<img src="Pics/3b1b014.png" width=350>

最好可以让迭代步长和斜率成正比，当趋向于最小值时，斜率小，步长小，避免 over-shotting

对于多变量微积分，函数梯度给出了最陡的上升方向，使得函数增加的最快方向，走反方向可以得到最陡的下降方向

<img src="Pics/3b1b015.png" width=400>

负梯度 包含两个信息
1. input vector 分量是应该 增加 or 减小
2. 哪个分量的变化 更加重要(在当前input附近)，或者说 cost function 对 分量的 敏感性，也可以认为是 梯度下降的 性价比

<img src="Pics/3b1b016.png" width=500>

对于 random input 可能会给出错误结果

<img src="Pics/3b1b018.png" width=500>

neural network 可能不仅仅是在记忆(对于 randomly-labeled data 学习的很慢，randomly-labeled data 的 cost function 没有规律)

<img src="Pics/3b1b019.png" width=500>

---

# 03 - Back Propagation

[Chapter 03 - Back Propagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U)

由于 cost function 会 average over all training examples，因此 调整 梯度下降的 每一步 也会 基于 all training examples

调整参数的方法(单独一层)
1. 改变 bias
2. 改变 weight (in proportion to 上一层 neuron output/activation)
   1. weight 是 对应 output neuron 的，不需要 进行 neuron 间的 average
3. 改变 上一层的 neuron output/activation (in proportion to weight)
   1. 但是实际上并不能直接进行改变，因为是由再之前层的 weight & bias 决定的
   2. 同时需要考虑所有 output layer 对于 上一层 neuron output/activation 的改变期望(希望全部 output 同时 optimize)，因为 上一层 neuron output/activation 是 **shared 共用的**
   3. <img src="Pics/3b1b020.png" width=500>

记得需要 average over all training
1. <img src="Pics/3b1b021.png" width=500>


## Stochastic Gradient Descent 随机梯度下降

如果梯度下降每一步 都使用 全部训练样本average 计算会花费大量时间

训练样本打乱，分成多组 mini-batch，用于近似 all dataset

mini-batch 的每一步 计算 step

<img src="Pics/3b1b022.png" width=600>

没有 使用 all data 那么准确，但是速度更快


---

# 04 - Back Propagation Calculus

[Chapter 04 - Back Propagation Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)

考虑简单情况

<img src="Pics/3b1b023.png" width=600>

Chain Rule

<img src="Pics/3b1b024.png" width=350>

在 all data 上做 average

<img src="Pics/3b1b025.png" width=300>

考虑复杂情况
1. 对于 weight/bias 求导 chain rule 公式不变
   1. <img src="Pics/3b1b026.png" width=650>
2. 对于 activation 求导 chain rule 公式有些许变化，前面的 neuron activation 值会被 share
   1. <img src="Pics/3b1b027.png" width=650>



---

# 05 - Large Language Models explained briefly

[Chapter 05 - Large Language Models explained briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs)

接收文本，合理预测下一个单词

不是肯定的预测一个词，而是为所有可能的下一个词一个概率

<img src="Pics/3b1b028.png" width=500>

通过 System Prompt 可以使得 相同的模型 得到不同的输出

input 去掉最后一个词，用预测的最后一个值和真实值比较，通过反向传播来调整

<img src="Pics/3b1b029.png" width=500>

训练 LLM 所需的 计算量很大

Steps:
1. Pre-Training (have room for fine-tuning in specific task)
2. RLHF (Reinforcement Learning with Human Feedback)

Before 2017, 大多数 language model 是一个词一个词处理文本 (并非所有 language model 都能轻松实现 并行)

Google 2017 - Attention Is All You Need - **Transformer**

Transformer 不是从头到尾阅读文字，而是 并行的吸收信息

首先需要将文字转为数字，也就是 编码

<img src="Pics/3b1b030.png" width=500>

通过 Attention，使得 编码的数字列表 可以 进行交互，根据周围语境，完善编码含义 (并行进行)

同时 FeedForward Neural Network 增加 capacity to store more patterns about language

<img src="Pics/3b1b031.png" width=500>

最终，对于序列中的最后一个向量执行 一个函数，用于预测 下一个单词

<img src="Pics/3b1b032.png" width=500>


---

# 06 - Transformers

[Chapter 06 - Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)

GPT = Generative Pre-trained **Transformer**

可以使用 Transformer 构建很多不同类型的模型
1. voice-to-text
2. text-to-voice
3. text-to-image
4. translation/interpretation

预测的形式 : probability distribution

## High Level Preview of Transformer

High Level Preview
1. Embedding
2. Attention
3. MLPs
4. UnEmbedding

<img src="Pics/3b1b033.png" width=600>

整体流程 General Idea
1. input 被拆分为 Tokens
   1. 对于文本，是 单词、单词的一小部分、字符组合
      1. <img src="Pics/3b1b034.png" width=400>
   2. 对于其他模态，可能是 图像的小区域、声音的小片段
2. 每个 Token 对应到一个 vector (encode the meaning of the piece)
   1. <img src="Pics/3b1b035.png" width=600>
   2. 如果看做是 高维空间中的坐标，相同含义的 token 在空间中也相近
      1. <img src="Pics/3b1b036.png" width=300>
3. sequence of vectors 向量序列 经过 **Attention Block**，相互**交流**，并根据彼此信息更新自身值
   1. attention block 用于确定 上下文中 哪些词 对更新其他词的意思有关，以及 如何更新词义(vector)
   2. <img src="Pics/3b1b037.png" width=500>
4. 经过 Multilayer Perceptron / FeedForward Layer，此期间 向量并不交流，而是并行的进行处理
5. 重复进行多次 Attention Block & MLP
6. 直到最后，文章的核心意义融入到 sequence 中的 最后一个 vector
   1. <img src="Pics/3b1b038.png" width=600>


## Premise of Deep Learning

machine learning 的 本质是 构建具有 Tunable Parameters 可调节参数 的结构，然后通过大量的实例 input/output 学习，而非编写固定的程序来做特定任务

<img src="Pics/3b1b039.png" width=600>

GPT-3 内部

<img src="Pics/3b1b040.png" width=500>

weight define the model & model process the data

<img src="Pics/3b1b041.png" width=500>

## Word Embedding

model 有一个 predefined vocabulary(针对 tokens)，并对应 Embedding Matrix (token 对应的 初始值)，learned based on data

<img src="Pics/3b1b042.png" width=600>

GPT-3
1. 50257 tokens
2. 12288 dimensions

Embedding 比 Transformer 早提出

<img src="Pics/3b1b043.png" width=400>

<img src="Pics/3b1b044.png" width=400>

向量 dot product 可用于判断 对齐程度

起初 从 embedding matrix 中获取的 vector 仅能代表 token 含义，不含 上下文信息

但 sequence of vectors 中 并不仅仅包含 token 本身，还带有 位置信息，此外 还会不断添加其他的上下文信息，最终指向一个新位置

<img src="Pics/3b1b045.png" width=400>

需要 model 能够高效融合 上下文信息

网络每次只能处理一定数量的 vector，该数量称为 上下文大小 context size (GPT-3 的 context size = 2048)

上下文大小 限制 Transformer 预测下一个 token 的过程中 可以容纳的 文本量

## UnEmbedding

使用 UnEmbedding Matrix，将 最后一个 vector 映射到 vocabulary 大小的 vector (logits)
1. 行数 = vocabulary 数
2. 列数 = embedding dimension 数


再通过 softmax 函数，将这些值转换为 概率分布 probability distribution

<img src="Pics/3b1b046.png" width=700>

虽然最后一步的 sequence of vectors 中 每一个 都包含丰富的 上下文信息，但是 利用 最终层的每一个向量 预测相邻后一个token(immediately after) 更加高效


## Softmax with Temperature

根据 probability distribution 定义，每个值 处于 0-1 之间，并且 sum = 1

$$\text{softmax} = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$$

通过 指数函数 转为非负，求和 作为 分母

<img src="Pics/3b1b047.png" width=600>

$$\text{softmax with Temperature} = \frac{e^{z_i/T}}{\sum_{j=1}^K e^{z_j/T}}$$

<img src="Pics/3b1b048.png" width=600>

Temperature 越大，不同 input 对应的 output 占比差距变小，使得分布更加均匀 (较小的数值会获得更多权重)

Temperature = 0，对应模型 必定选择 预测概率最大的 token (100%)

Temperature 高，使得模型能选择概率低的词，creative but risky

P.S. : logits 是一个向量，下一步通常被投给 softmax/sigmoid

---

# 07 - Attention in Transformers

[Chapter 07 - Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)







---

# 08 - How might LLM Store Facts

[Chapter 08 - How might LLM Store Facts](https://www.youtube.com/watch?v=9-Jl0dxWQs8)






---


#

[](https://www.youtube.com/watch?v=KJtZARuO3JY)



