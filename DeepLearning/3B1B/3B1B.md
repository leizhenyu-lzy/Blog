# 3B1B - Neural Networks

[3B1B - Neural Networks - YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

## Table of Contents


# 01 - Deep Learning

[Chapter 01 - Deep Learning](https://www.youtube.com/watch?v=aircAruvnKk)

Convolutional Neural Network - good for Image Recognition

Long-Short Term Memory Network - good for Speech Recognition

Plain Vanilla = Multiple Perceptron

<img src="Pics/3b1b001.png" width=500>

Neuron hold a number(Activation)

Maybe
1. Hidden Layer 第一层 存储低维信息(小特征)
2. Hidden Layer 第二层 存储高维信息(大特征)
   1. <img src="Pics/3b1b003.png" width=500>
3. <img src="Pics/3b1b002.gif" width=500>


其他 Recognition Problem 也类似
1. <img src="Pics/3b1b004.png" width=700>

将上一层信息 使用 weighted sum + bias 得到下一层信息，下一层 Neuron 表明 某种 Pattern 得分

需要将 weighted sum + bias 的值，即 Neuron的值(Activation) 限制在 `[-1, 1]`
1. 需要 从实数轴 `[-∞，+∞]` 映射到 `[-1, 1]`
2. Sigmoid Function $\sigma$ (Slow Learner)
   1. <img src="Pics/3b1b005.png" width=500>
   2. $$\sigma(x) = \frac{1}{1+e^{-x}}$$
3. ReLU
   1. <img src="Pics/3b1b009.png" width=500>
   2. $$ReLU(a) = max(0, a)$$
4. $activation = \sigma(\sum{w_i a_i})$

Bias 可以理解为 给 Neuron 设定的 Activate 的 Threshold

参数量计算
1. <img src="Pics/3b1b006.png" width=700>

Learning 可以看做 寻找 最合适的 参数(Weights & Biases)

矩阵表示法
1. <img src="Pics/3b1b007.png" width=700>
2. <img src="Pics/3b1b008.png" width=700>

整个 Network 就是一个复杂的函数

---


# 02 - Gradient Descent (Learning Find the Parameters)

[Chapter 02 - Gradient Descent](https://www.youtube.com/watch?v=IHZwWFHWa-w)

Adjust Parameters to Improve Performance on Training Data

Test the Network that it Never Seen Before

**MNIST Database** - LeCun, Cortes, Burges
1. <img src="Pics/3b1b010.png" width=300>

相当于 Finding the Minima of a Function(Loss)





# 03 - Back Propagation

[Chapter 03 - Back Propagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U)



# 04 - Back Propagation Calculus

[Chapter 04 - Back Propagation Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)



# 05 - Visual Intro to Transformers

[Chapter 05 - Visual Intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)




# 06 - Attention in Transformers

[Chapter 06 - Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)



# 07 - How might LLM Store Facts

[Chapter 07 - How might LLM Store Facts](https://www.youtube.com/watch?v=9-Jl0dxWQs8)






