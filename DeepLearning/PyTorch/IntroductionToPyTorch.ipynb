{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Pics/torch001.png)\n",
    "\n",
    "# <a id='toc1_'></a>Intro to PyTorch    [&#8593;](#toc0_)\n",
    "\n",
    "[PyTorch 官网]((https://pytorch.org/))\n",
    "\n",
    "[PyTorch Tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html#)\n",
    "\n",
    "[torch.utils.data](https://pytorch.org/docs/stable/data.html#module-torch.utils.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [PyTorch   ](#toc1_)    \n",
    "- [Learn the Basics](#toc2_)    \n",
    "  - [0. Quickstart](#toc2_1_)    \n",
    "    - [Working with data](#toc2_1_1_)    \n",
    "    - [Creating Models](#toc2_1_2_)    \n",
    "    - [Optimizing the Model Parameters](#toc2_1_3_)    \n",
    "    - [Saving & Loading Models](#toc2_1_4_)    \n",
    "  - [1. Tensors](#toc2_2_)    \n",
    "    - [Tensor Initialization](#toc2_2_1_)    \n",
    "    - [Tensor Attributes](#toc2_2_2_)    \n",
    "    - [Tensor Operations](#toc2_2_3_)    \n",
    "    - [Bridge with NumPy](#toc2_2_4_)    \n",
    "  - [2. Datasets and DataLoaders](#toc2_3_)    \n",
    "    - [Loading a Dataset](#toc2_3_1_)    \n",
    "    - [Iterating and Visualizing the Dataset](#toc2_3_2_)    \n",
    "    - [Creating a Custom Dataset for your files](#toc2_3_3_)    \n",
    "    - [Preparing your data for training with DataLoaders](#toc2_3_4_)    \n",
    "    - [Iterate through the DataLoader](#toc2_3_5_)    \n",
    "  - [3. Transforms](#toc2_4_)    \n",
    "  - [4. Build Model](#toc2_5_)    \n",
    "    - [Get Device for Training](#toc2_5_1_)    \n",
    "    - [Define the Class](#toc2_5_2_)    \n",
    "    - [Model Layers](#toc2_5_3_)    \n",
    "    - [Model Parameters](#toc2_5_4_)    \n",
    "  - [5. Automatic Differentiation with torch.autograd](#toc2_6_)    \n",
    "    - [Computing Gradients](#toc2_6_1_)    \n",
    "    - [Disabling Gradient Tracking](#toc2_6_2_)    \n",
    "    - [More on Computational Graphs](#toc2_6_3_)    \n",
    "    - [Optional Reading: Tensor Gradients and Jacobian Products](#toc2_6_4_)    \n",
    "  - [6. Optimizing Model Parameters](#toc2_7_)    \n",
    "    - [Hyperparameters](#toc2_7_1_)    \n",
    "    - [Optimization Loop](#toc2_7_2_)    \n",
    "    - [Loss Function](#toc2_7_3_)    \n",
    "    - [Optimizer](#toc2_7_4_)    \n",
    "    - [Full Implementation](#toc2_7_5_)    \n",
    "  - [7. Save, Load and Use Model](#toc2_8_)    \n",
    "    - [Saving and Loading Model Weights](#toc2_8_1_)    \n",
    "    - [Saving and Loading Models with Shapes](#toc2_8_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Learn the Basics](#toc0_)\n",
    "\n",
    "[Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[0. Quickstart](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Working with data](#toc0_)\n",
    "\n",
    "PyTorch has two primitives to work with data:\n",
    "1. torch.utils.data.**DataLoader**  - DataLoader wraps an iterable around the Dataset\n",
    "2. torch.utils.data.**Dataset**     - Dataset stores the samples and their corresponding labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries - (all of which include datasets)\n",
    "1. TorchText\n",
    "2. TorchVision\n",
    "3. TorchAudio\n",
    "\n",
    "The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO\n",
    "\n",
    "[Full List of datasets](https://pytorch.org/vision/stable/datasets.html)\n",
    "\n",
    "Every TorchVision Dataset includes two arguments to modify the samples and labels respectively\n",
    "1. transform - 用于对样本进行变换\n",
    "   1. 可以包含多种不同的操作，如缩放、裁剪、归一化等\n",
    "   2. 目的是通过这些预处理步骤来增强模型的泛化能力或使模型训练更为高效\n",
    "   3. 可以使用transforms.Compose来组合多个图像变换操作\n",
    "2. target_transform - 用于对标签进行变换\n",
    "   1. 如进行编码或转换成某种特定的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the **Dataset** as an argument to **DataLoader**. \n",
    "\n",
    "This wraps an iterable over our dataset, and supports\n",
    "1. automatic batching\n",
    "2. sampling\n",
    "3. shuffling\n",
    "4. multi-process data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28]) torch.float32\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Creating Models](#toc0_)\n",
    "\n",
    "To define a neural network in PyTorch, we **create a class that inherits from nn.Module**\n",
    "1. define the layers of the network in the **\\_\\_init\\_\\_ function**\n",
    "2. specify how data will pass through the network in the **forward function**\n",
    "\n",
    "To accelerate operations in the neural network, we move it to the GPU or MPS if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "linear_relu_stack.0.weight: 401408 参数\n",
      "linear_relu_stack.0.bias: 512 参数\n",
      "linear_relu_stack.2.weight: 262144 参数\n",
      "linear_relu_stack.2.bias: 512 参数\n",
      "linear_relu_stack.4.weight: 5120 参数\n",
      "linear_relu_stack.4.bias: 10 参数\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 确保父类的构造函数也被正确地调用\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),  # 根据前面的 shape\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # numel() 是一个张量的方法，用于返回张量中所有元素的数量\n",
    "    print(f\"{name}: {param.numel()} 参数\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[Optimizing the Model Parameters](#toc0_)\n",
    "\n",
    "**loss function** & **optimizer**\n",
    "\n",
    "优化器 optimizer 在 PyTorch 中用于存储和更新模型的参数\n",
    "\n",
    "当创建一个优化器实例时，需要将模型参数传递给它，这样优化器就知道它需要优化哪些参数\n",
    "\n",
    "模型的参数是通过模型的 `.parameters()` 方法获取的，该方法返回模型所有可训练参数的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches)\n",
    "\n",
    "and backpropagates the prediction error to adjust the model’s parameters\n",
    "\n",
    "also check the model’s performance against the test dataset to ensure it is learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)  # 构成了计算图的一部分，每个操作都有可能添加新的节点和边到图中。此时，pred 是计算图中的一个节点，它保存了从输入 X 到输出 pred 的整个计算路径。\n",
    "        loss = loss_fn(pred, y)  # 损失函数接收模型的预测 pred 和真实标签 y 作为输入，计算出一个损失值 loss。\n",
    "                                 # 这一步将损失值与预测值 pred（以及通过 pred 间接与模型输入 X）联系起来，构成了计算图的最终输出节点。损失值 loss 反映了模型预测和真实标签之间的差异。\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()  # 损失函数 loss 和计算图之间的关系是通过模型预测 pred 来建立的\n",
    "        optimizer.step()  # 更新模型的参数。基于之前计算的梯度，这个步骤会调整模型的参数以最小化损失函数。\n",
    "        optimizer.zero_grad()  # 清零梯度, PyTorch 默认会累加梯度，如果不手动清零，那么下次调用 .backward() 时，新计算的梯度会和旧的梯度累加起来\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # :>7f 是格式说明符，: 表示开始格式说明符，> 表示右对齐，7f 表示总宽度为7的浮点数，包括小数点和小数部分，确保输出具有一致的宽度，方便阅读。\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()  # item 将张量转化为标量\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            # type 将 bool 张量 转为 浮点 张量\n",
    "\n",
    "            # print(pred.argmax(1))\n",
    "            # print(y)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    # 0.1f 表示格式化为浮点数，保留一位小数。\n",
    "    # 8f 表示格式化为浮点数，保留八位数（总长度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.295696  [   64/60000]\n",
      "loss: 2.288214  [ 6464/60000]\n",
      "loss: 2.260324  [12864/60000]\n",
      "loss: 2.257531  [19264/60000]\n",
      "loss: 2.244630  [25664/60000]\n",
      "loss: 2.207962  [32064/60000]\n",
      "loss: 2.221496  [38464/60000]\n",
      "loss: 2.179507  [44864/60000]\n",
      "loss: 2.175304  [51264/60000]\n",
      "loss: 2.147292  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 2.137548 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.147805  [   64/60000]\n",
      "loss: 2.144528  [ 6464/60000]\n",
      "loss: 2.069092  [12864/60000]\n",
      "loss: 2.092287  [19264/60000]\n",
      "loss: 2.046380  [25664/60000]\n",
      "loss: 1.970410  [32064/60000]\n",
      "loss: 2.018287  [38464/60000]\n",
      "loss: 1.925674  [44864/60000]\n",
      "loss: 1.931690  [51264/60000]\n",
      "loss: 1.863262  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 1.853597 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.887406  [   64/60000]\n",
      "loss: 1.867226  [ 6464/60000]\n",
      "loss: 1.726344  [12864/60000]\n",
      "loss: 1.784328  [19264/60000]\n",
      "loss: 1.668724  [25664/60000]\n",
      "loss: 1.610135  [32064/60000]\n",
      "loss: 1.662283  [38464/60000]\n",
      "loss: 1.548992  [44864/60000]\n",
      "loss: 1.576392  [51264/60000]\n",
      "loss: 1.481444  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.485861 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.549197  [   64/60000]\n",
      "loss: 1.528256  [ 6464/60000]\n",
      "loss: 1.358917  [12864/60000]\n",
      "loss: 1.454623  [19264/60000]\n",
      "loss: 1.328188  [25664/60000]\n",
      "loss: 1.316102  [32064/60000]\n",
      "loss: 1.358355  [38464/60000]\n",
      "loss: 1.272576  [44864/60000]\n",
      "loss: 1.307070  [51264/60000]\n",
      "loss: 1.215743  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.231384 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.303065  [   64/60000]\n",
      "loss: 1.298614  [ 6464/60000]\n",
      "loss: 1.117528  [12864/60000]\n",
      "loss: 1.243057  [19264/60000]\n",
      "loss: 1.114168  [25664/60000]\n",
      "loss: 1.128344  [32064/60000]\n",
      "loss: 1.172868  [38464/60000]\n",
      "loss: 1.102804  [44864/60000]\n",
      "loss: 1.140064  [51264/60000]\n",
      "loss: 1.060151  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.073874 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Saving & Loading Models](#toc0_)\n",
    "\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "\n",
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "# print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[1. Tensors](#toc0_)\n",
    "\n",
    "\n",
    "[Tensor Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. \n",
    "\n",
    "In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to **NumPy’s ndarrays**\n",
    "\n",
    "except that **tensors can run on GPUs or other hardware accelerators**\n",
    "\n",
    "In fact, tensors and NumPy arrays **can often share the same underlying memory**, eliminating the need to copy data.\n",
    "\n",
    "Tensors are also **optimized for automatic differentiation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Tensor Initialization](#toc0_)\n",
    "\n",
    "Tensors can be initialized in various ways\n",
    "1. Directly from data - The **data type is automatically inferred**.\n",
    "2. From a NumPy array\n",
    "3. From another tensor\n",
    "4. With random or constant values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.1926, 0.1006],\n",
      "        [0.3462, 0.5497]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9529, 0.7584, 0.4060],\n",
      "        [0.2671, 0.7593, 0.3491]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)  # 与 np_array 共享空间\n",
    "# x_np = torch.tensor(np_array)  # 不与 np_array 共享空间\n",
    "print(x_np)\n",
    "\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[Tensor Attributes](#toc0_)\n",
    "\n",
    "Tensor attributes describe their\n",
    "1. shape\n",
    "2. datatype\n",
    "3. device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "tensor = tensor.to(\"cuda\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[Tensor Operations](#toc0_)\n",
    "\n",
    "Over 100 tensor operations, including\n",
    "1. transposing\n",
    "2. indexing\n",
    "3. slicing\n",
    "4. mathematical operations\n",
    "5. linear algebra\n",
    "6. random sampling\n",
    "\n",
    "Each of them can be run on the GPU (at typically higher speeds than on a CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "print(torch.is_tensor(tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)  # dim 是几，cat后 就改变该 dim 的值\n",
    "print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place operations Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[Bridge with NumPy](#toc0_)\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()  # tensor -> numpy\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)  # 共享空间  # numpy -> tensor\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.tensor(n)  # 不共享空间\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[2. Datasets and DataLoaders](#toc0_)\n",
    "\n",
    "\n",
    "**dataset code to be decoupled from our model training code**\n",
    "\n",
    "PyTorch provides two data primitives to use pre-loaded datasets & own data.\n",
    "1. `torch.utils.data.DataLoader` - DataLoader wraps an **iterable** around the Dataset to enable easy access to the samples\n",
    "2. `torch.utils.data.Dataset` - Dataset stores the samples and labels\n",
    "\n",
    "PyTorch domain libraries provide a number of pre-loaded datasets\n",
    "\n",
    "subclass torch.utils.data.Dataset and implement functions specific to the particular data\n",
    "\n",
    "They can be used to prototype and benchmark your model\n",
    "1. [Image Datasets](https://pytorch.org/vision/stable/datasets.html)\n",
    "2. [Text Datasets](https://pytorch.org/text/stable/datasets.html)\n",
    "3. [Audio Datasets](https://pytorch.org/audio/stable/datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[Loading a Dataset](#toc0_)\n",
    "\n",
    "28×28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the FashionMNIST Dataset with the following parameters:\n",
    "1. **root**            - is the path where the train/test data is stored\n",
    "2. **train**           - specifies training or test dataset\n",
    "3. **download=True**   - downloads the data from the internet if it’s not available at root\n",
    "4. **transform** & **target_transform** - specify the feature and label transformations\n",
    "   1. transform 用于对 输入数据 进行变换\n",
    "   2. target_transform 用于对目标数据 (例如图像的标签或目标值) 进行变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[<class 'torch.Tensor'>, <class 'int'>]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(training_data[0]))  # 一个Tensor + 一个label\n",
    "print([type(i) for i in training_data[0]])\n",
    "print(torch.is_tensor(training_data[0][0]))  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[Iterating and Visualizing the Dataset](#toc0_)\n",
    "\n",
    "We can index Datasets manually like a list: training_data[index]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABp80lEQVR4nO3deXRV5fX4/x0CmUcgARIggYAgoIKMIrMoFhT1i1ZxAhXlpwyl2lrniWrFqSACVaugWC1ScUBBxAIqiKIoKCCjTCIkYUjIRAJyfn90kY8xz37gHm7I8Lxfa3Wtss/d95x7c597tifZ+4R4nucJAAAAarxalX0AAAAAODUo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/E6BkJAQeeihh0r/PWPGDAkJCZFt27ZV2jEBOL6TWavDhw+X9PT0oB8TAJwMCj+DY1/2x/4XEREhp512mowePVoyMzMr+/CAGu3777+Xyy+/XNLS0iQiIkJSU1Pl/PPPl8mTJ1f2oQFO++25MSQkRJKTk6Vv374yf/78yj48nKDalX0AVdkjjzwizZo1k0OHDsnSpUtl2rRpMm/ePFmzZo1ERUVV9uEBNc7nn38uffv2laZNm8rNN98sDRs2lJ07d8oXX3whkyZNkjFjxlT2IQLOO3Zu9DxPMjMzZcaMGTJw4ECZO3euXHTRRZV9eDgOCj+L3/3ud9KpUycRERkxYoTUq1dPnnnmGXn33Xdl6NChlXx0FaegoECio6Mr+zDgoEcffVTi4+Plq6++koSEhDLbsrKyKuegAJTx63OjiMhNN90kDRo0kDfeeIPCrxrgV70B6Nevn4iIbN26Vfr06SN9+vQp95iT+bueqVOnStu2bSU8PFxSUlJk1KhRkpOTU7p99OjREhMTI4WFheVyhw4dKg0bNpRffvmlNDZ//nzp2bOnREdHS2xsrAwaNEjWrl1b7nhjYmJky5YtMnDgQImNjZVrrrnG1/EDJ2vLli3Stm3bckWfiEhycnLp/58+fbr069dPkpOTJTw8XNq0aSPTpk0rl5Oeni4XXXSRLF26VLp06SIRERHSvHlzefXVV8s9du3atdKvXz+JjIyUxo0by1//+lc5evRouce9++67MmjQIElJSZHw8HDJyMiQ8ePHl1l7gEsSEhIkMjJSatf+v2tJTz31lHTv3l3q1asnkZGR0rFjR/nPf/5TLreoqEjGjh0r9evXl9jYWBk8eLDs2rWr3N/GI3go/AKwZcsWERGpV69e0J/7oYceklGjRklKSoo8/fTTMmTIEHn++eflggsukMOHD4uIyJVXXikFBQXywQcflMktLCyUuXPnyuWXXy6hoaEiIjJz5kwZNGiQxMTEyIQJE+T++++XdevWSY8ePcr9ofqRI0dkwIABkpycLE899ZQMGTIk6K8POBFpaWmycuVKWbNmjfVx06ZNk7S0NLnnnnvk6aefliZNmshtt90mU6ZMKffYzZs3y+WXXy7nn3++PP3005KYmCjDhw8v8x9Be/bskb59+8qqVavkrrvuknHjxsmrr74qkyZNKvd8M2bMkJiYGLn99ttl0qRJ0rFjR3nggQfkrrvuOvk3AKgGcnNzZe/evZKdnS1r166VW2+9VfLz8+Xaa68tfcykSZOkQ4cO8sgjj8hjjz0mtWvXliuuuKLc+Wv48OEyefJkGThwoEyYMEEiIyNl0KBBp/olucVDOdOnT/dExPv444+97Oxsb+fOnd6///1vr169el5kZKT3008/eb179/Z69+5dLnfYsGFeWlpamZiIeA8++GC559+6davneZ6XlZXlhYWFeRdccIH3yy+/lD7uueee80TEe/nllz3P87yjR496qamp3pAhQ8o8/5tvvumJiPfpp596nud5eXl5XkJCgnfzzTeXedyePXu8+Pj4MvFhw4Z5IuLdddddgb5NQNB99NFHXmhoqBcaGuqdc8453p133uktWLDAKykpKfO4wsLCcrkDBgzwmjdvXiaWlpZWZm143v/WW3h4uHfHHXeUxsaNG+eJiPfll1+WeVx8fHyZtarte+TIkV5UVJR36NCh0pjpuwCozo6du377v/DwcG/GjBllHvvbdVJSUuK1a9fO69evX2ls5cqVnoh448aNK/PY4cOHlztvIni44mfRv39/SUpKkiZNmshVV10lMTEx8vbbb0tqampQ9/Pxxx9LSUmJjBs3TmrV+r8fyc033yxxcXGl/4UUEhIiV1xxhcybN0/y8/NLHzdr1ixJTU2VHj16iIjIwoULJScnR4YOHSp79+4t/V9oaKh07dpVFi9eXO4Ybr311qC+JsCP888/X5YvXy6DBw+W1atXyxNPPCEDBgyQ1NRUee+990ofFxkZWfr/j1196N27t/z444+Sm5tb5jnbtGkjPXv2LP13UlKStGrVSn788cfS2Lx586Rbt27SpUuXMo8z/dnDr/edl5cne/fulZ49e0phYaGsX7/+5N4AoBqYMmWKLFy4UBYuXCivvfaa9O3bV0aMGCFz5swpfcyv18mBAwckNzdXevbsKd98801p/MMPPxQRkdtuu63M89PEVbFo7rCYMmWKnHbaaVK7dm1p0KCBtGrVqkxhFizbt28XEZFWrVqViYeFhUnz5s1Lt4v879e9EydOlPfee0+uvvpqyc/Pl3nz5snIkSMlJCREREQ2bdokIv/3N4m/FRcXV+bftWvXlsaNGwft9QAno3PnzjJnzhwpKSmR1atXy9tvvy1///vf5fLLL5dVq1ZJmzZtZNmyZfLggw/K8uXLy/3Na25ursTHx5f+u2nTpuX2kZiYKAcOHCj99/bt26Vr167lHvfbNSnyv78FvO+++2TRokVy8ODBcvsGarouXbqUae4YOnSodOjQQUaPHi0XXXSRhIWFyfvvvy9//etfZdWqVVJcXFz62GPnKZH/rbtatWpJs2bNyjx/ixYtKv5FOIzCz+K3H+5fCwkJEc/zysUr+g+8u3XrJunp6fLmm2/K1VdfLXPnzpWioiK58sorSx9z7A/SZ86cKQ0bNiz3HL/+A1wRkfDw8AopaIGTERYWJp07d5bOnTvLaaedJjfccIPMnj1brr32WjnvvPOkdevW8swzz0iTJk0kLCxM5s2bJ3//+9/LNWQc+7vX3zKt3+PJycmR3r17S1xcnDzyyCOSkZEhERER8s0338hf/vIXYzMIUNPVqlVL+vbtK5MmTZJNmzbJ/v37ZfDgwdKrVy+ZOnWqNGrUSOrUqSPTp0+X119/vbIP13kUfj4lJiaW+VXRMb++Onei0tLSRERkw4YN0rx589J4SUmJbN26Vfr371/m8b///e9l0qRJcvDgQZk1a5akp6dLt27dSrdnZGSIyP+6IH+bC1RHx/4DbPfu3TJ37lwpLi6W9957r8zVPNOfMJyotLS00ivlv7Zhw4Yy/16yZIns27dP5syZI7169SqNb9261fe+gZrgyJEjIiKSn58vb731lkRERMiCBQskPDy89DHTp08vk5OWliZHjx6VrVu3SsuWLUvjmzdvPjUH7Sgu8/iUkZEh69evl+zs7NLY6tWrZdmyZQE/V//+/SUsLEyeffbZMlchXnrpJcnNzS3X4XTllVdKcXGxvPLKK/Lhhx/K73//+zLbBwwYIHFxcfLYY4+VdgT/2q+PGahKFi9ebLwSN2/ePBH5369ej13B+/XjcnNzy51UAjFw4ED54osvZMWKFaWx7Oxs+de//lXmcaZ9l5SUyNSpU33vG6juDh8+LB999JGEhYXJ6aefLqGhoRISElLmN2Dbtm2Td955p0zegAEDRETKrR/u0lOxuOLn04033ijPPPOMDBgwQG666SbJysqSf/zjH9K2bdtyf/dzPElJSXL33XfLww8/LBdeeKEMHjxYNmzYIFOnTpXOnTuXaZEXETn77LOlRYsWcu+990pxcXGZX/OK/O9v+KZNmybXXXednH322XLVVVdJUlKS7NixQz744AM599xz5bnnnjvp9wAItjFjxkhhYaFcdtll0rp1aykpKZHPP/+89Mr2DTfcIJmZmRIWFiYXX3yxjBw5UvLz8+XFF1+U5ORk2b17t6/93nnnnTJz5ky58MIL5Q9/+INER0fLCy+8IGlpafLdd9+VPq579+6SmJgow4YNk7Fjx0pISIjMnDnT16+Ngepq/vz5pY1MWVlZ8vrrr8umTZvkrrvukri4OBk0aJA888wzcuGFF8rVV18tWVlZMmXKFGnRokWZ9dSxY0cZMmSITJw4Ufbt2yfdunWTTz75RDZu3CgiZf8eEEFUiR3FVdaxlvWvvvrK+rjXXnvNa968uRcWFua1b9/eW7Bgga9xLsc899xzXuvWrb06dep4DRo08G699VbvwIEDxn3fe++9noh4LVq0UI9v8eLF3oABA7z4+HgvIiLCy8jI8IYPH+59/fXXpY8ZNmyYFx0dbX2dwKkyf/5878Ybb/Rat27txcTEeGFhYV6LFi28MWPGeJmZmaWPe++997wzzzzTi4iI8NLT070JEyZ4L7/8crl1lZaW5g0aNKjcfkzjmL777juvd+/eXkREhJeamuqNHz/ee+mll8o957Jly7xu3bp5kZGRXkpKSunIGRHxFi9eXPo4xrmgpjGNc4mIiPDat2/vTZs2zTt69GjpY1966SWvZcuWXnh4uNe6dWtv+vTp3oMPPuj9tuwoKCjwRo0a5dWtW9eLiYnxLr30Um/Dhg2eiHiPP/74qX6JTgjxPP5TFQAAVA2rVq2SDh06yGuvvcadpCoAf+MHAAAqRVFRUbnYxIkTpVatWmUaqBA8/I0fAACoFE888YSsXLlS+vbtK7Vr15b58+fL/Pnz5ZZbbpEmTZpU9uHVSPyqFwAAVIqFCxfKww8/LOvWrZP8/Hxp2rSpXHfddXLvvfeWmzmL4KDwAwAAcAR/4wcAAOAICj8AAABHUPgBAAA44oT/cvJUTdD2s5/K/jPFO++8U91WXFxsjH/55Zdqjnbnj1/f/ua3LrjgAmPcdieD//znP+q2YKpVy/zfF1XhhvaV/dkxYVo9aiLWGnBqHG+tccUPAADAERR+AAAAjqDwAwAAcASFHwAAgCNOeIBzTfsj2PT0dGP8vPPOU3PatGljjNvewq5duxrjjRs3VnO0ZojIyEg1p7Cw0Bh/88031Zyff/7ZGP/xxx/VnKVLlxrj+/fvV3M0ts/UqfpDcP7gHFXVmDFjjPHw8HA156mnnjLGq0LTHGsNODVo7gAAAICIUPgBAAA4g8IPAADAERR+AAAAjqDwAwAAcASFHwAAgCMqZZxLsMd4XHbZZcZ4z5491Zx69eoZ47m5uWqONrIkMzNTzTl06JAxnpaWpubExsYa42FhYWrOihUrjPHatfXbMWsjbbT9i4hERUUZ47b34F//+pcxvnHjRjVH+4wwYqLm8PM9EOz3prJ//n369FG3TZo0yRi3jVvSvgtP1ftm209VuDf3b7my1uAWxrkAAABARCj8AAAAnEHhBwAA4AgKPwAAAEdQ+AEAADiiUrp6/bjyyivVbb169TLGc3Jy1JyioiJj/MiRI2pOZGSkMX748GE1R3t7bd3DxcXFxritqzciIsIYt/3cSkpKjHHbR6JWLfN/K9g6gaOjo43x++67T8355Zdf1G3BVNldnSaVvdZOlWB392vP52c/tv2HhoYa47bPrHYMy5cvV3Oys7ON8YsvvljN0WjrVkTvtg32z4e1BpwadPUCAABARCj8AAAAnEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARtSv7AE7UkCFD1G179uwxxm2t+tpoFts4F9s2jTYqoW7dugHn2PavjYCx0cZS2EY/aNtsx5afn2+Mn3HGGWrOqlWr1G1wl21Na9tsow20HD/jT2zmz59vjGvfQyIil112WcD78TNqxvZaNdp76vJ4FNu4rQ4dOhjjX375ZUUdDqDiih8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOKLKdfU2bNjQGC8sLFRzYmJijPHw8HA1Z9++fYEdmIWta1DbVlJSouZonXG2zlltP7ZuX60DUIuLiERFRRnjCQkJao7WBdmiRQs1h67ems9Pt62ftWbrNPXToatp3ry5uq1t27bG+KxZs9Sc3r17G+NLly5Vc7T17qdL2fa+aR2swXw/q6o+ffoY47YO7XPOOccYb9OmjZqze/duYzwnJ0fNSUxMNMZ37typ5kRERBjjtrWmvVbbz7+goMAYt3Wca+fwOnXqqDmHDx82xm3vm/Z6kpKSAt5PUVGRmtOoUSNj3DblQ1vTCxYsUHOOhyt+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHVLlxLunp6ca4rbVcawdv2rSpmqONhzl06JCaox2DrYVdOzbbeAVtjELt2vqPSxsP42fMyt69e9WclJQUY9w2yiAvL88Yj4+PV3NQ89nGhfgZzaKNeLCNTtJ07dpV3aaNxhg3bpyas2PHDmO8VatWas5pp51mjP/3v/9Vc7TvFdv3p5Zje6/9vKfVSVxcnLrtlltuMcY//PBDNUf7rtV+xrZjsJ071q1bZ4x36tRJzdHGnNhGpmjf6bbPjDbKxHb+PHjwoDEeGxur5mjjaWzH5ke9evWMcW3kmYhed9hGqGljz77//nvL0dlxxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHFHlunrT0tKMcVsHaFZWljGudQiLiGzbts0Yt3X1ajeMtnXkHDlyxBi3dRhp3bvac9mOwdYJnJ+fb4zb3gPtJuDajaRtx2brBEbN4aebTvvc2joa/XSaPv7448b4wIED1Rztu8h2bN99950xfvHFF1uOLnBah6TtO0qbPGDTpUsXYzw6Ojrg56pMYWFhxnivXr3UnH379hnjWre3iD5hwtY9vH79emO8cePGak7z5s2N8f3796s52s9M69wV0de0bQ1qEyYOHz6s5mg/n59//lnN0d7TpKQkNUfrbLadC7Ozs43x1NRUNef00083xn/88Uc1Z8+ePcZ4QUGBmnM8XPEDAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiiyo1zadiwoTFuu5GzNo7AlqO1xO/du1fN0W5abRvjoI05sd00XWuVt900WzsG7SbXIvr7Zmuv18YFbNmyRc3RWuVtN9pGzaF91m2jhmyjizTt27c3xu+88041R7vRujbuSUTkm2++McZPO+00NadVq1bqtmDSvjv8jGy5//771W3XXXedMf7ZZ58FvJ/KpI0f+emnn9Qc7TtdG1cioq8B24gRbSyJNk5GRKRFixbGuPY5FxFZs2aNMW47f2ojYGxrWht/YqOdIxo0aKDmREREGOO2UTPaqLaYmBg1p379+sa4dr4TEfn+++8Dei4R+zncL674AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjqlxXr9bdYuuCzc3NNcbT09PVHK1719ZlpbHdAF3rkNU6w0T8dfVqHVi2jmPtpuK2nO7duxvjGzduVHO0Ti+tkwo1i/Z5tnXuat10U6dOVXO6dOlijH/00Udqzg033GCM29anxvZ9s2DBAmP81VdfVXOuv/56Y1x7P0X070nbWps7d64xrt1QXkRk//79xrh2Q/mqSnsvf/zxRzVH65Bu2rSpmqN1mtomKBw6dMgY79Gjh5qjdaNr3/Uiejf67t271RztfGM7T2tr2tYFW1RUZIzbPptZWVnGuPZ+iuhdz7ZuaG0N2DqOCwoKjPGtW7eqOX369DHGIyMj1Zzj4YofAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARVW6cizZOxXajYm2Mwssvv6zmaOMNtBtj247B1iausY1M8UMbC2BrR9fGw8THx6s5zz77rDGenJys5uTn5xvjJ9OOjqrF9nnWRj/ceuutas7tt99ujH/77bdqTs+ePY1xPzeH90MbpSGij5S56KKL1Bzte822H200x2effabmaOtTi4voY1tef/11Neeee+5Rt1UWbSxIQkKCmvPzzz8b49qYFxGRzMzMgPejfXfbPs/aaDHb+tRGptjGxqxcudIY19a6iD6yJDU1Vc3RRuSsX79ezdFGwrVt21bN0UbXaO+NiD6ixzYKSltT0dHRao7289bGpJ0IrvgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCOqXFdv3bp1jfHc3Fw1Jy0tzRj/8MMP1ZzOnTsb440aNVJztJtJa51UNrbuJ1t3mEbr6rV1Q2dkZBjjtq7BiRMnGuOzZ89Wc7SbWft531yg3TjedgP06mjfvn3qNu1zNmXKlID3o3Xwi4gcOXLEGLetTz8/h/vvv98YP/fcc9WcyZMnG+PXXnutmvP1118b49p3l4j+c7B1Ts6dO9cYX7t2rZpTFWnnG1tHc5MmTYzx1atXqznad12XLl3UnC1bthjjtp9lSUmJMa51x4qILFiwwBi3dWjfdtttxrjWiS6id+p/8cUXas6sWbOMca0bW0SfsvHpp5+qOdo63Lt3r5qjTcWwnXO1z05WVpaao3UW2zqOj4crfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1S5cS7a6AXbeIXIyEhj3DYupGHDhsa4Nt5BRL8psm38ip8bKWs31La9Hq2129b6r90cW2tTFxHJy8szxrUbiovo40m0nxvMtPdRJLijXvzsx7Y+NW+++WbAObZj09aaNuLCxvZ++hm3o63DCRMmqDmPPvqoMf7JJ5+oOdrYqzVr1qg569atM8bj4+PVnCVLlqjbqpNvvvnGGB80aJCao73Hts9ZUlKSMW5bN9p3bVRUlJoTGxtrjNtGdDVu3NgYt50Lly9fboy/9dZbas6KFSuM8dGjR6s5MTExxrhtNMv3339vjGvjcUT0n0+3bt3UnI8//tgY195PEf08bRs5pf1MTwZX/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAEZXS1WvrYPHTBat1of78889qTlxcnDFeUFCg5mhdtbbOLK1DN5iv07bNdpNprWPK1jVme3802rHZPgdhYWHGuJ8OzeommB26VWH/2rqxdehqbB30fjqL/Qjm+2PrjtXWmu174J133gn4GFq0aGGM234+2dnZAe+nKtJ+lhEREWpOfn6+MW57v5KTk43xOXPmqDnTp083xm0TFB5//HFj/O2331Zzrr/+emP8p59+UnO0bteOHTuqOXv27DHGe/TooebccsstxrjtvKadOwYPHqzmNG3a1Bh/4YUX1BxtKsXWrVvVnF69ehnjtvOadp62Tfk4Hq74AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcUSnjXGw3HdZalG2ty4cPHzbGtRtwi+gt0sXFxWqONkrC1vqv8XMTeNsIGG0Ei+1G2/v37zfG27Vrp+b4obW9216PNrLChXEuNY22bvyMc7HRxrn4WWvBpu1n5syZak7Lli2N8XfffVfNGTNmTGAHJiIbNmwwxt97772An6u68fNd27x5c2M8KytLzfn000+Ncdv32ccff2yM33zzzWqONsKsdevWas7nn39ujGsjaERE9u3bZ4zn5uaqOU8++aQx/sADD6g52vP16dNHzbn//vuNcdtaT0hIMMZt3x2jRo0yxrXXKSLSqlUrYzwzM1PN0T4j2qi4E8EVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwRKV09cbFxanbtM4b243JtZuZ224krnUAajd4FtE7fm05wexotN2gXusstt24Xrtpdtu2bQM7MNF/BiL6+2PrONZybPupKbTPhq3DrCrvR2Pbj9ax5qdD15ajbbOtTz/vj9bpd/bZZ6s58+bNM8aHDh0a8P5ttK5ebVpCTaJ1TGpdniL6BIX69eurOU2aNDHGN23apObs3LnTGD9w4ICao32eVqxYoeZo0y/+9a9/qTk33nijMd69e3c1R/s82SZpTJ482RhPSUlRczp06GCM2+qO77//3hjXOrhF9PdN66wWEdmxY0fAOVoHs+3cfjxc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOKJSxrkkJiaq27QWZds4l7179wZ8DNq4kMLCQjVHGz/iZ1yEbTSLtp/Q0FA1x88oi5ycnID3o9FGw4j4G82hjadBYGw38tbWmu3n72d9aj9L2xgHbX3YPjN+xhto68bPOJdu3bqpOZdddpkxvnnzZjXnhhtuULdptJ+37b3Jy8szxm1jQ2oK7bv24MGDao42liQtLU3NadasmTE+c+ZMNWfZsmXGuG0M1qRJk4zx5cuXqzlNmzY1xh9++GE1R1sftvPa22+/bYz37dtXzenUqZMx/uijj6o52nvwzDPPqDnaccfExKg52vrQjllE5N133zXGL774YjUnOjraGGecCwAAAI6Lwg8AAMARFH4AAACOoPADAABwBIUfAACAIyqlq9fWsal1FNaurR9qdnZ2wMeQnJxsjNu6U7VjsB2bxk9Hjq1DU+tStnUl5efnB3wMml27dqnb6tWrF/D+bV2irvLTaernc2brzNNoN7s/3rZTwc/7Zuse1owdO1bdFh8fb4zfd999ao7WbWvj57i1tWbrHtXY3uvqZNOmTeq2jIwMY9y2brQO0AcffFDN0aYuNGjQQM156aWXjPHHHnss4P2sWrVKzXniiSeM8ddff13N0T4bHTp0CDinffv2ao52Xrn88svVnPr16xvjKSkpas62bduM8aKiIjVHq30KCgrUHNvz+cUVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyplnEtsbKy6TWstt41M2b59e8DHULduXWPc1lattWL7uam9baSNlmMb1aDdyDkqKkrNycrKCng/Gts4l9TUVGPcNq4iLi4u4GOo6fz8XGxjD26//XZj/Pvvv1dz1q1bZ4xfcMEFak6vXr2M8Xnz5qk5ixYtMsZ37typ5mzcuNEY9/O+2fTo0cMY79Onj5rz2WefGeNvvPFGwPu3jXXSxl/YRo1s2LDBGD+Zm8BXd99++626bfDgwca4NlJLRP+uPXz4cGAHJvZju+OOO4xx7bxqc9FFF6nbvv76a2NcW+si+ve9tm5FRPbt22eMDxo0SM3RzuE///yzmnPmmWca47b3bffu3ca47dyufUZso820z87J4IofAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiiUrp6ExMT1W1aB57txtS2G2prtO4aWweg1iEbHh4e8P5tncDazdFtN0DXbrQeExOj5uzdu9cYP3TokJqjyczMVLdp77WtazAhISHgY6gp/HShtmjRwhifPHmymqN1rNk6dLWO1latWqk5Bw8eNMZtnfqXXHKJMW77HtD28+ijj6o52o3WbYYPH26M2zrzXnjhhYD3o31H2Dp0tfVuOzbt+fx8Dm0/0+rENnFg5cqVxnjHjh0D3o/te27Lli3GeEZGhprzww8/GOO27lTtvFZUVKTmaOeb/fv3qzklJSXGuG2ShjYRorCwUM3RPrfNmjVTc7QpAklJSWqOttaaN2+u5uzZs8cYt6215ORkY9z2HhwPV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6olN772NjYgHO09nERkR07dgTt+Ww3WC4uLjbGbaNZtBuq+8mx3QRce7769eurOdpNpv2Mc9m1a5e6TXtPtfdTxD6GxlW2cT7//Oc/jXHbTeC1z5NtTEDr1q2N8a1bt6o5K1asMMYbNmyo5mgjRtauXavmbNiwwRi3jad5+eWXjXFtjISIyPnnn2+Mf/7552rO/Pnz1W2Bsn0O/Ixg0T4jtu8ol23cuNEYHzZsmJoza9YsY9w2zuWGG24wxm3nu1WrVhnjts+zNurF9lnSxvbs3LlTzdE+t7b3QDsX2Y6tXr16AT2XiH4uso3B0cb3+Bmd5Od8p415ORFc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1RKV294eLi6Teu2tXW02joKNVrHmq1jTjturQvX9ny290DrsrN1aGqvx9YNHR0dbYz7udG61k0m4q+z2U/nd003adIkdZvWGafdFFxEvzl7ZGSkmqPdOF7rdBQRadWqlTGemJio5sTFxRnjvXr1UnO07mGt41lE5MiRI8b4Y489puZon+fnn39ezQn0uUREjh49aozbOhq1rkGbkpISY9z23VHT2bpgBw8ebIy//fbbas7IkSON8WnTpqk5t99+uzH+l7/8Rc3p3bu3Mb58+XI1R/s527pTs7OzjXHbuUP73No+z9p3lO38qXXv2iZPNGrUyBhPSkpSc7Rznu17Tftu1da6iN7xezJd91zxAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4olLGudhavrXxJ7bW5czMzJM+puPtX0QfvWDL0Y7b9h5o7du20RwRERHGuNYOb9um3bDaxtYq37p1a2P8hx9+UHP83LS6phgxYoQxfs0116g5n3zyiTFuG0egfZ4KCgrUnPj4eGP8wgsvVHPS09ONcdvPf/fu3eo2zYABA4zxHj16qDmTJ082xs8880w1Z+XKlcb4kiVL9INT2L47bCMeNH5GPGjjNA4ePBjwc9UUtu8z7fO8fv16NUcbpzJmzBg155133jHGhw4dqub8+c9/NsZt3wN79+41xm2fTW0Mkm08kTY+zJajjZqxjTbbt2+fMZ6SkqLmaGNWtLE1tpycnBw1Rzuv1a1bV83RxtP4OU8fwxU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHBEpXT1ah2oInq3a2FhYVCPQetY0zqPRESKioqMcVv3ndaVpN0YXUR/f2w3s9ZuqG3rzGrYsKExnpubq+b4of3sbJ3Ntm013ddff22Mf//99wE/l63zq3nz5sZ4WlqamqN14GmdgSL6zcx/+eUXNadx48YB72fjxo3GuK1j7rbbbjPGbWvtjTfeULcFyk/nbrBpPwfbd4fLGjRoYIz37dtXzfnyyy+N8W3btqk5p59+ujGudRWL6N3D/9//9/+pOVrXaFhYmJqjfT/bzmvh4eHGeGpqqppje380GRkZxrjWhSuid+/auuQTExONcdt3rlZ31K9fX83R6gFbzvFwxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4IhKmZlhuymz1iZuu/GxH1rbu412Y2o/Y0m05xLRW+ITEhLUHK1NPJg3bfcrMzPTGNdG3Yi4PUpiyJAhxrhtDJI2mkUb2SOif852796t5vz888/GeFRUlJpjW+8abX3ExcWpOdr4Cdtai4+PN8a11ykiMmvWLHWb5lR9nv2Mh9G+o2zr02WXXXaZMa6N+7LRRpyIiHTu3NkYT0lJUXOWLFlijL/77rsBHRdqNq74AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKqWr13Zzdq3Laf/+/UE9hnPPPdcYX7lypZqjHbefrkU/HXO2m0xrbDfN1o7Bz35stJ+drUvV5a7eV155xRiPjY1Vc7p27WqMHz58WM3ROnFtXeraDdVtnxmtq9bWgap9Nm3rxs+N47UO3euvv17N0diOzfZzCCZbB7Nm27ZtQXsuP13F1Y2f7l1NcXGxum3p0qVB2w/wa1zxAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4olLGudjGRURHRxvj69evD+oxtGjRwhj3PE/N8dPGr403sO1HG2ViG3GijZSx5Wg3tbeNGPAjNzfXGE9MTFRzbKNearrNmzcb4+PGjQvqfrQ1oH0uRPQRMNq6FdHHnNjG09StW9cY1z5LIiI///yzMb569Wo1p6CgQN0WqFM1siXYxzB79uyg7d82qgtA1cAVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwRKV09a5Zs0bdds455xjjixcvDuoxZGZmBvX5oFu0aJEx3qdPHzVnw4YNFXQ0OEbrHgYA1Fxc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOCLE8zyvsg8CAAAAFY8rfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AE6aMWOGhISEyNdff33cx/bp00f69OlT8QcFABWMwk8REhJyQv9bsmRJZR8qUKOc7No7evSovPrqq9K1a1epW7euxMbGymmnnSbXX3+9fPHFFxV+/OvWrZOHHnpItm3bVuH7Ak6VLVu2yMiRI6V58+YSEREhcXFxcu6558qkSZOkqKioQvb5+uuvy8SJEyvkuV1Wu7IPoKqaOXNmmX+/+uqrsnDhwnLx008//VQeFlDjnezaGzt2rEyZMkUuueQSueaaa6R27dqyYcMGmT9/vjRv3ly6desW8DF99NFHJ/zYdevWycMPPyx9+vSR9PT0gPcFVDUffPCBXHHFFRIeHi7XX3+9tGvXTkpKSmTp0qXy5z//WdauXSsvvPBC0Pf7+uuvy5o1a2TcuHFBf26XUfgprr322jL//uKLL2ThwoXl4r9VWFgoUVFRFXloFaKgoECio6Mr+zAA32tPRCQzM1OmTp0qN998c7kT0cSJEyU7O9vXMYWFhR33MYcOHTqhxwHVydatW+Wqq66StLQ0WbRokTRq1Kh026hRo2Tz5s3ywQcfVOIRIlD8qvck9OnTR9q1aycrV66UXr16SVRUlNxzzz0iIpKVlSU33XSTNGjQQCIiIuSss86SV155pUz+kiVLjL+y2rZtm4SEhMiMGTNKY3v27JEbbrhBGjduLOHh4dKoUSO55JJLyv06af78+dKzZ0+Jjo6W2NhYGTRokKxdu7bMY4YPHy4xMTGyZcsWGThwoMTGxso111wTtPcFqCxbt24Vz/Pk3HPPLbctJCREkpOTy8WLi4vl9ttvl6SkJImOjpbLLrusXIH427/xO7Z2//3vf8t9990nqampEhUVJc8++6xcccUVIiLSt29f/iQE1d4TTzwh+fn58tJLL5Up+o5p0aKF/OEPfxARkSNHjsj48eMlIyNDwsPDJT09Xe655x4pLi4uk/Puu+/KoEGDJCUlRcLDwyUjI0PGjx8vv/zyS+lj+vTpIx988IFs3769dB1xBT04uOJ3kvbt2ye/+93v5KqrrpJrr71WGjRoIEVFRdKnTx/ZvHmzjB49Wpo1ayazZ8+W4cOHS05OTukiCcSQIUNk7dq1MmbMGElPT5esrCxZuHCh7Nixo3QxzJw5U4YNGyYDBgyQCRMmSGFhoUybNk169Ogh3377bZlFc+TIERkwYID06NFDnnrqqWp5lRL4rbS0NBERmT17tlxxxRUn9LkeM2aMJCYmyoMPPijbtm2TiRMnyujRo2XWrFnHzR0/fryEhYXJn/70JykuLpYLLrhAxo4dK88++6zcc889pb+O5k9CUF3NnTtXmjdvLt27dz/uY0eMGCGvvPKKXH755XLHHXfIl19+KX/729/khx9+kLfffrv0cTNmzJCYmBi5/fbbJSYmRhYtWiQPPPCAHDx4UJ588kkREbn33nslNzdXfvrpJ/n73/8uIiIxMTEV8yJd4+GEjBo1yvvt29W7d29PRLx//OMfZeITJ070RMR77bXXSmMlJSXeOeec48XExHgHDx70PM/zFi9e7ImIt3jx4jL5W7du9UTEmz59uud5nnfgwAFPRLwnn3xSPb68vDwvISHBu/nmm8vE9+zZ48XHx5eJDxs2zBMR76677jrh1w9UFtPas7n++us9EfESExO9yy67zHvqqae8H374odzjpk+f7omI179/f+/o0aOl8T/+8Y9eaGiol5OTUxrr3bu317t379J/H1u7zZs39woLC8s87+zZs43rGqhucnNzPRHxLrnkkuM+dtWqVZ6IeCNGjCgT/9Of/uSJiLdo0aLS2G/XjOd53siRI72oqCjv0KFDpbFBgwZ5aWlpvo8fZvyq9ySFh4fLDTfcUCY2b948adiwoQwdOrQ0VqdOHRk7dqzk5+fLJ598EtA+IiMjJSwsTJYsWSIHDhwwPmbhwoWSk5MjQ4cOlb1795b+LzQ0VLp27SqLFy8ul3PrrbcGdBxAdTB9+nR57rnnpFmzZvL222/Ln/70Jzn99NPlvPPOk127dpV7/C233CIhISGl/+7Zs6f88ssvsn379uPua9iwYRIZGRnU4weqioMHD4qISGxs7HEfO2/ePBERuf3228vE77jjDhGRMn8H+Os1k5eXJ3v37pWePXtKYWGhrF+//qSPG3b8qvckpaamlvuD7u3bt0vLli2lVq2ydfWxX/ecyAnl18LDw2XChAlyxx13SIMGDaRbt25y0UUXyfXXXy8NGzYUEZFNmzaJiEi/fv2MzxEXF1fm37Vr15bGjRsHdBxAVZGfny/5+fml/w4NDZWkpCQREalVq5aMGjVKRo0aJfv27ZNly5bJP/7xD5k/f75cddVV8tlnn5V5rqZNm5b5d2JiooiI+h9Zv9asWbOTfSlAlXXsvJGXl3fcx27fvl1q1aolLVq0KBNv2LChJCQklDnvrV27Vu677z5ZtGhRaXF5TG5ubhCOHDYUfifpZP5r/9dXGX7t13/gesy4cePk4osvlnfeeUcWLFgg999/v/ztb3+TRYsWSYcOHeTo0aMi8r+/8ztWDP5a7dplf9Th4eHlClOgunjqqafk4YcfLv13WlqacW5evXr1ZPDgwTJ48GDp06ePfPLJJ7J9+/bSvwUU+V/RaOJ53nGPg6t9qMni4uIkJSVF1qxZc8I52nntmJycHOndu7fExcXJI488IhkZGRIRESHffPON/OUvfyk9l6HiUPhVgLS0NPnuu+/k6NGjZYqrY5ewj510jl1ZyMnJKZOvXRHMyMiQO+64Q+644w7ZtGmTtG/fXp5++ml57bXXJCMjQ0REkpOTpX///sF+SUCVcv3110uPHj1K/30iBVinTp3kk08+kd27d5cp/ILteCc+oDq56KKL5IUXXpDly5fLOeecoz4uLS1Njh49Kps2bSrTzJSZmSk5OTmla27JkiWyb98+mTNnjvTq1av0cVu3bi33nKylisElnwowcOBA2bNnT5muwCNHjsjkyZMlJiZGevfuLSL/WyihoaHy6aeflsmfOnVqmX8XFhbKoUOHysQyMjIkNja2tE1+wIABEhcXJ4899pgcPny43DH5nV8GVEXNmzeX/v37l/7v2PiWPXv2yLp168o9vqSkRP773/8afxUVbMfmYf72P+iA6ujOO++U6OhoGTFihGRmZpbbvmXLFpk0aZIMHDhQRKTcnTaeeeYZEREZNGiQiPzfFfZfX1EvKSkpd94T+d9a4le/wccVvwpwyy23yPPPPy/Dhw+XlStXSnp6uvznP/+RZcuWycSJE0v/UDY+Pl6uuOIKmTx5soSEhEhGRoa8//77kpWVVeb5Nm7cKOedd578/ve/lzZt2kjt2rXl7bfflszMTLnqqqtE5H+X5KdNmybXXXednH322XLVVVdJUlKS7NixQz744AM599xz5bnnnjvl7wVwKv3000/SpUsX6devn5x33nnSsGFDycrKkjfeeENWr14t48aNk/r161foMbRv315CQ0NlwoQJkpubK+Hh4dKvXz/jDEGgqsvIyJDXX39drrzySjn99NPL3Lnj888/Lx1V9oc//EGGDRsmL7zwQumvc1esWCGvvPKKXHrppdK3b18REenevbskJibKsGHDZOzYsRISEiIzZ840/mlFx44dZdasWXL77bdL586dJSYmRi6++OJT/RbUPJXcVVxtaONc2rZta3x8Zmamd8MNN3j169f3wsLCvDPOOKN0PMuvZWdne0OGDPGioqK8xMREb+TIkd6aNWvKjHPZu3evN2rUKK9169ZedHS0Fx8f73Xt2tV78803yz3f4sWLvQEDBnjx8fFeRESEl5GR4Q0fPtz7+uuvSx8zbNgwLzo62v+bAZxCgYxzOXjwoDdp0iRvwIABXuPGjb06dep4sbGx3jnnnOO9+OKLZca2HBvn8tVXX5V5DtOYJW2cy+zZs43H8eKLL3rNmzf3QkNDGe2CGmHjxo3ezTff7KWnp3thYWFebGysd+6553qTJ08uHcFy+PBh7+GHH/aaNWvm1alTx2vSpIl39913lxnR4nmet2zZMq9bt25eZGSkl5KS4t15553eggULyq2V/Px87+qrr/YSEhI8EWG0S5CEeN4J/AUzAAAAqj3+xg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEec8J07quM982zH7Gd84cqVK43xOnXqqDl79+41xm23UNPuO9q2bVs1RzuGY7d0M2nZsqW6zRVVcYxldVxrfqSnp6vbxo8fb4w3btxYzXnooYeM8R9++EHN0W4HZbvLRlJSkjF+2223qTn79u0zxh999FE15+DBg+q26oi1Fhy/vv/7bx09etQYt32e7733XmO8Q4cOas6x+8z/VklJScDHduTIETVn//79xvjSpUvVnL/97W/qNo32nmrHXNUdb61xxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHBHinWCrlSvdT+edd56a89577xnj+fn5ak69evUCPjatE9fWoat1ANaurTdud+7c2RjftWuXmuNa91NlqI5rzaZTp07G+NSpU9Wcw4cPG+O2zvbNmzcHdmAikpeXZ4xHRUWpOdrPRztmEb1z0bZurrjiCmNcmxRQ1bHWAqN919rOHdrnbPXq1WqONt1hy5Ytak5CQoIxbjtHhYaGGuO2c9ShQ4eM8RYtWqg5a9asMcbPOOOMgI/tl19+UXO0z05V+JzT1QsAAAARofADAABwBoUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEfofdQ1gJ9W/d/97ncB52gt5yIiWVlZxritTVwb8WBre9dybGMpRowYYYw//PDDag4QqOzsbGP8559/VnO0dTNmzBg1Z/bs2cb4jh071Jw333zTGG/Tpo2ao418mjJlippz1llnGeMNGzZUc7Rt1XWcC8rzc46ynTs0trWWnJxsjBcUFKg52kgZbcyLiEhERIQxXlhYqOZo45Zsa2D9+vXqtkD5GQlXHXDFDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcUaO7ev10P5177rnqNu0m7LbOnzp16gSco3V62TrAtNdqew9sr1VTnTuZUDni4+ONcW09iYikp6cb4ykpKWpO69atjfGNGzeqOXv27DHGzz//fDXnwQcfNMZXrFih5lx44YXGeElJiZrTpEkTY1y7CT2qH8/z1G3aucP2menRo4cx3qJFCzVn27ZtxrhtIkRkZGRAcRH9/JWZmanmhIeHG+O2LuW2bduq2zTaedLPJI3qgCt+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABH1OhxLn7YWtiLioqMcdtoltDQUGPc1savsY1z0fZz5MgRNadu3boBHwMQqJYtWxrj2rgKEZG1a9ca4w899JCa07BhQ2N806ZNas6cOXOM8Z9++knNeeutt4zxyZMnqznayIro6Gg1p3v37sb4/Pnz1RzUHNrYlrCwMDVn6tSpxrjtHFW/fv2A9m97vt27d6s52sgU2wgY7bxmG6Wiranx48erOffff78xbjt/Vmdc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR4R4J9heausorY5iY2ON8fXr16s5fm7KrHVg2Z5Le6/9dALb9qN1WbVq1UrN0Tqbqys/72lFq2lrbfbs2ca4rWswNzfXGD98+LCak5KSYozbOvOKi4uNcdu6SUhIMMZ37dql5hw4cMAYT05OVnO0LuVLL71UzanKXF5rtWubB2jYPpu9e/c2xrXOXRH9O9323mtdsLZOYG0/WheuiL7etfdGRP/5aPsX0b8jIiIi1Jw9e/YY41dffbWas337dmPc9npOVZfw8dYaV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI7Q+45ruHPOOccYj4mJUXNycnKMcdvN5rX2bdu4CO35/LTX2/ajjbQ566yz1JwvvvhC3QZ3aaNURETatWtnjC9btkzN0UYv2EYlZGdnB5yjrQ/butFGP2j7FxG56aabjPGmTZuqOa+//roxrq1bEZG8vDx1GyqPnzEed999tzFu+2xqo0xsa2D//v3GuO18Ex4eHvB+tPfANnpEy7EdmzZSprCwUM3RRidNmDBBzbnqqquM8VM1suVkcMUPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABzhbFdvgwYNjHFbV5LWfWS70bftpvIarSvJ1s2ldTnZOqbCwsKMca3DCdDYulMPHjxojNu64bXPrdZNKKJ/nm383ARek5SUpG7bvHmzMW7rNNTeUzp3a45OnTqp29q2bWuMZ2Zmqjl+1kBkZKQxbjt3aNts50+Nn/Oaras30OcSEcnNzTXGO3furOZo63PHjh2BHVgl4IofAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARzo5zadmypTFuG82itYPbcrSRFX7GrOTn56s5Whu97di0benp6WoOYFK/fn11W926dY3xqKiogPdTUlKibtPGQtjGRWhjW7SRSiL6+rSNmtmwYYMxro2REBFJTU1Vt6FmuPjii9Vt2udWG78ioo8Ps4158TNyLJhjkPyMZrHR1q7tdWrnY9v3wIABA4zxF1980XJ0VQNX/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAEc529dq6ajVaJ5OtK8lP1+CRI0eMcT/dibbXqb2exo0bqzmAifaZPd42jZ9OP2192NaAn/1oncURERFqTmJiojFu62z2876heunWrZu6Tfv52z4zhYWFAT2XjZ+JEMXFxWqONnnCtga1Tlzb+VM7Ntv5U+t6tuV0797dGKerFwAAAFUGhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOMLZcS6NGjUyxv2MebHR2sHr1Kmj5mijWbR2eBG9hd3PjamTk5PVHMBk/fr16raioqKAn8/PSCPt82zL0dhGTGhrzc9IG9tYCm0EDGqOtLQ0dZuf8URajm00SzDPebb9+FmH2jnPdszaWrMdm20dapo2bRpwTlXBFT8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcISzXb1nnHGGMe6n88jWYaR179puZq3d7D3YnVlaTuPGjQN+Lrht9+7d6rb09HRjfNOmTWqOtg5t61PbZuu2tXXvBppj24/WqW/b/65duwI7MFRZMTExxnhCQoKas3//fmM8PDxczdHWgO1zpp0HbOebQJ/L9ny2Na119dqmCNSvX98Yt3XJh4WFGeO2Na1NBqkOuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHCEs+NcUlJSjHE/reXaqAYRkdjYWGP8m2++UXOaNGlijMfHx6s5hw8fNsZtbfzaa+Xm8AiUbTzRoUOHjHFt1JGIPsrClqPR1q1fthEPmpKSEmPctj5t425QvbRo0cIYt507tG2hoaFqjjYyxTZmxc9II+35gvlcIvo5yjbSJjIyMqDnEtG/IwoLC9Wc3NxcdVtVxxU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHCEs129WoesrfNHY+tK0m7+XFRUFPDz2boTbV2VgQp2FyTctm/fPmPc1qV+8OBBY1y72b2I3m1rW9PaNtsaCGZOdHS0mrN161Z1G6qXRo0aGeO2z6bWIWvr6tWez5ajsZ3XThXt9bRq1UrN0SZc5OXlqTlaN7St61rrLLZ9D/iZCFARuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHCEs3M7tFES2ugJEb3l209LvnbjehG9jV7bvy3HRsuxjdkAApWTk2OM2z5n2g3Q69Spo+Zo22wjFLRxDbbxF9qato1mKSgoMMZjY2PVnKysLHUbqpfU1NSAc/yMANq/f3/A+/HDzzlKY8vRRrPYxqzYviM02pq20faTkJCg5uzduzfg/VQErvgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCOc7erVBPsmylq3UElJiZrjp0NX6wCzdStprzUxMTHg/QOa7OxsY9xPp6Otm09bA6fqZvO2Y9O2FRcXqzm2zn9UL1qnp+0zo7F1nAeTn2kVNn7WYe3agZcofs6FfnK0bmS6egEAAFBlUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCNq9DgXW1u1xtbC7qe1XGv5trXx+7nRtR/aOBc/7xug0T7rtnElBQUFxnhUVJSao61d24gmbVyDbYyD9nq0G8rbjkF7nSIiERER6jZUL3Fxcca4bcSJ9hm0naOCee6wPZe2zXaO1I7bth/t/bGdP7Uc23vtZ6xOgwYNjPF69eqpOZs3bw54PxWBK34AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4Iga3dWbkZER1OfTuoL83MjZ1mmo7cdPx5YtR+tksuVoNwj30xUFN9SpU8cYLyoqCvi5bB2Ntm0abR362Y8tR+tgzsvLU3Pq16+vbkP1kpiYaIzbPjNal3hJSYmaE8yuXtt5TdvmZ/+2c4ftGDTaMdi6em0TBjTaz65Ro0YBP9epxhU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjavQ4lw4dOgSc4+fG1H7GSNhu6K49n5/Wdhs/xx0bG2uM5+TknOTRoKaKjo42xm2jTLSbvdvGIPkZsxLoc4no4ydsxxYREWGMFxQUqDktW7ZUt6F60daAnzFYts+Zn3OEdgza6C6/tOcL9ogmTTBH3YjoPwdtdE9VwhU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHBEje7q9dMVZ7sBdlRU1MkcThnFxcUB5wS7K0mzb98+dZv2HtDVC01cXJwxvn///oCfS+v2FdE7AP3cgN22H61D10brALQdW0ZGRsD7QdVUp04dY9zWOat16No6XcPDw43xwsJCNcfPeUU7Bm3/tv3YOpv9dOpr2/y8157nqTna64mPj1dzqgqu+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHFGjx7k0a9ZM3aa1kGtt9zZ+boxdVFQUcE6wx7lobe+213POOecY42+99VZQjgnVU8OGDdVt2jgX2ygTbWxQsNenNmbFNpZCG+diGwWlrTVbjjYWIjk5Wc3JyspSt6Hy+Pncat/3ts+zNobIz1gSP2z70dhGs2hso1m016OtdRGRsLCwgHM02vddVcIVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwRI3u6u3UqZO6LTs72xi33Zw9mGydVH5uzu1nP1oHlu094MbxMElPT1e3+el6t3XVavx0QWrdln5u6G7bj5/3QFuHp59+uppDV2/VpHWC+/mc2WifZ1u3rZ/Ps+ZUdQ/baMetTfIQ0d83W1evdj7WftZVCVf8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOqNHjXNasWaNu69OnjzGen5+v5vi50bbG1ibup+1dG/1gGwGj3QQ+MjJSzXniiScCOzA4IS0tTd2mjVGwjQ3S1oBt/IX2fLb1pB2bbT9+bkSvjaexfQ9o21q2bKnmfPLJJ4EdGE4J7TNjGzGifXfbxqz4GUdm+6xrtGPzs278jI2x5djWVKDPZ9uP9npiY2MD3v+pxhU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHBEje7qHTx4sLpNu5m51ukqInLw4EFj3E9XlO1m6trzHT58OOD92DqRi4qKjPHTTjst4P3AbU2bNlW3aZ2LVeFm5tpas3Xz2ToxA+VnP7bvKFRN2rQI289f++4uLCxUc7QO9ri4ODXHT2e7n3WjseX4ObdqXb22jmftGGw5WmezbSpGVcEVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAI2r0OBeb5ORkYzw3N1fNiY6ODjhHk5mZqW7T2sRttBxbG7827sZ2bIBJsMdFnCphYWHGuG0NamOVtFEaNrZRFiUlJcZ4/fr1A94PKldsbKwxbvuc+RmNkpOTY4xHRUWpOdrn2TZuSVvTftaALUd7f2yjzbRj08a82Ni+o6rz+uSKHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4okZ39dq6orRuocaNG6s5O3fuNMa1zkAR/Yba2nPZnk/rIhLRO4k++ugjNee///2vug0IRGpqqrpN66azdTRqa8B203StO9C2PrXvCD/dln66lG3fUYcOHTLGbd2WqJo+/vhjY7xt27ZqTmRkpDG+aNEiNSc7O9sYHz16tJqzb98+Y7xOnTpqjud5xrifrl7tuWzbbOtT6+otKipSc7799ltj3Pbz0V7rgQMH1Jyqgit+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABH1OhxLraWb01eXp667ZZbbjHGp0+fruZo4yfWrFmj5mhjIWw3f/7qq6+M8YsvvljNAYKlZcuWAef4GUviZ8yKbQySn3ER2jbbaBaNbQSM9v6ceeaZAe8Hlevll182xkeNGqXmaOeO7du3qzkPPPCAMW4bZRIXF2eM28a5aJ/b4uJiNUdba9r+RfT3QBt1IyISHR1tjC9ZskTNmTRpkjH+ww8/qDnaOJdPP/1UzakquOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6o0V29wfbmm28a47ZuvjfeeMMY126mLSKyfPlyY/yss85Sc7p27apuAyrapk2b1G3NmjUzxg8dOqTm+Om21dahLefIkSPqtmDuR8ux7V/LsXUaomrSOssLCgrUnPj4eGM8Pz8/4P3ff//9AedA5KefflK3NWnSxBjfs2dPRR1O0HDFDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgiBBPm5sAAACAGoUrfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAq/UywkJERGjx593MfNmDFDQkJCZNu2bRV/UACMTmYdDh8+XNLT04N+TABwMij8guj777+Xyy+/XNLS0iQiIkJSU1Pl/PPPl8mTJ1f4vh977DF55513Knw/QEWrzHUEQHfsP4R+/b/k5GTp27evzJ8/v7IPDyeIwi9IPv/8c+nUqZOsXr1abr75ZnnuuedkxIgRUqtWLZk0aVLAz3fddddJUVGRpKWlndDjKfxQEwR7HQEIvkceeURmzpwpr776qtx5552SnZ0tAwcOlPfff7+yDw0noHZlH0BN8eijj0p8fLx89dVXkpCQUGZbVlZWwM8XGhoqoaGh1sd4nieHDh2SyMjIgJ8fqIqCvY4ABN/vfvc76dSpU+m/b7rpJmnQoIG88cYbctFFF1XikeFEcMUvSLZs2SJt27Ytd7ISEUlOTi4Xe+edd6Rdu3YSHh4ubdu2lQ8//LDMdtPfFqWnp8tFF10kCxYskE6dOklkZKQ8//zzEhISIgUFBfLKK6+UXn4fPnx4kF8hUPFOdB1Nnz5d+vXrJ8nJyRIeHi5t2rSRadOmlcs5tmaWLl0qXbp0kYiICGnevLm8+uqr5R67du1a6devn0RGRkrjxo3lr3/9qxw9erTc4959910ZNGiQpKSkSHh4uGRkZMj48ePll19+ObkXD1RTCQkJEhkZKbVr/9+1pKeeekq6d+8u9erVk8jISOnYsaP85z//KZdbVFQkY8eOlfr160tsbKwMHjxYdu3aJSEhIfLQQw+dwlfhDq74BUlaWposX75c1qxZI+3atbM+dunSpTJnzhy57bbbJDY2Vp599lkZMmSI7NixQ+rVq2fN3bBhgwwdOlRGjhwpN998s7Rq1UpmzpwpI0aMkC5dusgtt9wiIiIZGRlBe23AqXKi62jatGnStm1bGTx4sNSuXVvmzp0rt912mxw9elRGjRpV5rGbN2+Wyy+/XG666SYZNmyYvPzyyzJ8+HDp2LGjtG3bVkRE9uzZI3379pUjR47IXXfdJdHR0fLCCy8Yr6bPmDFDYmJi5Pbbb5eYmBhZtGiRPPDAA3Lw4EF58skng/uGAFVQbm6u7N27VzzPk6ysLJk8ebLk5+fLtddeW/qYSZMmyeDBg+Waa66RkpIS+fe//y1XXHGFvP/++zJo0KDSxw0fPlzefPNNue6666Rbt27yySeflNmOCuAhKD766CMvNDTUCw0N9c455xzvzjvv9BYsWOCVlJSUeZyIeGFhYd7mzZtLY6tXr/ZExJs8eXJpbPr06Z6IeFu3bi2NpaWleSLiffjhh+X2Hx0d7Q0bNizorws4lU50HRUWFpbLHTBggNe8efMysWNr5tNPPy2NZWVleeHh4d4dd9xRGhs3bpwnIt6XX35Z5nHx8fHl1qFp3yNHjvSioqK8Q4cOlcaGDRvmpaWlnfBrB6q6Y+el3/4vPDzcmzFjRpnH/nadlJSUeO3atfP69etXGlu5cqUnIt64cePKPHb48OGeiHgPPvhghb0Wl/Gr3iA5//zzZfny5TJ48GBZvXq1PPHEEzJgwABJTU2V9957r8xj+/fvX+aK3JlnnilxcXHy448/Hnc/zZo1kwEDBgT9+IGq4ETX0a+vxB27+tC7d2/58ccfJTc3t8xztmnTRnr27Fn676SkJGnVqlWZ9TZv3jzp1q2bdOnSpczjrrnmmnLH+Ot95+Xlyd69e6Vnz55SWFgo69evP7k3AKgGpkyZIgsXLpSFCxfKa6+9Jn379pURI0bInDlzSh/z63Vy4MAByc3NlZ49e8o333xTGj/2J0633XZbmecfM2ZMBb8Ct1H4BVHnzp1lzpw5cuDAAVmxYoXcfffdkpeXJ5dffrmsW7eu9HFNmzYtl5uYmCgHDhw47j6aNWsW1GMGqpoTWUfLli2T/v37S3R0tCQkJEhSUpLcc889IiLlCr8TWW/bt2+Xli1blntcq1atysXWrl0rl112mcTHx0tcXJwkJSWV/orrt/sGaqIuXbpI//79pX///nLNNdfIBx98IG3atJHRo0dLSUmJiIi8//770q1bN4mIiJC6detKUlKSTJs2rcwa2b59u9SqVavcea1Fixan9PW4hsKvAoSFhUnnzp3lsccek2nTpsnhw4dl9uzZpdu1bl3P84773HTwwhXaOtqyZYucd955snfvXnnmmWfkgw8+kIULF8of//hHEZFyDRkns95+KycnR3r37i2rV6+WRx55RObOnSsLFy6UCRMmGPcNuKBWrVrSt29f2b17t2zatEk+++wzGTx4sERERMjUqVNl3rx5snDhQrn66qt9rTsEF80dFexYy/vu3bsrdD8hISEV+vxAZfr1Opo7d64UFxfLe++9V+Zq3uLFi30/f1pammzatKlcfMOGDWX+vWTJEtm3b5/MmTNHevXqVRrfunWr730DNcGRI0dERCQ/P1/eeustiYiIkAULFkh4eHjpY6ZPn14mJy0tTY4ePSpbt24tc8V98+bNp+agHcUVvyBZvHix8b9k5s2bJyLmXxkFU3R0tOTk5FToPoCKdiLr6NgVvF8/Ljc3t9xJJRADBw6UL774QlasWFEay87Oln/9619lHmfad0lJiUydOtX3voHq7vDhw/LRRx9JWFiYnH766RIaGiohISFlRhxt27at3E0Gjv29+m/XD3fpqVhc8QuSMWPGSGFhoVx22WXSunVrKSkpkc8//1xmzZol6enpcsMNN1To/jt27Cgff/yxPPPMM5KSkiLNmjWTrl27Vug+gWA7kXWUmZkpYWFhcvHFF8vIkSMlPz9fXnzxRUlOTvZ9Zf3OO++UmTNnyoUXXih/+MMfSse5pKWlyXfffVf6uO7du0tiYqIMGzZMxo4dKyEhITJz5kx+fQWnzJ8/v7SRKSsrS15//XXZtGmT3HXXXRIXFyeDBg2SZ555Ri688EK5+uqrJSsrS6ZMmSItWrQos546duwoQ4YMkYkTJ8q+fftKx7ls3LhRRPhNVoWpvIbimmX+/PnejTfe6LVu3dqLiYnxwsLCvBYtWnhjxozxMjMzSx8nIt6oUaPK5aelpZUZx6KNcxk0aJBx/+vXr/d69erlRUZGeiLCaBdUSye6jt577z3vzDPP9CIiIrz09HRvwoQJ3ssvv3zCa6Z3795e7969y8S+++47r3fv3l5ERISXmprqjR8/3nvppZfKPeeyZcu8bt26eZGRkV5KSkrpyBkR8RYvXlz6OMa5oKYxjXOJiIjw2rdv702bNs07evRo6WNfeuklr2XLll54eLjXunVrb/r06d6DDz7o/bbsKCgo8EaNGuXVrVvXi4mJ8S699FJvw4YNnoh4jz/++Kl+iU4I8Tz+UxUAAFQNq1atkg4dOshrr71mHKmEk8Pf+AEAgEpRVFRULjZx4kSpVatWmQYqBA9/4wcAACrFE088IStXrpS+fftK7dq1Zf78+TJ//ny55ZZbpEmTJpV9eDUSv+oFAACVYuHChfLwww/LunXrJD8/X5o2bSrXXXed3HvvvVK7NtemKgKFHwAAgCP4Gz8AAABHUPgBAAA4gsIPAADAESf8l5OnaoJ2rVqB16LBvDF6nTp11G2jR482xgcPHqzmrFy50hj/6quv1JzY2Fhj/IILLlBzkpKSjPFHHnlEzTmZe5v+lu3n5ufnoz1fMH/WIlIl77jAtHrURKw14NQ43lrjih8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR5zwAOdg/hHsqWrgaNOmjbrt//2//2eMd+3aVc358ssvjfHQ0FA1p3v37sZ4w4YN1RzNnDlz1G3btm0zxs8991w1JyIiwhj/6KOPAj6GwsJCNUdrmDl8+LCac6rwB+fAqcFaA04NmjsAAAAgIhR+AAAAzqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOCIShnnEmwPPvigMd6+fXs1Z9OmTcb4t99+q+YcOXIkoLhtW1RUlJqjjT/ZtWuXmlO/fn1jPDw8XM1JSEgwxk877TQ1RxtD89JLL6k5CxYsMMZt90U+VaNeGDEBnBqsNeDUYJwLAAAARITCDwAAwBkUfgAAAI6g8AMAAHAEhR8AAIAjalfkk9eqZa4rjx49GvBz3X333eq2evXqGeOvvvqqmlO7tvmlx8TEBHZgIpKTk6Nua9CggTGuddSK6N2u+/btU3O07t3o6Gg1R+sS3r59u5qjvW/XXHONmrNt2zZjfMOGDWpOMD87AADgf7jiBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwRIWOc9HGkhQXF6s5UVFRxnjTpk3VnE8//dQYT01NVXO00Si2cSHaiJGwsDA159ChQ8b4L7/8ouZERESo2zShoaEB7V9EP25tPI6ISHZ2tjG+cuVKNefSSy81xidMmKDmaK+HcS4AAPjHFT8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcESFdvX66cA844wzjPEjR44EvJ+EhAQ1R+tOtXXbal29WlxE72A+fPiwmmM7hkD5eT1aN7aISGxsrDFu6x62dVdrbO8PAADwhyt+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHVOg4Fz9jSS688EJjPD8/X83RxraEhoaqOdrIEluOH9oYGtu4Em2bbaSNNk7FNmomIiLCGI+Li1NztNE52pgXEZGCggJjvEGDBmpOZmamMW57PX7GBwEA4BKu+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyq0q9dPl6WtO1Sjdaf6ybF122qds7ZuW63j2Nadqu2ndm39x6V1KdveG63ruqSkRM3Rjtu2H+35unTpoubMnTtX3QYAwdC+fXtj/Pnnn1dzVq1aZYzXrVtXzdEmJYSFhak52vnTdr7RcjzPU3O0bbZzVEhISED7P94xBLof27lQm8yhPZdtm+3ns3//fmP8p59+CvjYoqOj1RztPb3uuuvUnOPhih8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEVOs7Fj8LCQmNcG1ciIhIVFRXwfrTxI1q7tY2tvV4bzWJre9deq62F/VTx8/5oLfF+fm6ofmxjFDTa6Afbc/kZF3HWWWcZ4+Hh4WrOihUrAt6Ptm60kUp++RlloX0X+RnHVd0MHjzYGLeNmmrRooUx7uc9LioqUnO00WK288CpWmsaP58Z27rVjsGWox1DMN8bEZHWrVsb47YxONoIGFt906RJE2OccS4AAAA4Lgo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6olDZRW9dLSkqKMZ6dna3mREZGGuMlJSVqjtb9pnVSifjrwNP2Y+viiY2NNcZzc3PVnOLi4oCeS0R/rbbOXa1LOSEhQc3Ztm2bMd65c2c1Z9asWca4C52GNY2fblvtO8LPz//MM89Ut40fP94Yr1evnpozduxYY3zlypVqTrC7d0/FfmzfUbbvyeokOjraGN+0aZOas2HDhoCeS0T/PNt+Xtq6sXX1auc82+QJbT+2teZnfWrbbN8PfvbjZw34OTZtDWgTQ2zPp02+ENHPubbP2/FwxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4IhKGediu7mwdnN0raVZRG9v99PWbRsB42eEgTYaxdaKrbWD29r4tffHNppFa5X38zrj4uLUbXl5ecZ4cnKymtOjRw9jfOnSpYEdGIIqmGNWbDdN93ND9UaNGhnjU6ZMUXO0sR2ffPKJmlO3bl1j/I9//KOa89lnnxnjX3/9tZrjhzaGxjY6qV27dsa4dhN6EZEXXnjBGF+xYoXl6KqeJk2aGOO28RpJSUnGuDZWTMTf+tDORbYRI9r505ajnSf9nD9to9q057O9N9r3gJ+xMbbvFD8jp7Rzq20/Wo5tVFuDBg2M8Q4dOliOzo4rfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgiArt6o2NjTXG77nnHjXnySefNMZtHWZaF4/txtSFhYXGuK172E9nlvZ8tv1o22ydZtprtXVmac9n62zW3oNWrVqpOWvXrjXGt2zZouYMGTLEGP/8888DPjYETzDfYz+dhja7d+82xm1rbebMmcb44sWL1ZwBAwYY48OGDVNzhg4daozv3LlTzSkuLjbGta5SEf212jpONbaue63rubp19Wodk7bPjDb1wNbRqnVz2j7n2lqznQe0Y7OdC23bNLZpERo/nbPae2B7r7XpF366h20dutr7Zjt/asddUFAQ8H6aN2+u5hwPV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6o0HEujzzyiDGu3UjcxtbCvnHjRmO8Tp06Ae/H1vKttWLbWsv9tMprx+2n5dv2evy8P3v37g0454wzzjDGv/jiCzWnbdu2xvigQYPUnLlz5wZ2YI7TPre2z0zfvn2N8YyMDDVHG/UzceJENWfXrl3GeHp6uprTuHFjY/yzzz5Tc+68805j/MYbb1Rzrr32WmN81apVas7WrVuN8fj4eDUnLi7OGLeNmIiIiDDGo6Ki1Bzte2XdunVqjp/vgaqoWbNmxrjt/dLGheTm5gacYzuvaWNBtJEtIvqatn3X+xk1o22z5WjjXGxjXrTjtp1zNcEeQRMdHW2M274/tWNo0qSJmqN9Rg4cOKDmHA9X/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAERXa1bt27Vpj3NaVlJCQYIx37dpVzUlJSTHG//vf/6o5WqeMn65eW7eQ1m1r69DVup793LjelhMbG2uM247t4MGDxnjr1q3VnJycHGNc60C05bjQuat1bfq5ybmtA9T2WdcMGzbMGN+5c6eao62Pxx9/XM15//33jfHTTz9dzUlKSjLG8/Pz1Zzw8HBj/Nxzz1VztC7hvLw8NUfrEtU+5yL6+tC+I0X092D27NlqzjvvvGOMa9/fIvbv8OpEW1ORkZFqjtZpqp2HRESysrKMca3bW0SkQYMGxrjW7S2if9/bfl5a97BtIoWf7w7tvbbtR/v+snVDax3Utk5g7X3T3hsR/Wd69tlnqzna5IHMzEw1JyYmxhjfsWOHmnM8XPEDAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiiQse5/POf/zTGt2zZouZ069bNGG/YsKGao41KmDlzppqj3ezd1r6t7cc2zkV7Pq1FW0Qf1/Dzzz+rOVoLu+3YtJtM28a5aGzt9VobvTauQERk/PjxAR9DTeFnbEswn+vPf/6zuk0bc5GRkaHmaJ8NbZyQiMj5559vjDdq1EjN0cZC2EZPaGMUbKOGtBET2mgYEX0NHDp0SM3Rvm927dql5vz+9783xrXRE8FmGx9UFWnrQxtbJaL/zLSfl4g+Asb2mdFGvWjf9SL6+tS+60VE4uPjjXE/409s3zfaOrSNztH2k5ubq+ZoPzvbeU07T9p+ps2bNzfGbe+1tg5tr0c7T9q+C4+HK34AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4IgK7erVLF68WN32448/GuMPPPCAmtO0aVNj3HYDdK3LytbJZOuQ1WgdRm3atFFzWrdubYx//vnnao7WJWzrZNI6pmy0zizbjeO17qNLLrkk4P24QOuMtHVM+nm/tO6zwYMHqzlaV1q7du3UHK3btqioSM3RuuEXLVqk5nz//ffG+Pbt29WcDRs2GOO2bstly5YZ47a1pn3faB2VIiJpaWnG+Lhx49ScU9W9qwlmR/qpYPs5a7RzhO37VFu72ne9iN5RWlhYqOZo5zzbuXDnzp3GuO07RevUj4uLU3O0z8ZPP/2k5mjfA7aJAElJSca47ZyrHbetHtC+v/bv36/maGyd2pp69eoFnHMMV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6olHEuNtrohTFjxqg5o0aNMsbT09PVHK29XRs9IaKPZLCNeTl8+LAxbmuV1/ZjG3+h3VQ+Ly9PzdFa8m20ERO33367mvP3v/894P1o74H2ftYk2tiDYI/K6NWrlzGekpKi5mg3jv/mm28C3r9tvMKXX35pjD/99NMB78fPMQwYMEDN0UaA1K9fX83R1po2rkJEJDEx0Rjv16+fmrNnzx5jXBv3ZDs224gm7Xtg4cKFak5VpH0Pa9+nIvr3vW19avvJz89Xc7TxQLbzjfYza9iwoZqjPZ/tfKOdP7Ozs9Uc7T1t0qSJmqN9F9k+z9q4G9u4JW3d2M432ggW7dxly7GN6NFo47hOBFf8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARldLVa7vxseaf//ynum3fvn3GeIcOHdScjRs3GuO2rl4/XbBaV5CtK0nrzLJ1AmvvqZ+bTNu6xrQbQ8+dO1fN0dhukO5C965Gu3H7ddddp+Zs3rzZGLd9Zjp16mSM2zrztE5T29rQbl5vW2utWrUyxkePHq3maF3K7du3V3MaNGhgjO/atUvN0d6fAwcOqDnamrZ1Tmpde/fff7+a88QTTxjjto7TLVu2GON+Jhz46U6sTAcPHjTGtc+5iEhubq4xblsDfjqB/XTxFxcXBxQX0Y/N1p2qnQeSk5MD3o/tHKV9nnbv3q3maO+b1lEroq+1kJAQNUf7brXl+Dk2rfPfTx1Vmus7EwAAANUKhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOKJCx7lo7cZ+xpLYnHnmmca4rR1da2+3jTDQ2I5ZG0tia99OT083xm03dNdugH3o0CE1RxtLYbtBuTY6p3v37mrO6tWrjXHbyBbbZ6Sm0z4b2rgSEZELLrjAGLetAW38SF5enprj53OmvR7tRu8iIqeddpoxbhvNorGNTNHG4NjGLWljiLQbvYuIfPXVV8Z4VFSUmtOoUSNjfOfOnWqOtm5s32vaz8eWo+2noKBAzamKsrKyjPGMjAw1R3uNwf7O8jPORWM7R2k/56rw/eznPdDGqdg+z9rIKT/jXGy057O9Tm0Mjm1Ez/FwxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHFGhXb1++OmU0W6AbrvR+qmidTvaOme1bipb56TW8Wt7P7VOJj9dvdpNrkX8dXe7bM2aNcb44sWL1Rzt5uhah7iISP369Y3xVq1aqTnaZ8PWpa5109m6bbXuN1tHq/Z5tnUnah1z3333nZoTGxtrjHfs2FHNueqqq4xx27rR3lPtdYrY3x+N9t2hdS+L6J3fWidyVaV1U0ZGRqo5tvUB2GjrRvseEtG/C7dt2+b7OLjiBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwRIWOc9HGddhuGK3l2EYLaCMRbDebt42f0GjHbRt/oo1e0EbQ2LbZ9uOH9l7bxkVobee2dnQ/Y1sYAVPeAw88ENTnq1u3rjEeFxen5sTExBjjZ5xxhpqjjW1JSkpScxITE43x7du3qzkbNmwwxrds2aLm2L4jToXU1FR1W+PGjY3x7OxsNefMM880xlu2bKnmLFy40BjXftYi+sihnJwcNeff//63uq2yTJkyxRi3jcXRPs+2c5T2vaWNk7Hl2Gjf3bbnys/PN8a10V0iIrm5uQHvRxurZDvf+BnRpG2z5WgjjbRRVLbn057L9nxRUVFqjrbNNt7reLjiBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOqNCu3mCydRpqDh06pG7TumtsHcdhYWHGuK2j1U+Hkca2H63j19ZlV1hYaIxrN6G3qVOnTsA5qFz79+8PKG6jdXnCbteuXb62aX788ceTORznvP322wHFgZqAK34AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdU6DgXbTSKn5tP2258rN3I2ZYTzGPTRqmI6GNObKNmtBEstptZa2rXDvxHbBs1o42UseUE870GAAD+ccUPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxRoV29Wtem1uVpyyksLFRztI7SI0eOqDnaMdg6gbXu3fDwcDVH64JNSEhQc7SO37CwMDVHe99sXb1+3oPo6Ghj3E/HsQ0dvwAABB9X/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqjQcS4abcSJiD7GQxvZYhMfH69uy8vLM8ZtI1O08See56k52pgT2/gTbZufMTg22tiWOnXqqDnaNu39FAnuWB8AAOAfV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBGV0tVr62j1IyoqKuD9aF2jERERao7W8WvrgtU6mP109dr2c+TIEWO8dm39R3zo0CFjvLi4WM3R3gPtufzSOn7p9gUAwD+u+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHFEp41yCPZIjLy/PGE9ISFBzcnJyjPEGDRqoOdrzaSNbRESSkpKM8Xr16qk5HTt2NMZTU1PVHNuoF402Bsc2mkUbd2Mbg6NhNAsAAKcWV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEhnud5J/TAkJCKPhYREalVy1yL2jpAzzrrLGM8PT1dzVm4cKExbuto1dg6WqOjo43xkpKSgHOys7PVnF9++UXdFuh+zj77bDXnjDPOMMbnzp2r5mzfvt0Y137WIqeu4/cEP/6n1Klaa8CpxFoDTo3jrTWu+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHHHC41wAAABQvXHFDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBH/P8NRU3VmUAinAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3  # 定义图形中的子图排列方式\n",
    "for i in range(1, cols * rows + 1):  # 确保了总共会处理9个元素\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()  # .item() 方法用于从只包含单个值的tensor中提取这个值并将其转换为一个Python数值\n",
    "    img, label = training_data[sample_idx]  # 通过随机索引从训练数据集中获取图像和其对应的标签\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    # squeeze() 方法用于去除数组中维度为1的轴\n",
    "    # cmap 参数代表“colormap”，这个参数决定了如何将数据的数值映射到颜色上。\"gray\" 表示使用灰度颜色映射，这意味着图像将以灰阶（从黑到白）显示，而不是彩色。\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_3_'></a>[Creating a Custom Dataset for your files](#toc0_)\n",
    "\n",
    "A custom Dataset class must implement three functions:\n",
    "1. `__init__`\n",
    "   1. run once when instantiating the Dataset object\n",
    "   2. initialize the directory containing the images, the annotations file, and both transforms\n",
    "2. `__len__`\n",
    "   1. returns the number of samples in our dataset\n",
    "3. `__getitem__`\n",
    "   1. loads and returns a sample from the dataset at the given index idx\n",
    "   2. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image\n",
    "   3. retrieves the corresponding label from the csv data in self.img_labels\n",
    "   4. calls the transform functions on them (if applicable)\n",
    "   5. returns the tensor image and corresponding label in a tuple\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from collections.abc import Iterable, Iterator\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_4_'></a>[Preparing your data for training with DataLoaders](#toc0_)\n",
    "\n",
    "The Dataset retrieves our dataset’s features and labels one sample at a time. \n",
    "\n",
    "While training a model, we typically want to \n",
    "1. pass samples in **minibatches**\n",
    "2. **reshuffle** the data at every epoch to reduce model overfitting\n",
    "3. use Python’s **multiprocessing** to speed up data retrieval\n",
    "\n",
    "DataLoader is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import Iterable, Iterator\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "print(isinstance(train_dataloader, Iterable))  # 只是   iterable\n",
    "print(isinstance(train_dataloader, Iterator))  # 还不是 iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_5_'></a>[Iterate through the DataLoader](#toc0_)\n",
    "\n",
    "load dataset into the DataLoader and can iterate through the dataset\n",
    "\n",
    "Each iteration below returns `a batch of train_features and train_labels`, containing **batch_size**=64 features and labels \n",
    "\n",
    "Because we specified **shuffle**=True, after we iterate over all batches the data is shuffled\n",
    "\n",
    "for finer-grained control over the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "# Display image and label\n",
    "# 用 iter() 函数将它转换为一个迭代器\n",
    "# 使用 next() 函数从这个迭代器中获取下一个批次的数据\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[3. Transforms](#toc0_)\n",
    "\n",
    "use transforms to perform some manipulation of the data and **make it suitable for training**.\n",
    "\n",
    "All **TorchVision** datasets have 2 parameters that accept callables containing the transformation logic\n",
    "1. **transform** to modify the features\n",
    "2. **target_transform** to modify the labels\n",
    "\n",
    "The FashionMNIST features are in **PIL Image format**, and the **labels are integers**\n",
    "\n",
    "For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\n",
    "\n",
    "To make these transformations, we use **ToTensor** and **Lambda**.\n",
    "1. **ToTensor** converts a PIL image or NumPy ndarray into a **FloatTensor**. and scales the image’s pixel intensity **values in the range [0., 1.]**\n",
    "2. **Lambda** transforms apply any **user-defined lambda function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "scatter_(...)\n",
      "    scatter_(dim, index, src, reduce=None) -> Tensor\n",
      "    \n",
      "    Writes all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      "    specified in the :attr:`index` tensor. For each value in :attr:`src`, its output\n",
      "    index is specified by its index in :attr:`src` for ``dimension != dim`` and by\n",
      "    the corresponding value in :attr:`index` for ``dimension = dim``.\n",
      "    \n",
      "    For a 3-D tensor, :attr:`self` is updated as::\n",
      "    \n",
      "        self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
      "        self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
      "        self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
      "    \n",
      "    This is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n",
      "    \n",
      "    :attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have\n",
      "    the same number of dimensions. It is also required that\n",
      "    ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      "    ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      "    Note that ``index`` and ``src`` do not broadcast.\n",
      "    \n",
      "    Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\n",
      "    between ``0`` and ``self.size(dim) - 1`` inclusive.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        When indices are not unique, the behavior is non-deterministic (one of the\n",
      "        values from ``src`` will be picked arbitrarily) and the gradient will be\n",
      "        incorrect (it will be propagated to all locations in the source that\n",
      "        correspond to the same index)!\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        The backward pass is implemented only for ``src.shape == index.shape``.\n",
      "    \n",
      "    Additionally accepts an optional :attr:`reduce` argument that allows\n",
      "    specification of an optional reduction operation, which is applied to all\n",
      "    values in the tensor :attr:`src` into :attr:`self` at the indices\n",
      "    specified in the :attr:`index`. For each value in :attr:`src`, the reduction\n",
      "    operation is applied to an index in :attr:`self` which is specified by\n",
      "    its index in :attr:`src` for ``dimension != dim`` and by the corresponding\n",
      "    value in :attr:`index` for ``dimension = dim``.\n",
      "    \n",
      "    Given a 3-D tensor and reduction using the multiplication operation, :attr:`self`\n",
      "    is updated as::\n",
      "    \n",
      "        self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n",
      "        self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n",
      "        self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
      "    \n",
      "    Reducing with the addition operation is the same as using\n",
      "    :meth:`~torch.Tensor.scatter_add_`.\n",
      "    \n",
      "    .. warning::\n",
      "        The reduce argument with Tensor ``src`` is deprecated and will be removed in\n",
      "        a future PyTorch release. Please use :meth:`~torch.Tensor.scatter_reduce_`\n",
      "        instead for more reduction options.\n",
      "    \n",
      "    Args:\n",
      "        dim (int): the axis along which to index\n",
      "        index (LongTensor): the indices of elements to scatter, can be either empty\n",
      "            or of the same dimensionality as ``src``. When empty, the operation\n",
      "            returns ``self`` unchanged.\n",
      "        src (Tensor or float): the source element(s) to scatter.\n",
      "        reduce (str, optional): reduction operation to apply, can be either\n",
      "            ``'add'`` or ``'multiply'``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> src = torch.arange(1, 11).reshape((2, 5))\n",
      "        >>> src\n",
      "        tensor([[ 1,  2,  3,  4,  5],\n",
      "                [ 6,  7,  8,  9, 10]])\n",
      "        >>> index = torch.tensor([[0, 1, 2, 0]])\n",
      "        >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
      "        tensor([[1, 0, 0, 4, 0],\n",
      "                [0, 2, 0, 0, 0],\n",
      "                [0, 0, 3, 0, 0]])\n",
      "        >>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
      "        >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
      "        tensor([[1, 2, 3, 0, 0],\n",
      "                [6, 7, 0, 0, 8],\n",
      "                [0, 0, 0, 0, 0]])\n",
      "    \n",
      "        >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      "        ...            1.23, reduce='multiply')\n",
      "        tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
      "                [2.0000, 2.0000, 2.0000, 2.4600]])\n",
      "        >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      "        ...            1.23, reduce='add')\n",
      "        tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
      "                [2.0000, 2.0000, 2.0000, 3.2300]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "\n",
    "    # 从一个整数转换为一个长度为10的独热编码（one-hot encoding）向量\n",
    "    # dim=0 指定了操作的维度。\n",
    "    # index=torch.tensor(y) 是一个包含标签整数值的张量，这里 y 就是原始的类别标签。因为FashionMNIST的标签是从0到9的整数，所以这个标签直接用作索引。\n",
    "    # value=1 指定了要填入的值，即在对应的位置上填入1来完成独热编码。\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "    # define a function to turn the integer into a one-hot encoded tensor\n",
    "    # It first creates a zero tensor of size 10 (the number of labels in our dataset)\n",
    "    # and calls scatter_ which assigns a value=1 on the index as given by the label y.\n",
    ")\n",
    "\n",
    "help(torch.Tensor.scatter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 5., 0.],\n",
      "        [0., 0., 0., 0., 0., 5.]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.zeros((4,6), dtype=np.float32)\n",
    "# 让张量 tensor 的第 [3,5] 和 [2,4] 的位置变成 5\n",
    "rows = torch.tensor([3, 2])\n",
    "cols = torch.tensor([5, 4])\n",
    "\n",
    "tensor = torch.from_numpy(arr)\n",
    "tensor.index_put_((rows, cols), values=torch.tensor(5.0))\n",
    "print(tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[4. Build Model](#toc0_)\n",
    "\n",
    "Neural networks comprise of **layers/modules** that perform operations on data.\n",
    "\n",
    "The **torch.nn** namespace provides the **building blocks** you need to build your own neural network.\n",
    "\n",
    "Every module in PyTorch **subclasses the nn.Module**.\n",
    "\n",
    "A neural network is a module itself that consists of other modules (layers).\n",
    "\n",
    "This nested structure allows for building and managing complex architectures easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_1_'></a>[Get Device for Training](#toc0_)\n",
    "\n",
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available.\n",
    "\n",
    "Let’s check to see if \n",
    "1. `torch.cuda`\n",
    "2. torch.backends.mps\n",
    "are available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_2_'></a>[Define the Class](#toc0_)\n",
    "\n",
    "We define our neural network by **subclassing nn.Module**\n",
    "\n",
    "**initialize** the neural network layers in `__init__`\n",
    "\n",
    "Every nn.Module subclass implements the operations on input data in the `forward` method.\n",
    "\n",
    "We create an instance of NeuralNetwork, and **move network to the device**, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "linear_relu_stack.0.weight: 401408 参数\n",
      "linear_relu_stack.0.bias: 512 参数\n",
      "linear_relu_stack.2.weight: 262144 参数\n",
      "linear_relu_stack.2.bias: 512 参数\n",
      "linear_relu_stack.4.weight: 5120 参数\n",
      "linear_relu_stack.4.bias: 10 参数\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # numel() 是一个张量的方法，用于返回张量中所有元素的数量\n",
    "    print(f\"{name}: {param.numel()} 参数\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data, this executes the model’s forward, along with some background operations\n",
    "\n",
    "Do not call `model.forward()` directly\n",
    "\n",
    "当你调用 model(input_data) 时，实际上触发的是模型的 `__call__` 方法。`__call__` 是 PyTorch 中所有 nn.Module 对象的一个特殊方法\n",
    "1. 模型模式的设置 - 自动处理模型是处于训练模式还是评估模式\n",
    "2. 钩子 Hooks - 允许你在模型的层或模块上注册 前向钩子 (forward hooks) 和 后向钩子 (backward hooks)\n",
    "3. 梯度计算和自动求导 - PyTorch 的自动求导机制 **Autograd** 会自动记录运算图，对于后续的反向传播和梯度计算至关重要。直接调用 model.forward() 可能不会正确处理这些运算图的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1009,  0.0312, -0.0545, -0.0699,  0.0129,  0.0319, -0.0797, -0.0470,\n",
      "          0.1217,  0.0937]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "Predicted class: tensor([8], device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "cuda:0\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "\n",
    "print(logits)\n",
    "\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# Softmax 的作用是在每个类别的得分之间进行归一化，使它们的总和为 1，并将它们解释为概率\n",
    "print(type(pred_probab))\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "print(type(y_pred))\n",
    "print(y_pred.device)  # cuda:0\n",
    "\n",
    "y_numpy = y_pred.cpu().numpy()\n",
    "print(max(y_numpy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_3_'></a>[Model Layers](#toc0_)\n",
    "\n",
    "Model Layers\n",
    "1. `nn.Flatten` - convert each 2D image into a contiguous array of 784 pixel values (**the mini_batch dimension (at dim=0) is maintained**)\n",
    "2. `nn.Linear` - Tapplies a linear transformation on the input using its stored `weights and biases`\n",
    "3. `nn.ReLU` - applied **after linear transformations** to `introduce nonlinearity`, helping neural networks learn a wide variety of phenomena\n",
    "4. `nn.Sequential` - an **ordered container of modules**\n",
    "5. `nn.Softmax` - scaled to values [0, 1] representing the model’s predicted probabilities for each class (dim parameter indicates the dimension along which the values must sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([3, 20])\n",
      "Before ReLU: tensor([[-0.5040,  0.3386, -0.3072, -0.2560,  0.3166,  0.5963,  0.1408, -0.1392,\n",
      "          0.6674, -0.2026, -0.1357, -0.3166, -0.0325,  0.0931,  0.6309,  0.1811,\n",
      "         -0.0093,  0.2928, -0.6290, -0.8887],\n",
      "        [-0.0962,  0.4754, -0.2446,  0.0549, -0.3236,  0.4447,  0.5172, -0.0896,\n",
      "          0.3683, -0.2294,  0.0543, -0.3056, -0.2021,  0.0982,  0.6969,  0.2079,\n",
      "         -0.0220,  0.3362, -0.5563, -0.4716],\n",
      "        [-0.1744,  0.4619, -0.3354, -0.0634,  0.3415,  0.4201,  0.1268, -0.1051,\n",
      "          0.8493, -0.2469, -0.1547,  0.0458, -0.3737, -0.0335,  0.4260, -0.0689,\n",
      "          0.3032,  0.3093, -0.3652, -0.6136]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.3386, 0.0000, 0.0000, 0.3166, 0.5963, 0.1408, 0.0000, 0.6674,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0931, 0.6309, 0.1811, 0.0000, 0.2928,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.4754, 0.0000, 0.0549, 0.0000, 0.4447, 0.5172, 0.0000, 0.3683,\n",
      "         0.0000, 0.0543, 0.0000, 0.0000, 0.0982, 0.6969, 0.2079, 0.0000, 0.3362,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.4619, 0.0000, 0.0000, 0.3415, 0.4201, 0.1268, 0.0000, 0.8493,\n",
      "         0.0000, 0.0000, 0.0458, 0.0000, 0.0000, 0.4260, 0.0000, 0.3032, 0.3093,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)  # take a sample mini_batch of 3 images\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "logits\n",
      "tensor([[ 0.0748,  0.0135, -0.0187,  0.0638,  0.0663, -0.2102, -0.0778,  0.1127,\n",
      "         -0.0693, -0.1490],\n",
      "        [ 0.1470,  0.0311, -0.1771,  0.0752,  0.1366, -0.2233, -0.3362,  0.0914,\n",
      "         -0.3273,  0.0440],\n",
      "        [ 0.0403,  0.1718, -0.1708, -0.0260,  0.0988, -0.2397, -0.2088,  0.1201,\n",
      "         -0.1593, -0.1147]], grad_fn=<AddmmBackward0>)\n",
      "pred_probab\n",
      "tensor([[0.1093, 0.1028, 0.0996, 0.1081, 0.1084, 0.0822, 0.0939, 0.1136, 0.0947,\n",
      "         0.0874],\n",
      "        [0.1203, 0.1071, 0.0870, 0.1120, 0.1191, 0.0831, 0.0742, 0.1138, 0.0749,\n",
      "         0.1085],\n",
      "        [0.1082, 0.1234, 0.0876, 0.1013, 0.1147, 0.0818, 0.0844, 0.1172, 0.0886,\n",
      "         0.0927]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.3288, 0.3135, 0.3687, 0.3418, 0.3220, 0.3381, 0.3774, 0.3348, 0.3722,\n",
      "         0.3079],\n",
      "        [0.3535, 0.3191, 0.3147, 0.3457, 0.3454, 0.3337, 0.2915, 0.3278, 0.2876,\n",
      "         0.3734],\n",
      "        [0.3177, 0.3673, 0.3167, 0.3124, 0.3326, 0.3282, 0.3311, 0.3373, 0.3402,\n",
      "         0.3186]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "seq_modules.to(\"cpu\")\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "print(next(seq_modules.parameters()).device)  # cpu\n",
    "\n",
    "softmax1 = nn.Softmax(dim=1)\n",
    "softmax0 = nn.Softmax(dim=0)\n",
    "\n",
    "print(\"logits\")\n",
    "print(logits)\n",
    "\n",
    "pred_probab1 = softmax1(logits)\n",
    "pred_probab0 = softmax0(logits)\n",
    "\n",
    "print(\"pred_probab\")\n",
    "print(pred_probab1)\n",
    "print(pred_probab0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0474, 0.0474, 0.0474],\n",
      "        [0.9526, 0.9526, 0.9526]])\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0]])\n",
    "\n",
    "# 该 dim 的坐标变化，其他dim的坐标不变\n",
    "\n",
    "print(nn.Softmax(dim=0)(matrix))  # 列方向归一化\n",
    "print(nn.Softmax(dim=1)(matrix))  # 行方向归一化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_4_'></a>[Model Parameters](#toc0_)\n",
    "\n",
    "Many layers inside a neural network are parameterized\n",
    "\n",
    "have associated weights and biases that are optimized during training\n",
    "\n",
    "Subclassing nn.Module automatically tracks all fields defined inside your model object\n",
    "\n",
    "and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
    "\n",
    "**权重矩阵在左侧，输入数据（向量或矩阵）在右侧**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0051,  0.0233, -0.0341,  ..., -0.0090,  0.0176,  0.0308],\n",
      "        [ 0.0287, -0.0293,  0.0143,  ...,  0.0188, -0.0036, -0.0158]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "401408\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0001, 0.0171], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "512\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0221, -0.0024,  0.0207,  ..., -0.0011, -0.0222,  0.0204],\n",
      "        [ 0.0147, -0.0423,  0.0155,  ..., -0.0283, -0.0263, -0.0416]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "262144\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0334, -0.0361], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "512\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0208, -0.0010,  0.0174,  ...,  0.0003, -0.0228, -0.0115],\n",
      "        [-0.0095,  0.0427,  0.0425,  ...,  0.0360,  0.0148,  0.0396]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "5120\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0221, 0.0200], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "    print(param.numel())\n",
    "    # 通过 param[:2] 打印出每个参数张量的前两个元素。这里的切片操作 [:2] 旨在提供一个参数值的简略视图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6650, 0.0900, 0.2448],\n",
      "        [0.3333, 0.3333, 0.3333]], dtype=torch.float16)\n",
      "tensor([1., 1.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2,0,1],\n",
    "                  [1,1,1]], dtype=torch.float16)\n",
    "sf = nn.Softmax(dim=1)\n",
    "result = sf(x)\n",
    "print(result)\n",
    "print(result.sum(dim=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[5. Automatic Differentiation with `torch.autograd`](#toc0_)\n",
    "\n",
    "When training neural networks, the most frequently used algorithm is **back propagation**.\n",
    "\n",
    "In this algorithm, **parameters (model weights)** are adjusted according to **the gradient of the loss function** with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called **torch.autograd**.\n",
    "\n",
    "It supports **automatic computation of gradient for any computational graph**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "one-layer neural network, with input x, parameters w and b, and some loss function.\n",
    "\"\"\"\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)  # parameter requires_grad\n",
    "b = torch.randn(3, requires_grad=True)  # parameter requires_grad\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this network, **w and b are parameters**, which we need to optimize. \n",
    "\n",
    "Thus, we need to be able to compute the gradients of loss function with respect to those variables. \n",
    "\n",
    "In order to do that, we **set the requires_grad property of those tensors**.\n",
    "\n",
    "You can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method.\n",
    "\n",
    "**computational graph**\n",
    "\n",
    "![](Pics/torch002.png)\n",
    "\n",
    "You can set the value of `requires_grad` when creating a tensor, or later by using `x.requires_grad_(True)` method.\n",
    "\n",
    "A function that we apply to tensors to construct computational graph is an object of class Function\n",
    "\n",
    "This object knows how to\n",
    "1. **compute the function in the forward direction**\n",
    "2. **compute its derivative during the backward propagation step**.\n",
    "\n",
    "A reference to the backward propagation function is **stored in grad_fn property of a tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7e5c7ec53970>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7e5c503f7880>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_1_'></a>[Computing Gradients](#toc0_)\n",
    "\n",
    "To optimize weights of parameters in the neural network, \n",
    "\n",
    "we need to **compute the derivatives of loss function** **with respect to parameters**,\n",
    "\n",
    "we need $\\frac{\\partial loss}{\\partial w}$ and $\\frac{\\partial loss}{\\partial b}$ under some fixed values of `x` and `y`\n",
    "\n",
    "To compute those derivatives, we call `loss.backward()`, \n",
    "\n",
    "and then retrieve the values from `w.grad` and `b.grad`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1791, 0.2155, 0.0352],\n",
      "        [0.1791, 0.2155, 0.0352],\n",
      "        [0.1791, 0.2155, 0.0352],\n",
      "        [0.1791, 0.2155, 0.0352],\n",
      "        [0.1791, 0.2155, 0.0352]])\n",
      "tensor([0.1791, 0.2155, 0.0352])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer 是与模型的参数绑定的，优化器会保存对这些参数的引用\n",
    "# 通过调用 optimizer.step()，优化器会使用这些绑定的参数中的梯度信息来更新这些参数的值\n",
    "# loss.backward() 时，PyTorch 会自动计算损失函数相对于每个参数的梯度，并将这些梯度存储在对应参数的 .grad 属性中\n",
    "# 调用 optimizer.zero_grad() 来清除所有参数的 .grad，以避免梯度累积\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)  # 构成了计算图的一部分，每个操作都有可能添加新的节点和边到图中。此时，pred 是计算图中的一个节点，它保存了从输入 X 到输出 pred 的整个计算路径。\n",
    "        loss = loss_fn(pred, y)  # 损失函数接收模型的预测 pred 和真实标签 y 作为输入，计算出一个损失值 loss。\n",
    "                                 # 这一步将损失值与预测值 pred（以及通过 pred 间接与模型输入 X）联系起来，构成了计算图的最终输出节点。损失值 loss 反映了模型预测和真实标签之间的差异。\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()  # 损失函数 loss 和计算图之间的关系是通过模型预测 pred 来建立的\n",
    "        optimizer.step()  # 更新模型的参数。基于之前计算的梯度，这个步骤会调整模型的参数以最小化损失函数。\n",
    "        optimizer.zero_grad()  # 清零梯度, PyTorch 默认会累加梯度，如果不手动清零，那么下次调用 .backward() 时，新计算的梯度会和旧的梯度累加起来\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # :>7f 是格式说明符，: 表示开始格式说明符，> 表示右对齐，7f 表示总宽度为7的浮点数，包括小数点和小数部分，确保输出具有一致的宽度，方便阅读。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only obtain the grad properties for the **leaf nodes** of the computational graph,\n",
    "\n",
    "which have **requires_grad** property set to **True**. \n",
    "\n",
    "For **all other nodes in our graph**, gradients will **not be available**.\n",
    "\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons.\n",
    "\n",
    "If we need to do **several backward calls on the same graph, we need to pass retain_graph=True to the backward call.**\n",
    "\n",
    "\n",
    "ChatGPT4 解释\n",
    "1. **只有叶子节点的梯度可获取**\n",
    "   1. 在 PyTorch 的计算图中，只有被标记为 requires_grad=True 的叶子节点（leaf nodes）的梯度（grad 属性）是可获取的。\n",
    "   2. 叶子节点通常是指那些直接由用户创建的张量，而不是通过任何操作从其他张量计算得来的张量。\n",
    "   3. 这是因为只有这些节点代表的张量是需要优化的参数，或者是需要通过梯度信息来更新的数据。\n",
    "2. **非叶子节点的梯度不可获取**\n",
    "   1. 对于计算图中的非叶子节点，即那些由其他张量通过操作生成的张量，默认情况下不会保留它们的梯度（grad 属性）。\n",
    "   2. 这是出于性能和内存使用的考虑。在大多数情况下，我们只关心对模型参数（即叶子节点）的更新，而**不关心中间计算步骤的梯度**。\n",
    "3. **单次反向传播限制**\n",
    "   1. 出于性能考虑，**PyTorch 默认允许在给定的计算图上只进行一次反向传播（即调用 .backward() 方法一次）**。\n",
    "   2. 这是因为**在完成一次反向传播后，PyTorch 会自动清理计算图中的中间结果以节省内存**。\n",
    "   3. **如果你试图在同一个计算图上再次调用 .backward()，将会抛出错误**。\n",
    "4. **多次反向传播**\n",
    "   1. 如果你需要在同一个计算图上进行**多次反向传播，可以在调用 .backward() 时传递参数 retain_graph=True**。\n",
    "   2. 这样做会让 PyTorch 保留计算图，允许你进行额外的 .backward() 调用。\n",
    "   3. 但请注意，这**会增加内存的使用**，因为计算图中的中间结果需要被保留。\n",
    "\n",
    "\n",
    "PyTorch\n",
    "1. **默认累加梯度**\n",
    "   1. PyTorch 会默认累加梯度，即每次调用 `.backward()` 时，计算出的梯度会被加到已有的 .grad 属性上\n",
    "   2. 如果你不手动清零梯度(`optimizer.zero_grad()`)，那么梯度会在多次反向传播中持续累加。\n",
    "2. **动态计算图**\n",
    "   1. PyTorch 使用动态计算图（也称为动态自动微分系统），这意味着图是在运行时即时创建的\n",
    "   2. 默认情况下，当你对计算图的根节点调用 `.backward()` 时，PyTorch 会自动计算梯度，并且释放用于梯度计算的中间变量和资源。这意味着计算图被“消耗”了\n",
    "   3. 想要对同一个计算图多次调用 `.backward()`，以便从同一个前向传播过程中获取多个不同的梯度计算结果。需要在调用 .backward() 时设置 `retain_graph=True`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_2_'></a>[Disabling Gradient Tracking](#toc0_)\n",
    "\n",
    "By default, all tensors with **requires_grad=True** are **tracking their computational history** and **support gradient computation**.\n",
    "\n",
    "However, there are some cases when we do not need to do that,\n",
    "\n",
    "for example, when we have trained the model and **just want to apply it to some input data**, \n",
    "\n",
    "we **only want to do forward computations** through the network.\n",
    "\n",
    "We can stop tracking computations by surrounding our computation code `with torch.no_grad() block`:\n",
    "\n",
    "Another way to achieve the same result is to use the `.detach()` method on the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)  # True\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)  # False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()  # 不改变原来的属性\n",
    "\n",
    "print(z_det.requires_grad)  # False\n",
    "print(z.requires_grad)  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "1. To mark some parameters in your neural network as frozen parameters.\n",
    "2. To speed up computations when you are only doing forward pass, computations that do not track gradients would be more efficient.\n",
    "\n",
    "评估 eval 时通常不需要计算梯度\n",
    "\n",
    "model.eval() 切换模型到评估模式，但它**不会禁用梯度计算**，因此通常与 torch.no_grad() 一起使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_3_'></a>[More on Computational Graphs](#toc0_)\n",
    "\n",
    "autograd keeps a record of `data` (tensors) and all executed `operations` (along with the resulting new tensors) in a `directed acyclic graph (DAG 有向无环图)` consisting of Function objects. \n",
    "\n",
    "In this DAG, **leaves are the input tensors**, **roots are the output tensors**. By tracing this graph from roots to leaves, you can automatically **compute the gradients using the chain rule**.\n",
    "\n",
    "In a **forward** pass, autograd does two things simultaneously:\n",
    "1. run the requested operation to **compute a resulting tensor**\n",
    "2. **maintain the operation’s gradient function** in the DAG\n",
    "\n",
    "The **backward** pass kicks off when **.backward()** is called on the DAG root. autograd then:\n",
    "1. computes the gradients from each **.grad_fn**,\n",
    "2. **accumulates** them in the respective tensor’s **.grad attribute**\n",
    "3. using the chain rule, **propagates all the way to the leaf tensors**.\n",
    "\n",
    "DAGs are **dynamic** in PyTorch An important thing to note is that the graph is recreated from scratch; \n",
    "\n",
    "after each **.backward() call**, autograd starts populating a new graph. \n",
    "\n",
    "This is exactly what allows you to use control flow statements in your model; \n",
    "\n",
    "you can change the shape, size and operations at every iteration if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_4_'></a>[Optional Reading: Tensor Gradients and Jacobian Products](#toc0_)\n",
    "\n",
    "在 PyTorch 中，雅可比矩阵 Jacobian Matrix 使用的是 **分子布局**\n",
    "\n",
    "```\n",
    "分子布局 - Numerator Layout\n",
    "1. 行数 与 分子的变量维度（即结果变量的维度）一致\n",
    "2. 列数 与 分母的变量维度（即输入变量的维度）一致\n",
    "```\n",
    "\n",
    "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
    "\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
    "\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, \n",
    "\n",
    "a gradient of $\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian matrix**:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Instead of computing the Jacobian matrix itself, \n",
    "\n",
    "PyTorch allows you to compute **Jacobian Product** $v^T\\cdot J$ for a given input vector $v=(v_1 \\dots v_m)$.\n",
    "\n",
    "This is achieved by calling `backward` with $v$ as an argument. \n",
    "\n",
    "The size of $v$ should be the same as the size of the original tensor, with respect to which we want to compute the product:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]], requires_grad=True)\n",
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "print(f\"input\\n{inp}\")\n",
    "out = (inp+1).pow(2).t()\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)  # backward默认累加梯度\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "\n",
    "inp.grad.zero_()  # 在归零梯度后，梯度的计算重新开始，之前累积的梯度不再有影响\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，调用 .backward() 后，PyTorch 会自动清除计算图以节省内存。\n",
    "\n",
    "对于更复杂的情况，比如当损失是一个向量而不是标量时，你需要传递一个**梯度张量**作为参数\n",
    "\n",
    "Previously we were calling backward() function without parameters\n",
    "\n",
    "This is essentially **equivalent to calling backward(torch.tensor(1.0))**\n",
    "\n",
    "which is a useful way to compute the gradients in case of a scalar-valued function\n",
    "\n",
    "such as loss during neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[6. Optimizing Model Parameters](#toc0_)\n",
    "\n",
    "Now that we have a model and data it’s time to **train, validate and test** our model by optimizing its parameters on our data. \n",
    "\n",
    "Training a model is an **iterative process**, in each iteration\n",
    "1. the model makes a guess about the output\n",
    "2. calculates the error in its guess (loss)\n",
    "3. collects the derivatives of the error with respect to its parameters (as we saw in the previous section)\n",
    "4. optimizes these parameters using gradient descent. \n",
    "\n",
    "[反向传播演算|附录深入学习第3章 - 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"/home/lzy/Datasets\",\n",
    "    train = True,\n",
    "    download= True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"/home/lzy/Datasets\",\n",
    "    train = False,\n",
    "    download= True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_1_'></a>[Hyperparameters](#toc0_)\n",
    "\n",
    "Hyperparameters are **adjustable parameters** that let you **control the model optimization process**. \n",
    "\n",
    "Different hyperparameter values can **impact model training** and **convergence rates**收敛\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "1. **`Number of Epochs`** - the number times to iterate over the dataset\n",
    "2. **`Batch Size`** - **the number of data samples propagated** through the network before the parameters are updated\n",
    "3. **`Learning Rate`** - **how much to update models parameters** at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_2_'></a>[Optimization Loop](#toc0_)\n",
    "\n",
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop.\n",
    "\n",
    "Each iteration of the optimization loop is called an epoch.\n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "1. **`Train Loop`** - iterate over the **training dataset** and try to **converge to optimal parameters**.\n",
    "2. **`Validation/Test Loop`** - iterate over the **test dataset** to **check if model performance is improving** - 不改名模型参数\n",
    "\n",
    "\n",
    "Validation Loop\n",
    "1. 在训练的每个 epoch 结束后，模型会在验证集上进行评估\n",
    "2. 通过比较在验证集上的表现，可以判断模型是否在训练过程中有改进\n",
    "\n",
    "Test Loop\n",
    "1. 测试集通常只在模型训练完成后使用，目的是提供一个模型的最终性能评估，而不是用于在训练过程中监控模型的改进\n",
    "2. 不建议在训练过程中频繁使用测试集，因为这可能会导致测试集上的过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_3_'></a>[Loss Function](#toc0_)\n",
    "\n",
    "[torch.nn 的 Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer.\n",
    "\n",
    "Loss function **measures the degree of dissimilarity of obtained result to the target value**\n",
    "\n",
    "and it is the loss function that we **want to minimize** during training.\n",
    "\n",
    "To calculate the loss we **make a prediction using the inputs of our given data sample and compare it against the true data label value**.\n",
    "\n",
    "Common loss functions include\n",
    "1. nn.**MSELoss** (Mean Square Error) for regression tasks\n",
    "2. nn.**NLLLoss** (Negative Log Likelihood) for classification\n",
    "3. nn.**CrossEntropyLoss** combines nn.**LogSoftmax** and nn.**NLLLoss**.\n",
    "\n",
    "We pass our model’s output logits to nn.**CrossEntropyLoss**, which will normalize the logits and compute the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4076)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 测试使用\n",
    "import numpy as np\n",
    "print(loss_fn(torch.tensor([[1,2,3]], dtype=torch.float32),torch.tensor([1], dtype=torch.int64)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_4_'></a>[Optimizer](#toc0_)\n",
    "\n",
    "process of **adjusting model parameters** to **reduce model error** in each training step.\n",
    "\n",
    "**Optimization algorithms** define how this process is **performed** (in this example we use Stochastic Gradient Descent(SGD))\n",
    "\n",
    "All optimization logic is encapsulated in the `optimizer` object\n",
    "\n",
    "additionally, there are many different optimizers available in PyTorch such as **ADAM** and **RMSProp**, that work better for different kinds of models and data.\n",
    "\n",
    "[`torch.optim`](https://pytorch.org/docs/stable/optim.html) is a package implementing various optimization algorithms\n",
    "\n",
    "We **initialize the optimizer by registering the model’s parameters that need to be trained**, and **passing in the learning rate** hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "1. Call **optimizer.zero_grad()** to reset the gradients of model parameters. **Gradients by default add up**; to **prevent double-counting**, we **explicitly zero them at each iteration**.\n",
    "2. Backpropagate the prediction loss with a call to **loss.backward()**. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "3. Once we have our gradients, we call **optimizer.step()** to **adjust the parameters by the gradients collected in the backward pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_5_'></a>[Full Implementation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296582  [   64/60000]\n",
      "loss: 2.289059  [ 6464/60000]\n",
      "loss: 2.270224  [12864/60000]\n",
      "loss: 2.273022  [19264/60000]\n",
      "loss: 2.239754  [25664/60000]\n",
      "loss: 2.206733  [32064/60000]\n",
      "loss: 2.225665  [38464/60000]\n",
      "loss: 2.181712  [44864/60000]\n",
      "loss: 2.186435  [51264/60000]\n",
      "loss: 2.147869  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.145623 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.150752  [   64/60000]\n",
      "loss: 2.142325  [ 6464/60000]\n",
      "loss: 2.085541  [12864/60000]\n",
      "loss: 2.113395  [19264/60000]\n",
      "loss: 2.039187  [25664/60000]\n",
      "loss: 1.986238  [32064/60000]\n",
      "loss: 2.021068  [38464/60000]\n",
      "loss: 1.933813  [44864/60000]\n",
      "loss: 1.949265  [51264/60000]\n",
      "loss: 1.869976  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.869629 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.903413  [   64/60000]\n",
      "loss: 1.868660  [ 6464/60000]\n",
      "loss: 1.755455  [12864/60000]\n",
      "loss: 1.802445  [19264/60000]\n",
      "loss: 1.666929  [25664/60000]\n",
      "loss: 1.639316  [32064/60000]\n",
      "loss: 1.659827  [38464/60000]\n",
      "loss: 1.560332  [44864/60000]\n",
      "loss: 1.589720  [51264/60000]\n",
      "loss: 1.480274  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.495265 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.568890  [   64/60000]\n",
      "loss: 1.525445  [ 6464/60000]\n",
      "loss: 1.375300  [12864/60000]\n",
      "loss: 1.446845  [19264/60000]\n",
      "loss: 1.315304  [25664/60000]\n",
      "loss: 1.331816  [32064/60000]\n",
      "loss: 1.343020  [38464/60000]\n",
      "loss: 1.264905  [44864/60000]\n",
      "loss: 1.297558  [51264/60000]\n",
      "loss: 1.207256  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.223001 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.302945  [   64/60000]\n",
      "loss: 1.280585  [ 6464/60000]\n",
      "loss: 1.110398  [12864/60000]\n",
      "loss: 1.221688  [19264/60000]\n",
      "loss: 1.093301  [25664/60000]\n",
      "loss: 1.130406  [32064/60000]\n",
      "loss: 1.153733  [38464/60000]\n",
      "loss: 1.085019  [44864/60000]\n",
      "loss: 1.120104  [51264/60000]\n",
      "loss: 1.051697  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.060736 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.129023  [   64/60000]\n",
      "loss: 1.131386  [ 6464/60000]\n",
      "loss: 0.944270  [12864/60000]\n",
      "loss: 1.089094  [19264/60000]\n",
      "loss: 0.961015  [25664/60000]\n",
      "loss: 0.999696  [32064/60000]\n",
      "loss: 1.039285  [38464/60000]\n",
      "loss: 0.976617  [44864/60000]\n",
      "loss: 1.009480  [51264/60000]\n",
      "loss: 0.955879  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.959516 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.012899  [   64/60000]\n",
      "loss: 1.037335  [ 6464/60000]\n",
      "loss: 0.834629  [12864/60000]\n",
      "loss: 1.003838  [19264/60000]\n",
      "loss: 0.878260  [25664/60000]\n",
      "loss: 0.909205  [32064/60000]\n",
      "loss: 0.964307  [38464/60000]\n",
      "loss: 0.909456  [44864/60000]\n",
      "loss: 0.935157  [51264/60000]\n",
      "loss: 0.891704  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.891717 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.930055  [   64/60000]\n",
      "loss: 0.972833  [ 6464/60000]\n",
      "loss: 0.757604  [12864/60000]\n",
      "loss: 0.944696  [19264/60000]\n",
      "loss: 0.822674  [25664/60000]\n",
      "loss: 0.843480  [32064/60000]\n",
      "loss: 0.911186  [38464/60000]\n",
      "loss: 0.865943  [44864/60000]\n",
      "loss: 0.882443  [51264/60000]\n",
      "loss: 0.845112  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.843212 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.867199  [   64/60000]\n",
      "loss: 0.924652  [ 6464/60000]\n",
      "loss: 0.700383  [12864/60000]\n",
      "loss: 0.901018  [19264/60000]\n",
      "loss: 0.782667  [25664/60000]\n",
      "loss: 0.794065  [32064/60000]\n",
      "loss: 0.870605  [38464/60000]\n",
      "loss: 0.836291  [44864/60000]\n",
      "loss: 0.843213  [51264/60000]\n",
      "loss: 0.808906  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.806366 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.817152  [   64/60000]\n",
      "loss: 0.885864  [ 6464/60000]\n",
      "loss: 0.655823  [12864/60000]\n",
      "loss: 0.867299  [19264/60000]\n",
      "loss: 0.752011  [25664/60000]\n",
      "loss: 0.755737  [32064/60000]\n",
      "loss: 0.837585  [38464/60000]\n",
      "loss: 0.814511  [44864/60000]\n",
      "loss: 0.812533  [51264/60000]\n",
      "loss: 0.779297  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.776860 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.775678  [   64/60000]\n",
      "loss: 0.852855  [ 6464/60000]\n",
      "loss: 0.619779  [12864/60000]\n",
      "loss: 0.840316  [19264/60000]\n",
      "loss: 0.727366  [25664/60000]\n",
      "loss: 0.725138  [32064/60000]\n",
      "loss: 0.809416  [38464/60000]\n",
      "loss: 0.797337  [44864/60000]\n",
      "loss: 0.787406  [51264/60000]\n",
      "loss: 0.754073  [57664/60000]\n",
      "Current Learning Rate: 0.001\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.752163 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.740150  [   64/60000]\n",
      "loss: 0.824239  [ 6464/60000]\n",
      "loss: 0.590785  [12864/60000]\n",
      "loss: 0.819097  [19264/60000]\n",
      "loss: 0.708737  [25664/60000]\n",
      "loss: 0.702732  [32064/60000]\n",
      "loss: 0.787327  [38464/60000]\n",
      "loss: 0.785887  [44864/60000]\n",
      "loss: 0.769553  [51264/60000]\n",
      "loss: 0.735825  [57664/60000]\n",
      "Current Learning Rate: 0.0008\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.734767 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.715047  [   64/60000]\n",
      "loss: 0.803520  [ 6464/60000]\n",
      "loss: 0.570654  [12864/60000]\n",
      "loss: 0.804217  [19264/60000]\n",
      "loss: 0.695746  [25664/60000]\n",
      "loss: 0.687076  [32064/60000]\n",
      "loss: 0.770927  [38464/60000]\n",
      "loss: 0.777611  [44864/60000]\n",
      "loss: 0.756560  [51264/60000]\n",
      "loss: 0.722424  [57664/60000]\n",
      "Current Learning Rate: 0.00064\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.721996 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.696645  [   64/60000]\n",
      "loss: 0.788098  [ 6464/60000]\n",
      "loss: 0.556149  [12864/60000]\n",
      "loss: 0.793390  [19264/60000]\n",
      "loss: 0.686350  [25664/60000]\n",
      "loss: 0.675649  [32064/60000]\n",
      "loss: 0.758456  [38464/60000]\n",
      "loss: 0.771385  [44864/60000]\n",
      "loss: 0.746744  [51264/60000]\n",
      "loss: 0.712384  [57664/60000]\n",
      "Current Learning Rate: 0.0005120000000000001\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.712391 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.682849  [   64/60000]\n",
      "loss: 0.776488  [ 6464/60000]\n",
      "loss: 0.545433  [12864/60000]\n",
      "loss: 0.785301  [19264/60000]\n",
      "loss: 0.679298  [25664/60000]\n",
      "loss: 0.666981  [32064/60000]\n",
      "loss: 0.748966  [38464/60000]\n",
      "loss: 0.766542  [44864/60000]\n",
      "loss: 0.739101  [51264/60000]\n",
      "loss: 0.704810  [57664/60000]\n",
      "Current Learning Rate: 0.0004096000000000001\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.705058 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.672302  [   64/60000]\n",
      "loss: 0.767657  [ 6464/60000]\n",
      "loss: 0.537400  [12864/60000]\n",
      "loss: 0.779186  [19264/60000]\n",
      "loss: 0.673888  [25664/60000]\n",
      "loss: 0.660227  [32064/60000]\n",
      "loss: 0.741681  [38464/60000]\n",
      "loss: 0.762731  [44864/60000]\n",
      "loss: 0.733063  [51264/60000]\n",
      "loss: 0.699031  [57664/60000]\n",
      "Current Learning Rate: 0.0003276800000000001\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.699392 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.664150  [   64/60000]\n",
      "loss: 0.760824  [ 6464/60000]\n",
      "loss: 0.531311  [12864/60000]\n",
      "loss: 0.774511  [19264/60000]\n",
      "loss: 0.669670  [25664/60000]\n",
      "loss: 0.654875  [32064/60000]\n",
      "loss: 0.736021  [38464/60000]\n",
      "loss: 0.759712  [44864/60000]\n",
      "loss: 0.728263  [51264/60000]\n",
      "loss: 0.694568  [57664/60000]\n",
      "Current Learning Rate: 0.0002621440000000001\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.694975 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.657779  [   64/60000]\n",
      "loss: 0.755468  [ 6464/60000]\n",
      "loss: 0.526655  [12864/60000]\n",
      "loss: 0.770920  [19264/60000]\n",
      "loss: 0.666322  [25664/60000]\n",
      "loss: 0.650613  [32064/60000]\n",
      "loss: 0.731611  [38464/60000]\n",
      "loss: 0.757293  [44864/60000]\n",
      "loss: 0.724454  [51264/60000]\n",
      "loss: 0.691057  [57664/60000]\n",
      "Current Learning Rate: 0.00020971520000000012\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.691511 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.652757  [   64/60000]\n",
      "loss: 0.751233  [ 6464/60000]\n",
      "loss: 0.523061  [12864/60000]\n",
      "loss: 0.768138  [19264/60000]\n",
      "loss: 0.663631  [25664/60000]\n",
      "loss: 0.647212  [32064/60000]\n",
      "loss: 0.728149  [38464/60000]\n",
      "loss: 0.755346  [44864/60000]\n",
      "loss: 0.721436  [51264/60000]\n",
      "loss: 0.688235  [57664/60000]\n",
      "Current Learning Rate: 0.0001677721600000001\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.688783 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.648766  [   64/60000]\n",
      "loss: 0.747856  [ 6464/60000]\n",
      "loss: 0.520264  [12864/60000]\n",
      "loss: 0.765963  [19264/60000]\n",
      "loss: 0.661467  [25664/60000]\n",
      "loss: 0.644493  [32064/60000]\n",
      "loss: 0.725410  [38464/60000]\n",
      "loss: 0.753765  [44864/60000]\n",
      "loss: 0.719062  [51264/60000]\n",
      "loss: 0.685939  [57664/60000]\n",
      "Current Learning Rate: 0.00013421772800000008\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.686629 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Back - Propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    scheduler.step()\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_8_'></a>[7. Save, Load and Use Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_1_'></a>[Saving and Loading Model Weights](#toc0_)\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "```\n",
    "1. 仅保存模型的 参数(权重和偏置)，即模型的 state_dict\n",
    "2. state_dict 是一个 Python 字典，其中包含了模型中所有可学习参数(例如卷积层、全连接层的权重和偏置)的张量\n",
    "3. 优点是灵活性高，载时，你可以将这些参数加载到一个同样结构的模型中\n",
    "4. 常用于模型的训练和部署阶段，只需要保存和加载参数，而不需要保存整个模型的结构\n",
    "\n",
    "\n",
    "```python\n",
    "torch.save(model, 'model.pth')\n",
    "model = torch.load('model.pth')\n",
    "```\n",
    "1. 不仅保存了模型的 参数，还保存了模型的 结构，即整个模型对象\n",
    "2. 优点是简单易用，因为它保存了模型的所有信息，加载时不需要重新定义模型结构\n",
    "3. 适用于模型原型开发阶段、快速测试或在完全相同的环境中复现模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models **store the learned parameters in an internal state dictionary**, called **state_dict**.\n",
    "\n",
    "These can be persisted via the `torch.save` method:\n",
    "\n",
    "be sure to **call `model.eval()` method before inferencing** to **set the `dropout` and `batch normalization` layers to evaluation mode**.\n",
    "1. 确保 Dropout 层被禁用，所有神经元都处于激活状态。\n",
    "2. 确保 Batch Normalization 层使用这些训练期间计算得到的参数进行归一化，而不是当前输入批次的统计量\n",
    "\n",
    "Failing to do this will **yield inconsistent inference results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\n",
    "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_2_'></a>[Saving and Loading Models with Shapes](#toc0_)\n",
    "\n",
    "这种方式不仅保存了模型的 参数，还保存了模型的 结构，即整个模型对象\n",
    "\n",
    "When loading model weights, we needed to **instantiate the model class** first, because the **class defines the structure of a network**. \n",
    "\n",
    "We might want to **save the structure of this class together with the model**, \n",
    "\n",
    "in which case we can pass model (and not model.state_dict()) to the saving function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')\n",
    "model = torch.load('model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach uses **Python pickle module** when serializing the model, \n",
    "\n",
    "thus it **relies on the actual class definition to be available** when loading the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
