{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>Intro to PyTorch    [&#8593;](#toc0_)\n",
    "\n",
    "![](Pics/torch001.png)\n",
    "\n",
    "[PyTorch 官网]((https://pytorch.org/))\n",
    "\n",
    "[PyTorch Tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [PyTorch   ](#toc1_)    \n",
    "- [Learn the Basics](#toc2_)    \n",
    "  - [0. Quickstart](#toc2_1_)    \n",
    "    - [Working with data](#toc2_1_1_)    \n",
    "    - [Creating Models](#toc2_1_2_)    \n",
    "    - [Optimizing the Model Parameters](#toc2_1_3_)    \n",
    "    - [Saving & Loading Models](#toc2_1_4_)    \n",
    "  - [1. Tensors](#toc2_2_)    \n",
    "    - [Tensor Initialization](#toc2_2_1_)    \n",
    "    - [Tensor Attributes](#toc2_2_2_)    \n",
    "    - [Tensor Operations](#toc2_2_3_)    \n",
    "    - [Bridge with NumPy](#toc2_2_4_)    \n",
    "  - [2. Datasets and DataLoaders](#toc2_3_)    \n",
    "    - [Loading a Dataset](#toc2_3_1_)    \n",
    "    - [Iterating and Visualizing the Dataset](#toc2_3_2_)    \n",
    "    - [Creating a Custom Dataset for your files](#toc2_3_3_)    \n",
    "    - [Preparing your data for training with DataLoaders](#toc2_3_4_)    \n",
    "    - [Iterate through the DataLoader](#toc2_3_5_)    \n",
    "  - [3. Transforms](#toc2_4_)    \n",
    "  - [4. Build Model](#toc2_5_)    \n",
    "    - [Get Device for Training](#toc2_5_1_)    \n",
    "    - [Define the Class](#toc2_5_2_)    \n",
    "    - [Model Layers](#toc2_5_3_)    \n",
    "    - [Model Parameters](#toc2_5_4_)    \n",
    "  - [5. Automatic Differentiation with torch.autograd](#toc2_6_)    \n",
    "    - [Computing Gradients](#toc2_6_1_)    \n",
    "    - [Disabling Gradient Tracking](#toc2_6_2_)    \n",
    "    - [More on Computational Graphs](#toc2_6_3_)    \n",
    "    - [Optional Reading: Tensor Gradients and Jacobian Products](#toc2_6_4_)    \n",
    "  - [6. Optimizing Model Parameters](#toc2_7_)    \n",
    "    - [Hyperparameters](#toc2_7_1_)    \n",
    "    - [Optimization Loop](#toc2_7_2_)    \n",
    "    - [Loss Function](#toc2_7_3_)    \n",
    "    - [Optimizer](#toc2_7_4_)    \n",
    "    - [Full Implementation](#toc2_7_5_)    \n",
    "  - [7. Save, Load and Use Model](#toc2_8_)    \n",
    "    - [Saving and Loading Model Weights](#toc2_8_1_)    \n",
    "    - [Saving and Loading Models with Shapes](#toc2_8_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Learn the Basics](#toc0_)\n",
    "\n",
    "[Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[0. Quickstart](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Working with data](#toc0_)\n",
    "\n",
    "PyTorch has two primitives to work with data:\n",
    "1. torch.utils.data.**DataLoader**  - DataLoader wraps an iterable around the Dataset\n",
    "2. torch.utils.data.**Dataset**     - Dataset stores the samples and their corresponding labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries - (all of which include datasets)\n",
    "1. TorchText\n",
    "2. TorchVision\n",
    "3. TorchAudio\n",
    "\n",
    "The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO\n",
    "\n",
    "[Full List of datasets](https://pytorch.org/vision/stable/datasets.html)\n",
    "\n",
    "Every TorchVision Dataset includes two arguments to modify the samples and labels respectively\n",
    "1. transform - 用于对样本进行变换\n",
    "   1. 可以包含多种不同的操作，如缩放、裁剪、归一化等\n",
    "   2. 目的是通过这些预处理步骤来增强模型的泛化能力或使模型训练更为高效\n",
    "   3. 可以使用transforms.Compose来组合多个图像变换操作\n",
    "2. target_transform - 用于对标签进行变换\n",
    "   1. 如进行编码或转换成某种特定的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the **Dataset** as an argument to **DataLoader**. \n",
    "\n",
    "This wraps an iterable over our dataset, and supports\n",
    "1. automatic batching\n",
    "2. sampling\n",
    "3. shuffling\n",
    "4. multi-process data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Creating Models](#toc0_)\n",
    "\n",
    "To define a neural network in PyTorch, we **create a class that inherits from nn.Module**\n",
    "1. define the layers of the network in the **\\_\\_init\\_\\_ function**\n",
    "2. specify how data will pass through the network in the **forward function**\n",
    "\n",
    "To accelerate operations in the neural network, we move it to the GPU or MPS if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 确保父类的构造函数也被正确地调用\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),  # 根据前面的 shape\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # numel() 是一个张量的方法，用于返回张量中所有元素的数量\n",
    "    print(f\"{name}: {param.numel()} 参数\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[Optimizing the Model Parameters](#toc0_)\n",
    "\n",
    "**loss function** & **optimizer**\n",
    "\n",
    "优化器 optimizer 在 PyTorch 中用于存储和更新模型的参数\n",
    "\n",
    "当创建一个优化器实例时，需要将模型参数传递给它，这样优化器就知道它需要优化哪些参数\n",
    "\n",
    "模型的参数是通过模型的 `.parameters()` 方法获取的，该方法返回模型所有可训练参数的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches)\n",
    "\n",
    "and backpropagates the prediction error to adjust the model’s parameters\n",
    "\n",
    "also check the model’s performance against the test dataset to ensure it is learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)  # 构成了计算图的一部分，每个操作都有可能添加新的节点和边到图中。此时，pred 是计算图中的一个节点，它保存了从输入 X 到输出 pred 的整个计算路径。\n",
    "        loss = loss_fn(pred, y)  # 损失函数接收模型的预测 pred 和真实标签 y 作为输入，计算出一个损失值 loss。\n",
    "                                 # 这一步将损失值与预测值 pred（以及通过 pred 间接与模型输入 X）联系起来，构成了计算图的最终输出节点。损失值 loss 反映了模型预测和真实标签之间的差异。\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()  # 损失函数 loss 和计算图之间的关系是通过模型预测 pred 来建立的\n",
    "        optimizer.step()  # 更新模型的参数。基于之前计算的梯度，这个步骤会调整模型的参数以最小化损失函数。\n",
    "        optimizer.zero_grad()  # 清零梯度, PyTorch 默认会累加梯度，如果不手动清零，那么下次调用 .backward() 时，新计算的梯度会和旧的梯度累加起来\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # :>7f 是格式说明符，: 表示开始格式说明符，> 表示右对齐，7f 表示总宽度为7的浮点数，包括小数点和小数部分，确保输出具有一致的宽度，方便阅读。\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()  # item 将张量转化为标量\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            # type 将 bool 张量 转为 浮点 张量\n",
    "\n",
    "            # print(pred.argmax(1))\n",
    "            # print(y)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    # 0.1f 表示格式化为浮点数，保留一位小数。\n",
    "    # 8f 表示格式化为浮点数，保留八位数（总长度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Saving & Loading Models](#toc0_)\n",
    "\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "\n",
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "# print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[1. Tensors](#toc0_)\n",
    "\n",
    "\n",
    "[Tensor Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. \n",
    "\n",
    "In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to **NumPy’s ndarrays**\n",
    "\n",
    "except that **tensors can run on GPUs or other hardware accelerators**\n",
    "\n",
    "In fact, tensors and NumPy arrays **can often share the same underlying memory**, eliminating the need to copy data.\n",
    "\n",
    "Tensors are also **optimized for automatic differentiation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Tensor Initialization](#toc0_)\n",
    "\n",
    "Tensors can be initialized in various ways\n",
    "1. Directly from data - The **data type is automatically inferred**.\n",
    "2. From a NumPy array\n",
    "3. From another tensor\n",
    "4. With random or constant values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8199, 0.6335],\n",
      "        [0.0458, 0.6603]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9193, 0.3883, 0.0663],\n",
      "        [0.4424, 0.1092, 0.6596]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)  # 与 np_array 共享空间\n",
    "# x_np = torch.tensor(np_array)  # 不与 np_array 共享空间\n",
    "print(x_np)\n",
    "\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[Tensor Attributes](#toc0_)\n",
    "\n",
    "Tensor attributes describe their\n",
    "1. shape\n",
    "2. datatype\n",
    "3. device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "tensor = tensor.to(\"cuda\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[Tensor Operations](#toc0_)\n",
    "\n",
    "Over 100 tensor operations, including\n",
    "1. transposing\n",
    "2. indexing\n",
    "3. slicing\n",
    "4. mathematical operations\n",
    "5. linear algebra\n",
    "6. random sampling\n",
    "\n",
    "Each of them can be run on the GPU (at typically higher speeds than on a CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "print(torch.is_tensor(tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)  # dim 是几，cat后 就改变该 dim 的值\n",
    "print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[133., 133., 133., 133.],\n",
      "        [133., 133., 133., 133.],\n",
      "        [133., 133., 133., 133.],\n",
      "        [133., 133., 133., 133.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[133., 133., 133., 133.],\n",
      "        [133., 133., 133., 133.],\n",
      "        [133., 133., 133., 133.],\n",
      "        [133., 133., 133., 133.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place operations Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[Bridge with NumPy](#toc0_)\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()  # tensor -> numpy\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)  # 共享空间  # numpy -> tensor\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.tensor(n)  # 不共享空间\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[2. Datasets and DataLoaders](#toc0_)\n",
    "\n",
    "\n",
    "**dataset code to be decoupled from our model training code**\n",
    "\n",
    "PyTorch provides two data primitives to use pre-loaded datasets & own data.\n",
    "1. `torch.utils.data.DataLoader` - DataLoader wraps an **iterable** around the Dataset to enable easy access to the samples\n",
    "2. `torch.utils.data.Dataset` - Dataset stores the samples and labels\n",
    "\n",
    "PyTorch domain libraries provide a number of pre-loaded datasets\n",
    "\n",
    "subclass torch.utils.data.Dataset and implement functions specific to the particular data\n",
    "\n",
    "They can be used to prototype and benchmark your model\n",
    "1. [Image Datasets](https://pytorch.org/vision/stable/datasets.html)\n",
    "2. [Text Datasets](https://pytorch.org/text/stable/datasets.html)\n",
    "3. [Audio Datasets](https://pytorch.org/audio/stable/datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[Loading a Dataset](#toc0_)\n",
    "\n",
    "28×28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the FashionMNIST Dataset with the following parameters:\n",
    "1. **root**            - is the path where the train/test data is stored\n",
    "2. **train**           - specifies training or test dataset\n",
    "3. **download=True**   - downloads the data from the internet if it’s not available at root\n",
    "4. **transform** & **target_transform** - specify the feature and label transformations\n",
    "   1. transform 用于对 输入数据 进行变换\n",
    "   2. target_transform 用于对目标数据 (例如图像的标签或目标值) 进行变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[<class 'torch.Tensor'>, <class 'int'>]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(training_data[0]))  # 一个Tensor + 一个label\n",
    "print([type(i) for i in training_data[0]])\n",
    "print(torch.is_tensor(training_data[0][0]))  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[Iterating and Visualizing the Dataset](#toc0_)\n",
    "\n",
    "We can index Datasets manually like a list: training_data[index]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3  # 定义图形中的子图排列方式\n",
    "for i in range(1, cols * rows + 1):  # 确保了总共会处理9个元素\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()  # .item() 方法用于从只包含单个值的tensor中提取这个值并将其转换为一个Python数值\n",
    "    img, label = training_data[sample_idx]  # 通过随机索引从训练数据集中获取图像和其对应的标签\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    # squeeze() 方法用于去除数组中维度为1的轴\n",
    "    # cmap 参数代表“colormap”，这个参数决定了如何将数据的数值映射到颜色上。\"gray\" 表示使用灰度颜色映射，这意味着图像将以灰阶（从黑到白）显示，而不是彩色。\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_3_'></a>[Creating a Custom Dataset for your files](#toc0_)\n",
    "\n",
    "A custom Dataset class must implement three functions:\n",
    "1. `__init__`\n",
    "   1. run once when instantiating the Dataset object\n",
    "   2. initialize the directory containing the images, the annotations file, and both transforms\n",
    "2. `__len__`\n",
    "   1. returns the number of samples in our dataset\n",
    "3. `__getitem__`\n",
    "   1. loads and returns a sample from the dataset at the given index idx\n",
    "   2. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image\n",
    "   3. retrieves the corresponding label from the csv data in self.img_labels\n",
    "   4. calls the transform functions on them (if applicable)\n",
    "   5. returns the tensor image and corresponding label in a tuple\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from collections.abc import Iterable, Iterator\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_4_'></a>[Preparing your data for training with DataLoaders](#toc0_)\n",
    "\n",
    "The Dataset retrieves our dataset’s features and labels one sample at a time. \n",
    "\n",
    "While training a model, we typically want to \n",
    "1. pass samples in **minibatches**\n",
    "2. **reshuffle** the data at every epoch to reduce model overfitting\n",
    "3. use Python’s **multiprocessing** to speed up data retrieval\n",
    "\n",
    "DataLoader is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import Iterable, Iterator\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "print(isinstance(train_dataloader, Iterable))  # 只是   iterable\n",
    "print(isinstance(train_dataloader, Iterator))  # 还不是 iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_5_'></a>[Iterate through the DataLoader](#toc0_)\n",
    "\n",
    "load dataset into the DataLoader and can iterate through the dataset\n",
    "\n",
    "Each iteration below returns `a batch of train_features and train_labels`, containing **batch_size**=64 features and labels \n",
    "\n",
    "Because we specified **shuffle**=True, after we iterate over all batches the data is shuffled\n",
    "\n",
    "for finer-grained control over the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[3. Transforms](#toc0_)\n",
    "\n",
    "Data does not always come in its final processed form that is required for training machine learning algorithms\n",
    "\n",
    "use transforms to perform some manipulation of the data and **make it suitable for training**.\n",
    "\n",
    "All TorchVision datasets have two parameters that accept callables containing the transformation logic\n",
    "1. transform to modify the features\n",
    "2. target_transform to modify the labels\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers.\n",
    "\n",
    "For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\n",
    "\n",
    "To make these transformations, we use **ToTensor** and **Lambda**.\n",
    "1. **ToTensor** converts a PIL image or NumPy ndarray into a **FloatTensor**. and scales the image’s pixel intensity **values in the range [0., 1.]**\n",
    "2. **Lambda** transforms apply any **user-defined lambda function**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"/home/lzy/Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "\n",
    "    # 从一个整数转换为一个长度为10的独热编码（one-hot encoding）向量\n",
    "    # dim=0 指定了操作的维度。\n",
    "    # index=torch.tensor(y) 是一个包含标签整数值的张量，这里 y 就是原始的类别标签。因为FashionMNIST的标签是从0到9的整数，所以这个标签直接用作索引。\n",
    "    # value=1 指定了要填入的值，即在对应的位置上填入1来完成独热编码。\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "    # Here, we define a function to turn the integer into a one-hot encoded tensor.\n",
    "    # It first creates a zero tensor of size 10 (the number of labels in our dataset)\n",
    "    # and calls scatter_ which assigns a value=1 on the index as given by the label y.\n",
    ")\n",
    "\n",
    "help(torch.Tensor.scatter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "np_arr = np.zeros(shape=(3,5), dtype=np.uint8)\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "index = torch.tensor([1, 0, 3])\n",
    "print(index)\n",
    "index.unsqueeze_(1)\n",
    "print(index)\n",
    "scatter = tensor.scatter_(dim=1, index=index, value=1)\n",
    "print(scatter)\n",
    "\n",
    "print(scatter.max(axis=0))\n",
    "print(scatter.numpy().max(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[4. Build Model](#toc0_)\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data.\n",
    "\n",
    "The **torch.nn** namespace provides all the building blocks you need to build your own neural network.\n",
    "\n",
    "Every module in PyTorch subclasses the nn.Module.\n",
    "\n",
    "A neural network is a module itself that consists of other modules (layers).\n",
    "\n",
    "This nested structure allows for building and managing complex architectures easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_1_'></a>[Get Device for Training](#toc0_)\n",
    "\n",
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available.\n",
    "\n",
    "Let’s check to see if \n",
    "1. torch.cuda\n",
    "2. torch.backends.mps\n",
    "are available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_2_'></a>[Define the Class](#toc0_)\n",
    "\n",
    "We define our neural network by **subclassing nn.Module**\n",
    "\n",
    "**initialize** the neural network layers in \\_\\_init\\_\\_\n",
    "\n",
    "Every nn.Module subclass implements the operations on input data in the forward method.\n",
    "\n",
    "We create an instance of NeuralNetwork, and **move network to the device**, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_3_'></a>[Model Layers](#toc0_)\n",
    "\n",
    "Model Layers\n",
    "1. nn.Flatten - convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)\n",
    "2. nn.Linear - The linear layer is a module that applies a linear transformation on the input using its stored weights and biases\n",
    "3. nn.ReLU - applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena\n",
    "4. nn.Sequential - an ordered container of modules. The data is passed through all the modules in the same order as defined\n",
    "5. nn.Softmax - scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3,28,28)  # take a sample minibatch of 3 images\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "print(next(seq_modules.parameters()).device)  # cpu\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "\n",
    "print(pred_probab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_4_'></a>[Model Parameters](#toc0_)\n",
    "\n",
    "Many layers inside a neural network are parameterized\n",
    "\n",
    "have associated weights and biases that are optimized during training\n",
    "\n",
    "Subclassing nn.Module automatically tracks all fields defined inside your model object\n",
    "\n",
    "and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
    "\n",
    "**权重矩阵在左侧，输入数据（向量或矩阵）在右侧**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "    # 通过 param[:2] 打印出每个参数张量的前两个元素。这里的切片操作 [:2] 旨在提供一个参数值的简略视图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2,0,1],\n",
    "                  [1,1,1]], dtype=torch.float16)\n",
    "sf = nn.Softmax(dim=1)\n",
    "result = sf(x)\n",
    "print(result)\n",
    "print(result.sum(dim=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[5. Automatic Differentiation with torch.autograd](#toc0_)\n",
    "\n",
    "When training neural networks, the most frequently used algorithm is **back propagation**.\n",
    "\n",
    "In this algorithm, **parameters (model weights)** are adjusted according to **the gradient of the loss function** with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called **torch.autograd**.\n",
    "\n",
    "It supports **automatic computation of gradient for any computational graph**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "one-layer neural network, with input x, parameters w and b, and some loss function.\n",
    "\"\"\"\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)  # parameter requires_grad\n",
    "b = torch.randn(3, requires_grad=True)  # parameter requires_grad\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this network, **w and b are parameters**, which we need to optimize. \n",
    "\n",
    "Thus, we need to be able to compute the gradients of loss function with respect to those variables. \n",
    "\n",
    "In order to do that, we **set the requires_grad property of those tensors**.\n",
    "\n",
    "You can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method.\n",
    "\n",
    "**computational graph**\n",
    "\n",
    "![](Pics/torch002.png)\n",
    "\n",
    "You can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method.\n",
    "\n",
    "A function that we apply to tensors to construct computational graph is in fact an object of class Function.\n",
    "\n",
    "This object knows how to\n",
    "1. **compute the function in the forward direction**\n",
    "2. **compute its derivative during the backward propagation step**.\n",
    "\n",
    "A reference to the backward propagation function is **stored in grad_fn property of a tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_1_'></a>[Computing Gradients](#toc0_)\n",
    "\n",
    "To optimize weights of parameters in the neural network, \n",
    "\n",
    "we need to compute the derivatives of our loss function with respect to parameters,\n",
    "\n",
    "namely, we need $\\frac{\\partial loss}{\\partial w}$ and $\\frac{\\partial loss}{\\partial b}$ \n",
    "\n",
    "under some fixed values of `x` and `y`. To compute those derivatives, we call `loss.backward()`, \n",
    "\n",
    "and then retrieve the values from `w.grad` and `b.grad`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only obtain the grad properties for the leaf nodes of the computational graph,\n",
    "\n",
    "which have **requires_grad** property set to **True**. \n",
    "\n",
    "For **all other nodes in our graph**, gradients will **not be available**.\n",
    "\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons.\n",
    "\n",
    "If we need to do **several backward calls on the same graph, we need to pass retain_graph=True to the backward call.**\n",
    "\n",
    "\n",
    "ChatGPT4 解释\n",
    "1. **只有叶子节点的梯度可获取**\n",
    "   1. 在 PyTorch 的计算图中，只有被标记为 requires_grad=True 的叶子节点（leaf nodes）的梯度（grad 属性）是可获取的。\n",
    "   2. 叶子节点通常是指那些直接由用户创建的张量，而不是通过任何操作从其他张量计算得来的张量。\n",
    "   3. 这是因为只有这些节点代表的张量是需要优化的参数，或者是需要通过梯度信息来更新的数据。\n",
    "2. **非叶子节点的梯度不可获取**\n",
    "   1. 对于计算图中的非叶子节点，即那些由其他张量通过操作生成的张量，默认情况下不会保留它们的梯度（grad 属性）。\n",
    "   2. 这是出于性能和内存使用的考虑。在大多数情况下，我们只关心对模型参数（即叶子节点）的更新，而**不关心中间计算步骤的梯度**。\n",
    "3. **单次反向传播限制**\n",
    "   1. 出于性能考虑，**PyTorch 默认允许在给定的计算图上只进行一次反向传播（即调用 .backward() 方法一次）**。\n",
    "   2. 这是因为**在完成一次反向传播后，PyTorch 会自动清理计算图中的中间结果以节省内存**。\n",
    "   3. **如果你试图在同一个计算图上再次调用 .backward()，将会抛出错误**。\n",
    "4. **多次反向传播**\n",
    "   1. 如果你需要在同一个计算图上进行**多次反向传播，可以在调用 .backward() 时传递参数 retain_graph=True**。\n",
    "   2. 这样做会让 PyTorch 保留计算图，允许你进行额外的 .backward() 调用。\n",
    "   3. 但请注意，这**会增加内存的使用**，因为计算图中的中间结果需要被保留。\n",
    "\n",
    "\n",
    "PyTorch\n",
    "1. **默认累加梯度**\n",
    "   1. PyTorch 会默认累加梯度，即每次调用 .backward() 时，计算出的梯度会被加到已有的 .grad 属性上\n",
    "   2. 如果你不手动清零梯度(optimizer.zero_grad())，那么梯度会在多次反向传播中持续累加。\n",
    "2. **动态计算图**\n",
    "   1. PyTorch 使用动态计算图（也称为动态自动微分系统），这意味着图是在运行时即时创建的\n",
    "   2. 默认情况下，当你对计算图的根节点调用 .backward() 时，PyTorch 会自动计算梯度，并且释放用于梯度计算的中间变量和资源。这意味着计算图被“消耗”了\n",
    "   3. 想要对同一个计算图多次调用 .backward()，以便从同一个前向传播过程中获取多个不同的梯度计算结果。需要在调用 .backward() 时设置 retain_graph=True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_2_'></a>[Disabling Gradient Tracking](#toc0_)\n",
    "\n",
    "By default, all tensors with **requires_grad=True** are **tracking their computational history** and **support gradient computation**.\n",
    "\n",
    "However, there are some cases when we do not need to do that,\n",
    "\n",
    "for example, when we have trained the model and **just want to apply it to some input data**, \n",
    "\n",
    "we **only want to do forward computations** through the network.\n",
    "\n",
    "We can stop tracking computations by surrounding our computation code **with torch.no_grad() block**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)  # True\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)  # False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()  # 不改变原来的属性\n",
    "\n",
    "print(z_det.requires_grad)  # False\n",
    "print(z.requires_grad)  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "1. To mark some parameters in your neural network as frozen parameters.\n",
    "2. To speed up computations when you are only doing forward pass, computations that do not track gradients would be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_3_'></a>[More on Computational Graphs](#toc0_)\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG 有向无环图) consisting of Function objects. \n",
    "\n",
    "In this DAG, **leaves are the input tensors**, **roots are the output tensors**. By tracing this graph from roots to leaves, you can automatically **compute the gradients using the chain rule**.\n",
    "\n",
    "In a **forward** pass, autograd does two things simultaneously:\n",
    "1. run the requested operation to **compute a resulting tensor**\n",
    "2. **maintain the operation’s gradient function** in the DAG\n",
    "\n",
    "The **backward** pass kicks off when **.backward()** is called on the DAG root. autograd then:\n",
    "1. computes the gradients from each **.grad_fn**,\n",
    "2. **accumulates** them in the respective tensor’s **.grad attribute**\n",
    "3. using the chain rule, **propagates all the way to the leaf tensors**.\n",
    "\n",
    "DAGs are **dynamic** in PyTorch An important thing to note is that the graph is recreated from scratch; \n",
    "\n",
    "after each **.backward() call**, autograd starts populating a new graph. \n",
    "\n",
    "This is exactly what allows you to use control flow statements in your model; \n",
    "\n",
    "you can change the shape, size and operations at every iteration if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_4_'></a>[Optional Reading: Tensor Gradients and Jacobian Products](#toc0_)\n",
    "\n",
    "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
    "\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
    "\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, \n",
    "\n",
    "a gradient of $\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian matrix**:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Instead of computing the Jacobian matrix itself, \n",
    "\n",
    "PyTorch allows you to compute **Jacobian Product** $v^T\\cdot J$ for a given input vector $v=(v_1 \\dots v_m)$.\n",
    "\n",
    "This is achieved by calling `backward` with $v$ as an argument. \n",
    "\n",
    "The size of $v$ should be the same as the size of the original tensor, with respect to which we want to compute the product:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "print(f\"input\\n{inp}\")\n",
    "out = (inp+1).pow(2).t()\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)  # backward默认累加梯度\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "\n",
    "inp.grad.zero_()  # 在归零梯度后，梯度的计算重新开始，之前累积的梯度不再有影响\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，调用 .backward() 后，PyTorch 会自动清除计算图以节省内存。\n",
    "\n",
    "对于更复杂的情况，比如当损失是一个向量而不是标量时，你需要传递一个**梯度张量**作为参数\n",
    "\n",
    "Previously we were calling backward() function without parameters\n",
    "\n",
    "This is essentially **equivalent to calling backward(torch.tensor(1.0))**\n",
    "\n",
    "which is a useful way to compute the gradients in case of a scalar-valued function\n",
    "\n",
    "such as loss during neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[6. Optimizing Model Parameters](#toc0_)\n",
    "\n",
    "Now that we have a model and data it’s time to **train, validate and test** our model by optimizing its parameters on our data. \n",
    "\n",
    "Training a model is an **iterative process**, in each iteration\n",
    "1. the model makes a guess about the output\n",
    "2. calculates the error in its guess (loss)\n",
    "3. collects the derivatives of the error with respect to its parameters (as we saw in the previous section)\n",
    "4. optimizes these parameters using gradient descent. \n",
    "\n",
    "[反向传播演算|附录深入学习第3章 - 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"/home/lzy/Datasets\",\n",
    "    train = True,\n",
    "    download= True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"/home/lzy/Datasets\",\n",
    "    train = False,\n",
    "    download= True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_1_'></a>[Hyperparameters](#toc0_)\n",
    "\n",
    "Hyperparameters are **adjustable parameters** that let you **control the model optimization process**. \n",
    "\n",
    "Different hyperparameter values can **impact model training** and **convergence rates**收敛\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "1. **Number of Epochs** - the number times to iterate over the dataset\n",
    "2. **Batch Size** - **the number of data samples propagated** through the network before the parameters are updated\n",
    "3. **Learning Rate** - **how much to update models parameters** at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_2_'></a>[Optimization Loop](#toc0_)\n",
    "\n",
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop.\n",
    "\n",
    "Each iteration of the optimization loop is called an epoch.\n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "1. **Train Loop** - iterate over the **training dataset** and try to **converge to optimal parameters**.\n",
    "2. **Validation/Test Loop** - iterate over the **test dataset** to **check if model performance is improving**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_3_'></a>[Loss Function](#toc0_)\n",
    "\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer.\n",
    "\n",
    "**Loss function measures the degree of dissimilarity of obtained result to the target value**\n",
    "\n",
    "and it is the loss function that we **want to minimize** during training.\n",
    "\n",
    "To calculate the loss we **make a prediction using the inputs of our given data sample and compare it against the true data label value**.\n",
    "\n",
    "Common loss functions include\n",
    "1. nn.**MSELoss** (Mean Square Error) for regression tasks\n",
    "2. nn.**NLLLoss** (Negative Log Likelihood) for classification\n",
    "3. nn.**CrossEntropyLoss** combines nn.**LogSoftmax** and nn.**NLLLoss**.\n",
    "\n",
    "We pass our model’s output logits to nn.**CrossEntropyLoss**, which will normalize the logits and compute the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 测试使用\n",
    "import numpy as np\n",
    "print(loss_fn(torch.tensor([[1,2,3]], dtype=torch.float32),torch.tensor([1], dtype=torch.int64)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_4_'></a>[Optimizer](#toc0_)\n",
    "\n",
    "Optimization is the process of **adjusting model parameters** to **reduce model error** in each training step.\n",
    "\n",
    "**Optimization algorithms** define how this process is **performed** (in this example we use Stochastic Gradient Descent(SGD)).\n",
    "\n",
    "**All optimization logic is encapsulated in the optimizer object**. Here, we use the SGD optimizer;\n",
    "\n",
    "additionally, there are many different optimizers available in PyTorch such as **ADAM** and **RMSProp**, that work better for different kinds of models and data.\n",
    "\n",
    "We **initialize the optimizer by registering the model’s parameters that need to be trained**, and **passing in the learning rate** hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "1. Call **optimizer.zero_grad()** to reset the gradients of model parameters. **Gradients by default add up**; to **prevent double-counting**, we **explicitly zero them at each iteration**.\n",
    "2. Backpropagate the prediction loss with a call to **loss.backward()**. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "3. Once we have our gradients, we call **optimizer.step()** to **adjust the parameters by the gradients collected in the backward pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_5_'></a>[Full Implementation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    scheduler.step()\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_8_'></a>[7. Save, Load and Use Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_1_'></a>[Saving and Loading Model Weights](#toc0_)\n",
    "\n",
    "PyTorch models **store the learned parameters in an internal state dictionary**, called **state_dict**.\n",
    "\n",
    "These can be persisted via the torch.save method:\n",
    "\n",
    "be sure to **call model.eval() method before inferencing** to **set the dropout and batch normalization layers to evaluation mode**.\n",
    "1. 确保 Dropout 层被禁用，所有神经元都处于激活状态。\n",
    "2. 确保 Batch Normalization 层使用这些训练期间计算得到的参数进行归一化，而不是当前输入批次的统计量\n",
    "\n",
    "Failing to do this will **yield inconsistent inference results.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\n",
    "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_8_2_'></a>[Saving and Loading Models with Shapes](#toc0_)\n",
    "\n",
    "When loading model weights, we needed to **instantiate the model class** first, because the **class defines the structure of a network**. \n",
    "\n",
    "We might want to **save the structure of this class together with the model**, \n",
    "\n",
    "in which case we can pass model (and not model.state_dict()) to the saving function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')\n",
    "model = torch.load('model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach uses **Python pickle module** when serializing the model, \n",
    "\n",
    "thus it **relies on the actual class definition to be available** when loading the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
