# 5-Day Gen AI Intensive Course

## Day 1 - Foundation Models and Prompt Engineering

[Day 1 Livestream â€“ 5-Day Gen AI Intensive Course | Kaggle](https://www.youtube.com/watch?v=kpRyiJUUFxY)

[Foundational Large Language Models & Text Generation](./Newwhitepaper_Foundational%20Large%20Language%20models%20&%20text%20generation.pdf)

[Prompt Engineering](./Newwhitepaper_Prompt%20Engineering_v4.pdf)

```
ğŸ’ Day 1 Assignments

Complete the Intro Unit - â€œFoundational Large Language Models & Text Generationâ€, which is:
1. [Optional] Listen to the summary podcast episode (https://youtu.be/mQDlCZZsOyo) for this unit (created by NotebookLM, https://notebooklm.google.com/).
2. Read the â€œFoundational Large Language Models & Text Generationâ€ whitepaper (https://www.kaggle.com/whitepaper-foundational-llm-and-text-generation).

Complete Unit 1 - â€œPrompt Engineeringâ€, which is:
1. [Optional] Listen to the summary podcast episode (https://youtu.be/F_hJ2Ey4BNc) for this unit (created by NotebookLM).
2. Read the â€œPrompt Engineeringâ€ whitepaper (https://www.kaggle.com/whitepaper-prompt-engineering).
3. Complete this code lab (https://www.kaggle.com/code/markishere/day-1-prompting) on Kaggle where youâ€™ll learn prompting fundamentals. Make sure you phone verify (https://www.kaggle.com/settings) your account before starting, it's necessary for the code labs.


The code lab will walk you through getting started with the Gemini API and cover several prompt techniques and how different parameters impact the prompts.
```

[Google AI Studio è·å– API Key](https://aistudio.google.com/apikey)


```Python
import google.generativeai as genai
from IPython.display import HTML, Markdown, display

flash = genai.GenerativeModel('gemini-1.5-flash')

latest = genai.GenerativeModel(
    'gemini-1.5-flash-latest',
    generation_config=genai.GenerationConfig(
        temperature=1,
        top_p=1,
        max_output_tokens=1024,
    ))
```

å•è½®åº”ç­” - generate_content
```python
response = flash.generate_content("Explain AI to me like I'm an expert. in text format please")
print(response.text)
Markdown(response.text)  # render
```

å¤šè½®å¯¹è¯ - start_chat
```python
chat = flash.start_chat(history=[])
response = chat.send_message('Hello! My name is LEO.')
print(response.text)
# While you have the `chat` object around, the conversation state persists. Confirm that by asking if it knows my name.
response = chat.send_message('Do you remember what my name is?')
print(response.text)
```

<img src="Pics/gk001.png">

æŸ¥çœ‹ model åˆ—è¡¨

```python
for model in genai.list_models():
    print(model.name)

'''
models/chat-bison-001
models/text-bison-001
models/embedding-gecko-001
models/gemini-1.0-pro-latest
models/gemini-1.0-pro
models/gemini-pro
models/gemini-1.0-pro-001
models/gemini-1.0-pro-vision-latest
models/gemini-pro-vision
models/gemini-1.5-pro-latest
models/gemini-1.5-pro-001
models/gemini-1.5-pro-002
models/gemini-1.5-pro
models/gemini-1.5-pro-exp-0801
models/gemini-1.5-pro-exp-0827
models/gemini-1.5-flash-latest
models/gemini-1.5-flash-001
models/gemini-1.5-flash-001-tuning
models/gemini-1.5-flash
models/gemini-1.5-flash-exp-0827
models/gemini-1.5-flash-002
models/gemini-1.5-flash-8b
models/gemini-1.5-flash-8b-001
models/gemini-1.5-flash-8b-latest
models/gemini-1.5-flash-8b-exp-0827
models/gemini-1.5-flash-8b-exp-0924
models/embedding-001
models/text-embedding-004
models/aqa
'''
```


æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯

```python
for model in genai.list_models():
    if model.name == 'models/gemini-1.5-flash':
        print(model)
        break
'''
Model(name='models/gemini-1.5-flash',
      base_model_id='',
      version='001',
      display_name='Gemini 1.5 Flash',
      description='Fast and versatile multimodal model for scaling across diverse tasks',
      input_token_limit=1000000,
      output_token_limit=8192,
      supported_generation_methods=['generateContent', 'countTokens'],
      temperature=1.0,
      max_temperature=2.0,
      top_p=0.95,
      top_k=40)
'''
```

æ¨¡å‹å±æ€§
1. Output length
   1. `generation_config=genai.GenerationConfig(max_output_tokens=20)`
   2. affects cost and performance
   3. specify the `max_output_tokens` parameter
   4. stop generating tokens once the specified length is reached
   5. ä¸ä¼šå½±å“ç”Ÿæˆçš„å†…å®¹ï¼Œåˆ°é•¿åº¦è‡ªåŠ¨åœæ­¢
2. Temperature
   1. `generation_config=genai.GenerationConfig(temperature=2.0)`
   2. controls the degree of randomness in token selection
   3. higher temperatures result in a higher number of candidate tokens & produce more diverse results
   4. lower temperatures have the opposite effect
   5. temperature of 0 results in **greedy decoding**(select the most probable token at each step)
3. Top-K & Top-P
   1. `generation_config=genai.GenerationConfig(temperature=1.0,top_k=64,top_p=0.95,)` (default values for gemini-1.5-flash-001)
   2. also used to control the diversity of the model's output
   3. éƒ½æ˜¯ æ ¹æ®å€™é€‰å•è¯çš„ æ¦‚ç‡ æ’åºæ¥ç­›é€‰æœ€ç»ˆçš„å€™é€‰è¯
   4. åœ¨ Top-K æˆ– Top-P ç­›é€‰å‡ºå€™é€‰è¯ä¹‹åï¼ŒTemperature è¿˜ä¼šè¿›ä¸€æ­¥æ§åˆ¶ä»è¿™äº›å€™é€‰è¯ä¸­é€‰å–æœ€ç»ˆå•è¯çš„éšæœºæ€§
   5. **Top-K** is a **positive integer**, defines the number of most probable tokens from which to select the output token
      1. top-K of 1 selects a single token, performing **greedy decoding**
   6. **Top-P** defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates
      1. åŸºäºæ¦‚ç‡çš„æ€»å’Œï¼Œè€Œä¸æ˜¯å•çº¯çš„å‰ K ä¸ªå•è¯
      2. top-P of 1 typically selects every token in the model's vocabulary

Retry Policy : When running lots of queries, it's a good practice to use a retry policy so your code automatically retries when hitting Resource Exhausted (quota limit) errors.
```python
retry_policy = {
    "retry": retry.Retry(
        predicate=retry.if_transient_error,  # é”™è¯¯ç±»å‹åˆ¤æ–­æ¡ä»¶
        initial=10,  # åˆå§‹é‡è¯•ç­‰å¾…æ—¶é—´(ç§’)
        multiplier=1.5,  # æ¯æ¬¡é‡è¯•é—´éš”çš„ä¹˜æ•°(æ¸å¢åŠ é‡è¯•é—´éš”)
        timeout=300  # æœ€å¤§çš„é‡è¯•æ€»æ—¶é•¿(seconds)
    )
}
```


<img src="Pics/gk002.png">

Zero-Shot Prompt : æ— éœ€åšç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ä¾ç„¶å¯ä»¥å®Œæˆä¸€äº›ç®€å•çš„ä»»åŠ¡(ç¿»è¯‘ã€åˆ†ç±»)ï¼Œå¾—åˆ°ä¸é”™çš„æ•ˆæœ(æ³›åŒ–)
1. example
    ```python
    zero_shot_prompt = """Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.
    Review: "Her" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece.
    Sentiment:"""
    # Sentiment: **POSITIVE**
    ```

One-Shot and Few-Shot
1. example
    ```python
    few_shot_prompt = """Parse a customer's pizza order into valid JSON:

    EXAMPLE:
    I want a small pizza with cheese, tomato sauce, and pepperoni.
    JSON Response:
    #```
    {
    "size": "small",
    "type": "normal",
    "ingredients": ["cheese", "tomato sauce", "peperoni"]
    }
    #```

    EXAMPLE:
    Can I get a large pizza with tomato sauce, basil and mozzarella
    JSON Response:
    #```
    {
    "size": "large",
    "type": "normal",
    "ingredients": ["tomato sauce", "basil", "mozzarella"]
    }

    ORDER:
    """
    response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)
    ```

Chain of Thoughts (CoT):
1. instruct the model to output intermediate reasoning steps
2. enhance the LLM's reasoning abilities by prompting it to produce intermediate reasoning steps
3. typically gets better results, especially when combined with few-shot examples
4. prompt æœ«å°¾æ·»åŠ  `Let's think step by step.`
5. eg :
    ```python
    prompt = """When I was 4 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step."""

    response = model.generate_content(prompt, request_options=retry_policy)
    print(response.text)
    ```

ReAct: Reason and Act
1. [ReAct Prompting](https://github.com/ysymyth/ReAct/)
2. åŸºæœ¬æµç¨‹æ˜¯
   1. æ¨ç† (Reasoning) :    æ¨¡å‹é¦–å…ˆåˆ†æå½“å‰çš„é—®é¢˜æˆ–æƒ…å¢ƒï¼Œè¿›è¡Œæ¨ç†ï¼Œæ€è€ƒä¸‹ä¸€æ­¥è¯¥å¦‚ä½•è¡ŒåŠ¨
   2. è¡ŒåŠ¨ (Acting) :       åŸºäºæ¨ç†çš„ç»“æœï¼Œæ¨¡å‹æ‰§è¡ŒæŸç§è¡ŒåŠ¨ï¼Œæ¯”å¦‚æŸ¥è¯¢ä¿¡æ¯ã€è¿›è¡Œè®¡ç®—ã€æ‰§è¡Œæ“ä½œç­‰
   3. è§‚å¯Ÿ (Observation) :  æ¨¡å‹åœ¨è¡ŒåŠ¨åè§‚å¯Ÿåˆ°çš„ç»“æœ
   4. å¾ªç¯ (Loop) :         æ¨¡å‹æ ¹æ®è§‚å¯Ÿåˆ°çš„ç»“æœè¿›è¡Œæ–°çš„æ¨ç†ï¼Œè°ƒæ•´è¡ŒåŠ¨ç­–ç•¥ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚

Code Prompting/Execution/Explaining
1. Gemini family of models can be used to generate code, configuration and scripts
    ```python
    code_prompt = """
    Write a Python function to calculate the factorial of a number. No explanation, provide only the code.
    """
    ```
2. Gemini API can automatically run generated code, and will return the output
    ```python
    model = genai.GenerativeModel(
    'gemini-1.5-flash-latest',
    tools='code_execution',)

    code_exec_prompt = """
    Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.
    """
    for part in response.candidates[0].content.parts:
        print(part)

    '''
    text: "I will calculate the sum of the first 14 odd prime numbers. \n\nFirst, I need to determine the first 14 odd primes. \n\n"
    '''

    '''
    executable_code {
    language: PYTHON
    code: "\nimport sympy\n\nprimes = list(sympy.primerange(1, 50)) # check primes up to 50\nodd_primes = [prime for prime in primes if prime % 2 != 0] # filter odd primes\nprint(f\'{odd_primes[:14]=}\') # print first 14 odd primes\n"
    }
    '''

    '''
    code_execution_result {
    outcome: OUTCOME_OK
    output: "odd_primes[:14]=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n"
    }
    '''
    ```
3. Gemini family of models can explain code to you
    ```python
    file_contents = !curl https://xxx.com/xxx.sh
    explain_prompt = f"""
    Please explain what this file does at a very high level. What is it, and why would I use it?
    {file_contents}
    """
    ```



è¾“å‡ºå½¢å¼ - åœ¨åˆ›å»º model æ—¶å€™æŒ‡å®š
1. JSON
    ```python
    import typing_extensions as typing

    class PizzaOrder(typing.TypedDict):
        size: str
        ingredients: list[str]
        type: str

    model = genai.GenerativeModel(
        'gemini-1.5-flash-latest',
        generation_config=genai.GenerationConfig(
            temperature=0.1,
            response_mime_type="application/json",  # ç”Ÿæˆçš„å“åº”åº”è¯¥æ˜¯ JSON æ ¼å¼çš„
            response_schema=PizzaOrder,  # æŒ‡å®šæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºåº”è¯¥ç¬¦åˆ PizzaOrder ç±»å‹çš„ç»“æ„
        ))
    ```
2. enum - æ˜ç¡® è¾“å‡ºçš„ é€‰é¡¹é€‰æ‹©
    ```python
    import enum

    class Sentiment(enum.Enum):
        POSITIVE = "positive"
        NEUTRAL = "neutral"
        NEGATIVE = "negative"

    model = genai.GenerativeModel(
        'gemini-1.5-flash-001',
        generation_config=genai.GenerationConfig(
            response_mime_type="text/x.enum",
            response_schema=Sentiment
    ))
    ```



## Day 2 - Embeddings and Vector Databases

```
ğŸ’ Day 2 Assignments

Complete Unit 2: â€œEmbeddings and Vector Stores/Databasesâ€, which is:
1. [Optional] Listen to the summary podcast episode (https://youtube.com/watch?v=1CC39K76Nqs) for this unit (created by NotebookLM, https://notebooklm.google.com/).
2. Read the â€œEmbeddings and Vector Stores/Databasesâ€ whitepaper (https://kaggle.com/whitepaper-embeddings-and-vector-stores).
3. Complete these code labs on Kaggle:
4. Build a RAG question-answering system over custom documents - https://www.kaggle.com/code/markishere/day-2-document-q-a-with-rag
5. Explore text similarity with embeddings - https://www.kaggle.com/code/markishere/day-2-embeddings-and-similarity-scores
6. Build a neural classification network with Keras using embeddings - https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras

Here is the recording (https://www.youtube.com/watch?v=kpRyiJUUFxY) from this morningâ€™s livestream. We apologize for the live technical issues today! Fortunately our recording did not have the same errors.
```




## Day 3 - AI Agents

## Day 4 - Domain-Specific Models

## Day 5 - MLOps for Generative AI





